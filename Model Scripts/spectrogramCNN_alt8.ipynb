{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbdionne/toneclone/blob/main/spectrogramCNN_alt8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mIYygHe1JGIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed09448d-d517-4a6b-a224-2e5adbc77854"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/final_datasets\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.h5\" \"/content/final_datasets/final_real.h5\"\n",
        "#!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.csv\" \"/content/final_datasets/final_real.csv\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/Copy of full_real_test_w_DSP_features.csv\" \"/content/final_datasets/final_real.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/test_extra_TRM_DLY.h5\" \"/content/final_datasets/test_extra_TRM_DLY.h5\"\n",
        "#!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/test_extra_TRM_DLY.csv\" \"/content/final_datasets/test_extra_TRM_DLY.csv\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/Copy of full_test_w_DSP_features.csv\" \"/content/final_datasets/final_test.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/validate_extra_TRM_DLY.h5\" \"/content/final_datasets/validate_extra_TRM_DLY.h5\"\n",
        "#!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/validate_extra_TRM_DLY.csv\" \"/content/final_datasets/validate_extra_TRM_DLY.csv\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/Copy of full_validate_w_DSP_features.csv\" \"/content/final_datasets/final_validate.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/train_extra_TRM_DLY.h5\" \"/content/final_datasets/train_extra_TRM_DLY.h5\"\n",
        "#!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/train_extra_TRM_DLY.csv\" \"/content/final_datasets/train_extra_TRM_DLY.csv\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/Copy of full_train_w_DSP_features.csv\" \"/content/final_datasets/final_train.csv\""
      ],
      "metadata": {
        "id": "7QQiSvhn9atV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hvDlTB85Sraj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import torchaudio.transforms as T\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Downcasting object dtype arrays on .fillna\")\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for spectrogram data with data augmentation.\n",
        "    Includes:\n",
        "    - Random Gaussian noise\n",
        "    - Pitch shifting using torch.roll() with zero-padding (prevents wrapping)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, csv_file, augment=True, noise_level=0.03, pitch_shift_range=(-0.5, 0.5)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_file (str): Path to the HDF5 file containing spectrograms.\n",
        "            csv_file (str): Path to CSV file with labels.\n",
        "            augment (bool): Whether to apply data augmentation.\n",
        "            noise_level (float): Standard deviation of Gaussian noise to add.\n",
        "            pitch_shift_range (tuple): Min/max semitones for pitch shifting.\n",
        "        \"\"\"\n",
        "        self.hdf5_file_path = hdf5_file\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Manually define only important columns\n",
        "        self.label_map = [\n",
        "            'overdrive', 'distortion', 'fuzz', 'tremolo', 'phaser',\n",
        "            'flanger', 'chorus', 'delay', 'hall_reverb', 'plate_reverb',\n",
        "            'octaver', 'auto_filter'\n",
        "        ]\n",
        "\n",
        "        # Extract DSP feature columns\n",
        "        self.dsp_features = self.data.drop(columns=['key'] + self.label_map)\n",
        "\n",
        "        # Check for NaN or Inf values and replace them\n",
        "        if self.dsp_features.isna().sum().sum() > 0:\n",
        "            print(\"Warning: NaN values found in DSP features. Replacing with 0.\")\n",
        "            self.dsp_features = self.dsp_features.fillna(0)\n",
        "\n",
        "        if np.isinf(self.dsp_features.values).sum() > 0:\n",
        "            print(\"Warning: Inf values found in DSP features. Replacing with large finite numbers.\")\n",
        "            self.dsp_features = np.nan_to_num(self.dsp_features)\n",
        "\n",
        "        # Normalize DSP features\n",
        "        self.dsp_features = (self.dsp_features - self.dsp_features.mean()) / (self.dsp_features.std() + 1e-8)\n",
        "\n",
        "        self.hdf5_file = None  # Open HDF5 file once per worker\n",
        "\n",
        "        self.augment = augment\n",
        "        self.noise_level = noise_level\n",
        "        self.pitch_shift_range = pitch_shift_range\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Open HDF5 file per worker to avoid threading issues\n",
        "        if self.hdf5_file is None:\n",
        "            self.hdf5_file = h5py.File(self.hdf5_file_path, \"r\", swmr=True)\n",
        "\n",
        "        # Retrieve spectrogram\n",
        "        key = self.data.iloc[idx]['key']\n",
        "        spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Retrieve labels\n",
        "        label_values = self.data.iloc[idx][self.label_map].fillna(0).astype(float).values\n",
        "        label = torch.tensor(label_values, dtype=torch.float32)\n",
        "\n",
        "        # Retrieve DSP features\n",
        "        dsp_features = torch.tensor(self.dsp_features.iloc[idx].values, dtype=torch.float32)\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            spectrogram = self.add_noise(spectrogram)\n",
        "            spectrogram = self.pitch_shift(spectrogram)\n",
        "\n",
        "        return spectrogram, dsp_features, label\n",
        "\n",
        "    def add_noise(self, spectrogram):\n",
        "        \"\"\"Adds Gaussian noise where noise level is randomly chosen between 0 and self.noise_level.\"\"\"\n",
        "        noise_level = random.uniform(0, self.noise_level)  # Random noise per sample\n",
        "        noise = torch.randn_like(spectrogram) * noise_level  # Scale noise\n",
        "        return spectrogram + noise\n",
        "\n",
        "    def pitch_shift(self, spectrogram):\n",
        "        \"\"\"Shifts spectrogram frequency bins using torch.roll() with zero padding.\"\"\"\n",
        "        semitone_shift = random.uniform(*self.pitch_shift_range)  # Random shift between min/max\n",
        "        shift_bins = int(semitone_shift / 12 * spectrogram.shape[-2])  # Convert semitone shift to frequency bins\n",
        "\n",
        "        # Apply frequency bin shift using torch.roll() with zero-padding\n",
        "        shifted = torch.roll(spectrogram, shifts=shift_bins, dims=-2)  # Shift along frequency axis\n",
        "\n",
        "        if shift_bins > 0:  # Shift up (higher pitch)\n",
        "            shifted[..., :shift_bins, :] = 0  # Zero-pad low frequencies\n",
        "        elif shift_bins < 0:  # Shift down (lower pitch)\n",
        "            shifted[..., shift_bins:, :] = 0  # Zero-pad high frequencies\n",
        "\n",
        "        return shifted\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.hdf5_file is not None:\n",
        "            self.hdf5_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pVGFYMDhDnI9"
      },
      "outputs": [],
      "source": [
        "class spectrogramCNN(nn.Module):\n",
        "    def __init__(self, num_classes, dsp_feature_dim):\n",
        "        super(spectrogramCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layer for CNN output\n",
        "        self.cnn_fc = nn.Linear(512, 256)\n",
        "\n",
        "        # Fully connected layer for DSP features\n",
        "        self.dsp_fc = nn.Linear(dsp_feature_dim, 64)\n",
        "\n",
        "        # Combined fully connected layers\n",
        "        self.fc1 = nn.Linear(256 + 64, 256)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x, dsp_features):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.cnn_fc(x))\n",
        "\n",
        "        # Process DSP features\n",
        "        dsp_features = F.relu(self.dsp_fc(dsp_features))\n",
        "\n",
        "        # Concatenate feature sets\n",
        "        combined = torch.cat((x, dsp_features), dim=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        combined = F.relu(self.fc1(combined))\n",
        "        combined = self.dropout(combined)\n",
        "        combined = self.fc2(combined)\n",
        "\n",
        "        return combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p3BlietYUQpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0cc8cf6-9daf-4725-cb86-683d990b0a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: NaN values found in DSP features. Replacing with 0.\n",
            "Warning: NaN values found in DSP features. Replacing with 0.\n"
          ]
        }
      ],
      "source": [
        "# Initialize dataset from HD5F and csv file\n",
        "\n",
        "h5_train_path = '/content/final_datasets/train_extra_TRM_DLY.h5'\n",
        "csv_train_path = '/content/final_datasets/final_train.csv'\n",
        "\n",
        "h5_val_path = '/content/final_datasets/validate_extra_TRM_DLY.h5'\n",
        "csv_val_path = '/content/final_datasets/final_validate.csv'\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_altX.mod\"\n",
        "\n",
        "train_dataset = SpectrogramDataset(h5_train_path, csv_train_path)\n",
        "val_dataset = SpectrogramDataset(h5_val_path, csv_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUEbBB7wX05C",
        "outputId": "622b8212-f1ca-4b70-cc31-81d289314c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Batch [50/4502], Loss: 0.3509\n",
            "Epoch [1/15], Batch [100/4502], Loss: 0.3537\n",
            "Epoch [1/15], Batch [150/4502], Loss: 0.2984\n",
            "Epoch [1/15], Batch [200/4502], Loss: 0.2247\n",
            "Epoch [1/15], Batch [250/4502], Loss: 0.2331\n",
            "Epoch [1/15], Batch [300/4502], Loss: 0.1884\n",
            "Epoch [1/15], Batch [350/4502], Loss: 0.1454\n",
            "Epoch [1/15], Batch [400/4502], Loss: 0.1396\n",
            "Epoch [1/15], Batch [450/4502], Loss: 0.1283\n",
            "Epoch [1/15], Batch [500/4502], Loss: 0.1304\n",
            "Epoch [1/15], Batch [550/4502], Loss: 0.1395\n",
            "Epoch [1/15], Batch [600/4502], Loss: 0.1343\n",
            "Epoch [1/15], Batch [650/4502], Loss: 0.1205\n",
            "Epoch [1/15], Batch [700/4502], Loss: 0.1008\n",
            "Epoch [1/15], Batch [750/4502], Loss: 0.0891\n",
            "Epoch [1/15], Batch [800/4502], Loss: 0.1230\n",
            "Epoch [1/15], Batch [850/4502], Loss: 0.1490\n",
            "Epoch [1/15], Batch [900/4502], Loss: 0.1078\n",
            "Epoch [1/15], Batch [950/4502], Loss: 0.0738\n",
            "Epoch [1/15], Batch [1000/4502], Loss: 0.0786\n",
            "Epoch [1/15], Batch [1050/4502], Loss: 0.0974\n",
            "Epoch [1/15], Batch [1100/4502], Loss: 0.0850\n",
            "Epoch [1/15], Batch [1150/4502], Loss: 0.0727\n",
            "Epoch [1/15], Batch [1200/4502], Loss: 0.1326\n",
            "Epoch [1/15], Batch [1250/4502], Loss: 0.0977\n",
            "Epoch [1/15], Batch [1300/4502], Loss: 0.0584\n",
            "Epoch [1/15], Batch [1350/4502], Loss: 0.0584\n",
            "Epoch [1/15], Batch [1400/4502], Loss: 0.0685\n",
            "Epoch [1/15], Batch [1450/4502], Loss: 0.0939\n",
            "Epoch [1/15], Batch [1500/4502], Loss: 0.0509\n",
            "Epoch [1/15], Batch [1550/4502], Loss: 0.0681\n",
            "Epoch [1/15], Batch [1600/4502], Loss: 0.0462\n",
            "Epoch [1/15], Batch [1650/4502], Loss: 0.0626\n",
            "Epoch [1/15], Batch [1700/4502], Loss: 0.0383\n",
            "Epoch [1/15], Batch [1750/4502], Loss: 0.0696\n",
            "Epoch [1/15], Batch [1800/4502], Loss: 0.0481\n",
            "Epoch [1/15], Batch [1850/4502], Loss: 0.0583\n",
            "Epoch [1/15], Batch [1900/4502], Loss: 0.0575\n",
            "Epoch [1/15], Batch [1950/4502], Loss: 0.0415\n",
            "Epoch [1/15], Batch [2000/4502], Loss: 0.0713\n",
            "Epoch [1/15], Batch [2050/4502], Loss: 0.0585\n",
            "Epoch [1/15], Batch [2100/4502], Loss: 0.0611\n",
            "Epoch [1/15], Batch [2150/4502], Loss: 0.0357\n",
            "Epoch [1/15], Batch [2200/4502], Loss: 0.0494\n",
            "Epoch [1/15], Batch [2250/4502], Loss: 0.0497\n",
            "Epoch [1/15], Batch [2300/4502], Loss: 0.0579\n",
            "Epoch [1/15], Batch [2350/4502], Loss: 0.0498\n",
            "Epoch [1/15], Batch [2400/4502], Loss: 0.0426\n",
            "Epoch [1/15], Batch [2450/4502], Loss: 0.0454\n",
            "Epoch [1/15], Batch [2500/4502], Loss: 0.0704\n",
            "Epoch [1/15], Batch [2550/4502], Loss: 0.0435\n",
            "Epoch [1/15], Batch [2600/4502], Loss: 0.0418\n",
            "Epoch [1/15], Batch [2650/4502], Loss: 0.0439\n",
            "Epoch [1/15], Batch [2700/4502], Loss: 0.0335\n",
            "Epoch [1/15], Batch [2750/4502], Loss: 0.0563\n",
            "Epoch [1/15], Batch [2800/4502], Loss: 0.0552\n",
            "Epoch [1/15], Batch [2850/4502], Loss: 0.0404\n",
            "Epoch [1/15], Batch [2900/4502], Loss: 0.0521\n",
            "Epoch [1/15], Batch [2950/4502], Loss: 0.0289\n",
            "Epoch [1/15], Batch [3000/4502], Loss: 0.0278\n",
            "Epoch [1/15], Batch [3050/4502], Loss: 0.0675\n",
            "Epoch [1/15], Batch [3100/4502], Loss: 0.0398\n",
            "Epoch [1/15], Batch [3150/4502], Loss: 0.0175\n",
            "Epoch [1/15], Batch [3200/4502], Loss: 0.0492\n",
            "Epoch [1/15], Batch [3250/4502], Loss: 0.0659\n",
            "Epoch [1/15], Batch [3300/4502], Loss: 0.0308\n",
            "Epoch [1/15], Batch [3350/4502], Loss: 0.0234\n",
            "Epoch [1/15], Batch [3400/4502], Loss: 0.0278\n",
            "Epoch [1/15], Batch [3450/4502], Loss: 0.0234\n",
            "Epoch [1/15], Batch [3500/4502], Loss: 0.0641\n",
            "Epoch [1/15], Batch [3550/4502], Loss: 0.0196\n",
            "Epoch [1/15], Batch [3600/4502], Loss: 0.0213\n",
            "Epoch [1/15], Batch [3650/4502], Loss: 0.0521\n",
            "Epoch [1/15], Batch [3700/4502], Loss: 0.0338\n",
            "Epoch [1/15], Batch [3750/4502], Loss: 0.0650\n",
            "Epoch [1/15], Batch [3800/4502], Loss: 0.0615\n",
            "Epoch [1/15], Batch [3850/4502], Loss: 0.0300\n",
            "Epoch [1/15], Batch [3900/4502], Loss: 0.0126\n",
            "Epoch [1/15], Batch [3950/4502], Loss: 0.0299\n",
            "Epoch [1/15], Batch [4000/4502], Loss: 0.0129\n",
            "Epoch [1/15], Batch [4050/4502], Loss: 0.0386\n",
            "Epoch [1/15], Batch [4100/4502], Loss: 0.0428\n",
            "Epoch [1/15], Batch [4150/4502], Loss: 0.0321\n",
            "Epoch [1/15], Batch [4200/4502], Loss: 0.0344\n",
            "Epoch [1/15], Batch [4250/4502], Loss: 0.0253\n",
            "Epoch [1/15], Batch [4300/4502], Loss: 0.0196\n",
            "Epoch [1/15], Batch [4350/4502], Loss: 0.0179\n",
            "Epoch [1/15], Batch [4400/4502], Loss: 0.0191\n",
            "Epoch [1/15], Batch [4450/4502], Loss: 0.0109\n",
            "Epoch [1/15], Batch [4500/4502], Loss: 0.0218\n",
            "Epoch 1/15, Loss: 0.07753710089090456\n",
            "Updated Learning Rate: [8.577e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.81      0.89      3012\n",
            "  distortion       1.00      0.90      0.95      4518\n",
            "        fuzz       0.99      0.99      0.99      5271\n",
            "     tremolo       0.99      0.99      0.99      5271\n",
            "      phaser       0.99      0.99      0.99      4518\n",
            "     flanger       0.99      0.77      0.87      3012\n",
            "      chorus       0.73      1.00      0.84      5271\n",
            "       delay       0.99      0.89      0.94      7530\n",
            " hall_reverb       0.89      0.66      0.76      4518\n",
            "plate_reverb       0.81      0.82      0.81      3012\n",
            "     octaver       0.40      1.00      0.57      2259\n",
            " auto_filter       0.48      1.00      0.65      3765\n",
            "\n",
            "   micro avg       0.82      0.91      0.86     51957\n",
            "   macro avg       0.85      0.90      0.85     51957\n",
            "weighted avg       0.88      0.91      0.88     51957\n",
            " samples avg       0.81      0.87      0.82     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.1149, Accuracy: 0.6310, Precision: 0.8546, Recall: 0.9007, F1-score: 0.8540\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/15], Batch [50/4502], Loss: 0.0428\n",
            "Epoch [2/15], Batch [100/4502], Loss: 0.0373\n",
            "Epoch [2/15], Batch [150/4502], Loss: 0.0158\n",
            "Epoch [2/15], Batch [200/4502], Loss: 0.0164\n",
            "Epoch [2/15], Batch [250/4502], Loss: 0.0669\n",
            "Epoch [2/15], Batch [300/4502], Loss: 0.0183\n",
            "Epoch [2/15], Batch [350/4502], Loss: 0.0283\n",
            "Epoch [2/15], Batch [400/4502], Loss: 0.0108\n",
            "Epoch [2/15], Batch [450/4502], Loss: 0.0345\n",
            "Epoch [2/15], Batch [500/4502], Loss: 0.0305\n",
            "Epoch [2/15], Batch [550/4502], Loss: 0.0144\n",
            "Epoch [2/15], Batch [600/4502], Loss: 0.0494\n",
            "Epoch [2/15], Batch [650/4502], Loss: 0.0220\n",
            "Epoch [2/15], Batch [700/4502], Loss: 0.0162\n",
            "Epoch [2/15], Batch [750/4502], Loss: 0.0271\n",
            "Epoch [2/15], Batch [800/4502], Loss: 0.0551\n",
            "Epoch [2/15], Batch [850/4502], Loss: 0.0271\n",
            "Epoch [2/15], Batch [900/4502], Loss: 0.0304\n",
            "Epoch [2/15], Batch [950/4502], Loss: 0.0231\n",
            "Epoch [2/15], Batch [1000/4502], Loss: 0.0226\n",
            "Epoch [2/15], Batch [1050/4502], Loss: 0.0291\n",
            "Epoch [2/15], Batch [1100/4502], Loss: 0.0112\n",
            "Epoch [2/15], Batch [1150/4502], Loss: 0.0308\n",
            "Epoch [2/15], Batch [1200/4502], Loss: 0.0128\n",
            "Epoch [2/15], Batch [1250/4502], Loss: 0.0043\n",
            "Epoch [2/15], Batch [1300/4502], Loss: 0.0371\n",
            "Epoch [2/15], Batch [1350/4502], Loss: 0.0143\n",
            "Epoch [2/15], Batch [1400/4502], Loss: 0.0373\n",
            "Epoch [2/15], Batch [1450/4502], Loss: 0.0200\n",
            "Epoch [2/15], Batch [1500/4502], Loss: 0.0068\n",
            "Epoch [2/15], Batch [1550/4502], Loss: 0.0049\n",
            "Epoch [2/15], Batch [1600/4502], Loss: 0.0193\n",
            "Epoch [2/15], Batch [1650/4502], Loss: 0.0177\n",
            "Epoch [2/15], Batch [1700/4502], Loss: 0.0191\n",
            "Epoch [2/15], Batch [1750/4502], Loss: 0.0119\n",
            "Epoch [2/15], Batch [1800/4502], Loss: 0.0304\n",
            "Epoch [2/15], Batch [1850/4502], Loss: 0.0075\n",
            "Epoch [2/15], Batch [1900/4502], Loss: 0.0279\n",
            "Epoch [2/15], Batch [1950/4502], Loss: 0.0262\n",
            "Epoch [2/15], Batch [2000/4502], Loss: 0.0369\n",
            "Epoch [2/15], Batch [2050/4502], Loss: 0.0274\n",
            "Epoch [2/15], Batch [2100/4502], Loss: 0.0157\n",
            "Epoch [2/15], Batch [2150/4502], Loss: 0.0143\n",
            "Epoch [2/15], Batch [2200/4502], Loss: 0.0242\n",
            "Epoch [2/15], Batch [2250/4502], Loss: 0.0169\n",
            "Epoch [2/15], Batch [2300/4502], Loss: 0.0141\n",
            "Epoch [2/15], Batch [2350/4502], Loss: 0.0187\n",
            "Epoch [2/15], Batch [2400/4502], Loss: 0.0249\n",
            "Epoch [2/15], Batch [2450/4502], Loss: 0.0130\n",
            "Epoch [2/15], Batch [2500/4502], Loss: 0.0223\n",
            "Epoch [2/15], Batch [2550/4502], Loss: 0.0261\n",
            "Epoch [2/15], Batch [2600/4502], Loss: 0.0210\n",
            "Epoch [2/15], Batch [2650/4502], Loss: 0.0253\n",
            "Epoch [2/15], Batch [2700/4502], Loss: 0.0193\n",
            "Epoch [2/15], Batch [2750/4502], Loss: 0.0141\n",
            "Epoch [2/15], Batch [2800/4502], Loss: 0.0172\n",
            "Epoch [2/15], Batch [2850/4502], Loss: 0.0118\n",
            "Epoch [2/15], Batch [2900/4502], Loss: 0.0255\n",
            "Epoch [2/15], Batch [2950/4502], Loss: 0.0191\n",
            "Epoch [2/15], Batch [3000/4502], Loss: 0.0032\n",
            "Epoch [2/15], Batch [3050/4502], Loss: 0.0350\n",
            "Epoch [2/15], Batch [3100/4502], Loss: 0.0130\n",
            "Epoch [2/15], Batch [3150/4502], Loss: 0.0070\n",
            "Epoch [2/15], Batch [3200/4502], Loss: 0.0097\n",
            "Epoch [2/15], Batch [3250/4502], Loss: 0.0148\n",
            "Epoch [2/15], Batch [3300/4502], Loss: 0.0119\n",
            "Epoch [2/15], Batch [3350/4502], Loss: 0.0332\n",
            "Epoch [2/15], Batch [3400/4502], Loss: 0.0139\n",
            "Epoch [2/15], Batch [3450/4502], Loss: 0.0433\n",
            "Epoch [2/15], Batch [3500/4502], Loss: 0.0145\n",
            "Epoch [2/15], Batch [3550/4502], Loss: 0.0179\n",
            "Epoch [2/15], Batch [3600/4502], Loss: 0.0204\n",
            "Epoch [2/15], Batch [3650/4502], Loss: 0.0491\n",
            "Epoch [2/15], Batch [3700/4502], Loss: 0.0221\n",
            "Epoch [2/15], Batch [3750/4502], Loss: 0.0034\n",
            "Epoch [2/15], Batch [3800/4502], Loss: 0.0213\n",
            "Epoch [2/15], Batch [3850/4502], Loss: 0.0190\n",
            "Epoch [2/15], Batch [3900/4502], Loss: 0.0135\n",
            "Epoch [2/15], Batch [3950/4502], Loss: 0.0242\n",
            "Epoch [2/15], Batch [4000/4502], Loss: 0.0172\n",
            "Epoch [2/15], Batch [4050/4502], Loss: 0.0124\n",
            "Epoch [2/15], Batch [4100/4502], Loss: 0.0161\n",
            "Epoch [2/15], Batch [4150/4502], Loss: 0.0244\n",
            "Epoch [2/15], Batch [4200/4502], Loss: 0.0129\n",
            "Epoch [2/15], Batch [4250/4502], Loss: 0.0330\n",
            "Epoch [2/15], Batch [4300/4502], Loss: 0.0279\n",
            "Epoch [2/15], Batch [4350/4502], Loss: 0.0369\n",
            "Epoch [2/15], Batch [4400/4502], Loss: 0.0224\n",
            "Epoch [2/15], Batch [4450/4502], Loss: 0.0077\n",
            "Epoch [2/15], Batch [4500/4502], Loss: 0.0097\n",
            "Epoch 2/15, Loss: 0.020530747957006327\n",
            "Updated Learning Rate: [7.3564929e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.97      0.98      3012\n",
            "  distortion       1.00      0.99      0.99      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      0.99      1.00      5271\n",
            "      phaser       1.00      0.99      0.99      4518\n",
            "     flanger       1.00      0.93      0.96      3012\n",
            "      chorus       0.98      1.00      0.99      5271\n",
            "       delay       0.98      0.89      0.94      7530\n",
            " hall_reverb       0.87      0.90      0.89      4518\n",
            "plate_reverb       0.73      0.99      0.84      3012\n",
            "     octaver       0.86      0.99      0.92      2259\n",
            " auto_filter       0.89      1.00      0.94      3765\n",
            "\n",
            "   micro avg       0.95      0.97      0.96     51957\n",
            "   macro avg       0.94      0.97      0.95     51957\n",
            "weighted avg       0.95      0.97      0.96     51957\n",
            " samples avg       0.92      0.94      0.92     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0345, Accuracy: 0.8784, Precision: 0.9423, Recall: 0.9701, F1-score: 0.9536\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/15], Batch [50/4502], Loss: 0.0057\n",
            "Epoch [3/15], Batch [100/4502], Loss: 0.0189\n",
            "Epoch [3/15], Batch [150/4502], Loss: 0.0175\n",
            "Epoch [3/15], Batch [200/4502], Loss: 0.0126\n",
            "Epoch [3/15], Batch [250/4502], Loss: 0.0136\n",
            "Epoch [3/15], Batch [300/4502], Loss: 0.0152\n",
            "Epoch [3/15], Batch [350/4502], Loss: 0.0097\n",
            "Epoch [3/15], Batch [400/4502], Loss: 0.0041\n",
            "Epoch [3/15], Batch [450/4502], Loss: 0.0148\n",
            "Epoch [3/15], Batch [500/4502], Loss: 0.0117\n",
            "Epoch [3/15], Batch [550/4502], Loss: 0.0062\n",
            "Epoch [3/15], Batch [600/4502], Loss: 0.0107\n",
            "Epoch [3/15], Batch [650/4502], Loss: 0.0091\n",
            "Epoch [3/15], Batch [700/4502], Loss: 0.0134\n",
            "Epoch [3/15], Batch [750/4502], Loss: 0.0081\n",
            "Epoch [3/15], Batch [800/4502], Loss: 0.0049\n",
            "Epoch [3/15], Batch [850/4502], Loss: 0.0177\n",
            "Epoch [3/15], Batch [900/4502], Loss: 0.0067\n",
            "Epoch [3/15], Batch [950/4502], Loss: 0.0079\n",
            "Epoch [3/15], Batch [1000/4502], Loss: 0.0037\n",
            "Epoch [3/15], Batch [1050/4502], Loss: 0.0187\n",
            "Epoch [3/15], Batch [1100/4502], Loss: 0.0083\n",
            "Epoch [3/15], Batch [1150/4502], Loss: 0.0403\n",
            "Epoch [3/15], Batch [1200/4502], Loss: 0.0079\n",
            "Epoch [3/15], Batch [1250/4502], Loss: 0.0111\n",
            "Epoch [3/15], Batch [1300/4502], Loss: 0.0051\n",
            "Epoch [3/15], Batch [1350/4502], Loss: 0.0171\n",
            "Epoch [3/15], Batch [1400/4502], Loss: 0.0053\n",
            "Epoch [3/15], Batch [1450/4502], Loss: 0.0223\n",
            "Epoch [3/15], Batch [1500/4502], Loss: 0.0099\n",
            "Epoch [3/15], Batch [1550/4502], Loss: 0.0105\n",
            "Epoch [3/15], Batch [1600/4502], Loss: 0.0197\n",
            "Epoch [3/15], Batch [1650/4502], Loss: 0.0079\n",
            "Epoch [3/15], Batch [1700/4502], Loss: 0.0136\n",
            "Epoch [3/15], Batch [1750/4502], Loss: 0.0201\n",
            "Epoch [3/15], Batch [1800/4502], Loss: 0.0060\n",
            "Epoch [3/15], Batch [1850/4502], Loss: 0.0138\n",
            "Epoch [3/15], Batch [1900/4502], Loss: 0.0041\n",
            "Epoch [3/15], Batch [1950/4502], Loss: 0.0359\n",
            "Epoch [3/15], Batch [2000/4502], Loss: 0.0067\n",
            "Epoch [3/15], Batch [2050/4502], Loss: 0.0028\n",
            "Epoch [3/15], Batch [2100/4502], Loss: 0.0052\n",
            "Epoch [3/15], Batch [2150/4502], Loss: 0.0060\n",
            "Epoch [3/15], Batch [2200/4502], Loss: 0.0224\n",
            "Epoch [3/15], Batch [2250/4502], Loss: 0.0033\n",
            "Epoch [3/15], Batch [2300/4502], Loss: 0.0175\n",
            "Epoch [3/15], Batch [2350/4502], Loss: 0.0026\n",
            "Epoch [3/15], Batch [2400/4502], Loss: 0.0161\n",
            "Epoch [3/15], Batch [2450/4502], Loss: 0.0018\n",
            "Epoch [3/15], Batch [2500/4502], Loss: 0.0084\n",
            "Epoch [3/15], Batch [2550/4502], Loss: 0.0020\n",
            "Epoch [3/15], Batch [2600/4502], Loss: 0.0132\n",
            "Epoch [3/15], Batch [2650/4502], Loss: 0.0100\n",
            "Epoch [3/15], Batch [2700/4502], Loss: 0.0126\n",
            "Epoch [3/15], Batch [2750/4502], Loss: 0.0278\n",
            "Epoch [3/15], Batch [2800/4502], Loss: 0.0079\n",
            "Epoch [3/15], Batch [2850/4502], Loss: 0.0016\n",
            "Epoch [3/15], Batch [2900/4502], Loss: 0.0288\n",
            "Epoch [3/15], Batch [2950/4502], Loss: 0.0153\n",
            "Epoch [3/15], Batch [3000/4502], Loss: 0.0104\n",
            "Epoch [3/15], Batch [3050/4502], Loss: 0.0120\n",
            "Epoch [3/15], Batch [3100/4502], Loss: 0.0093\n",
            "Epoch [3/15], Batch [3150/4502], Loss: 0.0220\n",
            "Epoch [3/15], Batch [3200/4502], Loss: 0.0113\n",
            "Epoch [3/15], Batch [3250/4502], Loss: 0.0041\n",
            "Epoch [3/15], Batch [3300/4502], Loss: 0.0043\n",
            "Epoch [3/15], Batch [3350/4502], Loss: 0.0030\n",
            "Epoch [3/15], Batch [3400/4502], Loss: 0.0021\n",
            "Epoch [3/15], Batch [3450/4502], Loss: 0.0077\n",
            "Epoch [3/15], Batch [3500/4502], Loss: 0.0058\n",
            "Epoch [3/15], Batch [3550/4502], Loss: 0.0243\n",
            "Epoch [3/15], Batch [3600/4502], Loss: 0.0047\n",
            "Epoch [3/15], Batch [3650/4502], Loss: 0.0037\n",
            "Epoch [3/15], Batch [3700/4502], Loss: 0.0197\n",
            "Epoch [3/15], Batch [3750/4502], Loss: 0.0223\n",
            "Epoch [3/15], Batch [3800/4502], Loss: 0.0238\n",
            "Epoch [3/15], Batch [3850/4502], Loss: 0.0015\n",
            "Epoch [3/15], Batch [3900/4502], Loss: 0.0042\n",
            "Epoch [3/15], Batch [3950/4502], Loss: 0.0067\n",
            "Epoch [3/15], Batch [4000/4502], Loss: 0.0034\n",
            "Epoch [3/15], Batch [4050/4502], Loss: 0.0045\n",
            "Epoch [3/15], Batch [4100/4502], Loss: 0.0126\n",
            "Epoch [3/15], Batch [4150/4502], Loss: 0.0147\n",
            "Epoch [3/15], Batch [4200/4502], Loss: 0.0110\n",
            "Epoch [3/15], Batch [4250/4502], Loss: 0.0085\n",
            "Epoch [3/15], Batch [4300/4502], Loss: 0.0233\n",
            "Epoch [3/15], Batch [4350/4502], Loss: 0.0060\n",
            "Epoch [3/15], Batch [4400/4502], Loss: 0.0163\n",
            "Epoch [3/15], Batch [4450/4502], Loss: 0.0165\n",
            "Epoch [3/15], Batch [4500/4502], Loss: 0.0086\n",
            "Epoch 3/15, Loss: 0.012612497344904916\n",
            "Updated Learning Rate: [6.30966396033e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.99      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       0.99      1.00      0.99      5271\n",
            "      phaser       1.00      0.99      1.00      4518\n",
            "     flanger       1.00      0.97      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.96      0.98      7530\n",
            " hall_reverb       0.97      0.95      0.96      4518\n",
            "plate_reverb       0.96      0.99      0.97      3012\n",
            "     octaver       0.96      1.00      0.98      2259\n",
            " auto_filter       0.99      1.00      0.99      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51957\n",
            "   macro avg       0.99      0.99      0.99     51957\n",
            "weighted avg       0.99      0.99      0.99     51957\n",
            " samples avg       0.96      0.96      0.96     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0103, Accuracy: 0.9654, Precision: 0.9889, Recall: 0.9873, F1-score: 0.9880\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/15], Batch [50/4502], Loss: 0.0146\n",
            "Epoch [4/15], Batch [100/4502], Loss: 0.0162\n",
            "Epoch [4/15], Batch [150/4502], Loss: 0.0063\n",
            "Epoch [4/15], Batch [200/4502], Loss: 0.0071\n",
            "Epoch [4/15], Batch [250/4502], Loss: 0.0248\n",
            "Epoch [4/15], Batch [300/4502], Loss: 0.0042\n",
            "Epoch [4/15], Batch [350/4502], Loss: 0.0116\n",
            "Epoch [4/15], Batch [400/4502], Loss: 0.0113\n",
            "Epoch [4/15], Batch [450/4502], Loss: 0.0291\n",
            "Epoch [4/15], Batch [500/4502], Loss: 0.0028\n",
            "Epoch [4/15], Batch [550/4502], Loss: 0.0065\n",
            "Epoch [4/15], Batch [600/4502], Loss: 0.0023\n",
            "Epoch [4/15], Batch [650/4502], Loss: 0.0075\n",
            "Epoch [4/15], Batch [700/4502], Loss: 0.0021\n",
            "Epoch [4/15], Batch [750/4502], Loss: 0.0048\n",
            "Epoch [4/15], Batch [800/4502], Loss: 0.0013\n",
            "Epoch [4/15], Batch [850/4502], Loss: 0.0078\n",
            "Epoch [4/15], Batch [900/4502], Loss: 0.0042\n",
            "Epoch [4/15], Batch [950/4502], Loss: 0.0101\n",
            "Epoch [4/15], Batch [1000/4502], Loss: 0.0242\n",
            "Epoch [4/15], Batch [1050/4502], Loss: 0.0054\n",
            "Epoch [4/15], Batch [1100/4502], Loss: 0.0034\n",
            "Epoch [4/15], Batch [1150/4502], Loss: 0.0126\n",
            "Epoch [4/15], Batch [1200/4502], Loss: 0.0021\n",
            "Epoch [4/15], Batch [1250/4502], Loss: 0.0023\n",
            "Epoch [4/15], Batch [1300/4502], Loss: 0.0049\n",
            "Epoch [4/15], Batch [1350/4502], Loss: 0.0066\n",
            "Epoch [4/15], Batch [1400/4502], Loss: 0.0065\n",
            "Epoch [4/15], Batch [1450/4502], Loss: 0.0007\n",
            "Epoch [4/15], Batch [1500/4502], Loss: 0.0020\n",
            "Epoch [4/15], Batch [1550/4502], Loss: 0.0167\n",
            "Epoch [4/15], Batch [1600/4502], Loss: 0.0018\n",
            "Epoch [4/15], Batch [1650/4502], Loss: 0.0279\n",
            "Epoch [4/15], Batch [1700/4502], Loss: 0.0034\n",
            "Epoch [4/15], Batch [1750/4502], Loss: 0.0072\n",
            "Epoch [4/15], Batch [1800/4502], Loss: 0.0022\n",
            "Epoch [4/15], Batch [1850/4502], Loss: 0.0134\n",
            "Epoch [4/15], Batch [1900/4502], Loss: 0.0111\n",
            "Epoch [4/15], Batch [1950/4502], Loss: 0.0096\n",
            "Epoch [4/15], Batch [2000/4502], Loss: 0.0144\n",
            "Epoch [4/15], Batch [2050/4502], Loss: 0.0071\n",
            "Epoch [4/15], Batch [2100/4502], Loss: 0.0102\n",
            "Epoch [4/15], Batch [2150/4502], Loss: 0.0201\n",
            "Epoch [4/15], Batch [2200/4502], Loss: 0.0042\n",
            "Epoch [4/15], Batch [2250/4502], Loss: 0.0028\n",
            "Epoch [4/15], Batch [2300/4502], Loss: 0.0041\n",
            "Epoch [4/15], Batch [2350/4502], Loss: 0.0157\n",
            "Epoch [4/15], Batch [2400/4502], Loss: 0.0078\n",
            "Epoch [4/15], Batch [2450/4502], Loss: 0.0059\n",
            "Epoch [4/15], Batch [2500/4502], Loss: 0.0247\n",
            "Epoch [4/15], Batch [2550/4502], Loss: 0.0114\n",
            "Epoch [4/15], Batch [2600/4502], Loss: 0.0238\n",
            "Epoch [4/15], Batch [2650/4502], Loss: 0.0199\n",
            "Epoch [4/15], Batch [2700/4502], Loss: 0.0024\n",
            "Epoch [4/15], Batch [2750/4502], Loss: 0.0094\n",
            "Epoch [4/15], Batch [2800/4502], Loss: 0.0030\n",
            "Epoch [4/15], Batch [2850/4502], Loss: 0.0014\n",
            "Epoch [4/15], Batch [2900/4502], Loss: 0.0224\n",
            "Epoch [4/15], Batch [2950/4502], Loss: 0.0067\n",
            "Epoch [4/15], Batch [3000/4502], Loss: 0.0142\n",
            "Epoch [4/15], Batch [3050/4502], Loss: 0.0016\n",
            "Epoch [4/15], Batch [3100/4502], Loss: 0.0047\n",
            "Epoch [4/15], Batch [3150/4502], Loss: 0.0030\n",
            "Epoch [4/15], Batch [3200/4502], Loss: 0.0153\n",
            "Epoch [4/15], Batch [3250/4502], Loss: 0.0035\n",
            "Epoch [4/15], Batch [3300/4502], Loss: 0.0058\n",
            "Epoch [4/15], Batch [3350/4502], Loss: 0.0062\n",
            "Epoch [4/15], Batch [3400/4502], Loss: 0.0060\n",
            "Epoch [4/15], Batch [3450/4502], Loss: 0.0116\n",
            "Epoch [4/15], Batch [3500/4502], Loss: 0.0045\n",
            "Epoch [4/15], Batch [3550/4502], Loss: 0.0185\n",
            "Epoch [4/15], Batch [3600/4502], Loss: 0.0044\n",
            "Epoch [4/15], Batch [3650/4502], Loss: 0.0070\n",
            "Epoch [4/15], Batch [3700/4502], Loss: 0.0292\n",
            "Epoch [4/15], Batch [3750/4502], Loss: 0.0022\n",
            "Epoch [4/15], Batch [3800/4502], Loss: 0.0027\n",
            "Epoch [4/15], Batch [3850/4502], Loss: 0.0076\n",
            "Epoch [4/15], Batch [3900/4502], Loss: 0.0148\n",
            "Epoch [4/15], Batch [3950/4502], Loss: 0.0157\n",
            "Epoch [4/15], Batch [4000/4502], Loss: 0.0074\n",
            "Epoch [4/15], Batch [4050/4502], Loss: 0.0058\n",
            "Epoch [4/15], Batch [4100/4502], Loss: 0.0027\n",
            "Epoch [4/15], Batch [4150/4502], Loss: 0.0006\n",
            "Epoch [4/15], Batch [4200/4502], Loss: 0.0010\n",
            "Epoch [4/15], Batch [4250/4502], Loss: 0.0033\n",
            "Epoch [4/15], Batch [4300/4502], Loss: 0.0055\n",
            "Epoch [4/15], Batch [4350/4502], Loss: 0.0057\n",
            "Epoch [4/15], Batch [4400/4502], Loss: 0.0150\n",
            "Epoch [4/15], Batch [4450/4502], Loss: 0.0038\n",
            "Epoch [4/15], Batch [4500/4502], Loss: 0.0072\n",
            "Epoch 4/15, Loss: 0.00952040468007307\n",
            "Updated Learning Rate: [5.4117987787750416e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.99      0.99      3012\n",
            "  distortion       1.00      0.99      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      0.97      0.98      5271\n",
            "      phaser       1.00      0.97      0.98      4518\n",
            "     flanger       0.94      1.00      0.97      3012\n",
            "      chorus       0.93      1.00      0.96      5271\n",
            "       delay       1.00      0.82      0.90      7530\n",
            " hall_reverb       0.92      0.79      0.85      4518\n",
            "plate_reverb       0.66      1.00      0.79      3012\n",
            "     octaver       0.96      1.00      0.98      2259\n",
            " auto_filter       0.98      1.00      0.99      3765\n",
            "\n",
            "   micro avg       0.95      0.95      0.95     51957\n",
            "   macro avg       0.95      0.96      0.95     51957\n",
            "weighted avg       0.96      0.95      0.95     51957\n",
            " samples avg       0.91      0.92      0.91     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0450, Accuracy: 0.8797, Precision: 0.9492, Recall: 0.9596, F1-score: 0.9500\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/15], Batch [50/4502], Loss: 0.0209\n",
            "Epoch [5/15], Batch [100/4502], Loss: 0.0030\n",
            "Epoch [5/15], Batch [150/4502], Loss: 0.0207\n",
            "Epoch [5/15], Batch [200/4502], Loss: 0.0313\n",
            "Epoch [5/15], Batch [250/4502], Loss: 0.0025\n",
            "Epoch [5/15], Batch [300/4502], Loss: 0.0023\n",
            "Epoch [5/15], Batch [350/4502], Loss: 0.0195\n",
            "Epoch [5/15], Batch [400/4502], Loss: 0.0072\n",
            "Epoch [5/15], Batch [450/4502], Loss: 0.0030\n",
            "Epoch [5/15], Batch [500/4502], Loss: 0.0262\n",
            "Epoch [5/15], Batch [550/4502], Loss: 0.0128\n",
            "Epoch [5/15], Batch [600/4502], Loss: 0.0052\n",
            "Epoch [5/15], Batch [650/4502], Loss: 0.0199\n",
            "Epoch [5/15], Batch [700/4502], Loss: 0.0079\n",
            "Epoch [5/15], Batch [750/4502], Loss: 0.0092\n",
            "Epoch [5/15], Batch [800/4502], Loss: 0.0041\n",
            "Epoch [5/15], Batch [850/4502], Loss: 0.0010\n",
            "Epoch [5/15], Batch [900/4502], Loss: 0.0226\n",
            "Epoch [5/15], Batch [950/4502], Loss: 0.0021\n",
            "Epoch [5/15], Batch [1000/4502], Loss: 0.0011\n",
            "Epoch [5/15], Batch [1050/4502], Loss: 0.0038\n",
            "Epoch [5/15], Batch [1100/4502], Loss: 0.0010\n",
            "Epoch [5/15], Batch [1150/4502], Loss: 0.0009\n",
            "Epoch [5/15], Batch [1200/4502], Loss: 0.0032\n",
            "Epoch [5/15], Batch [1250/4502], Loss: 0.0094\n",
            "Epoch [5/15], Batch [1300/4502], Loss: 0.0016\n",
            "Epoch [5/15], Batch [1350/4502], Loss: 0.0270\n",
            "Epoch [5/15], Batch [1400/4502], Loss: 0.0328\n",
            "Epoch [5/15], Batch [1450/4502], Loss: 0.0218\n",
            "Epoch [5/15], Batch [1500/4502], Loss: 0.0102\n",
            "Epoch [5/15], Batch [1550/4502], Loss: 0.0181\n",
            "Epoch [5/15], Batch [1600/4502], Loss: 0.0238\n",
            "Epoch [5/15], Batch [1650/4502], Loss: 0.0206\n",
            "Epoch [5/15], Batch [1700/4502], Loss: 0.0325\n",
            "Epoch [5/15], Batch [1750/4502], Loss: 0.0019\n",
            "Epoch [5/15], Batch [1800/4502], Loss: 0.0020\n",
            "Epoch [5/15], Batch [1850/4502], Loss: 0.0066\n",
            "Epoch [5/15], Batch [1900/4502], Loss: 0.0048\n",
            "Epoch [5/15], Batch [1950/4502], Loss: 0.0168\n",
            "Epoch [5/15], Batch [2000/4502], Loss: 0.0047\n",
            "Epoch [5/15], Batch [2050/4502], Loss: 0.0005\n",
            "Epoch [5/15], Batch [2100/4502], Loss: 0.0063\n",
            "Epoch [5/15], Batch [2150/4502], Loss: 0.0257\n",
            "Epoch [5/15], Batch [2200/4502], Loss: 0.0070\n",
            "Epoch [5/15], Batch [2250/4502], Loss: 0.0043\n",
            "Epoch [5/15], Batch [2300/4502], Loss: 0.0006\n",
            "Epoch [5/15], Batch [2350/4502], Loss: 0.0078\n",
            "Epoch [5/15], Batch [2400/4502], Loss: 0.0059\n",
            "Epoch [5/15], Batch [2450/4502], Loss: 0.0013\n",
            "Epoch [5/15], Batch [2500/4502], Loss: 0.0166\n",
            "Epoch [5/15], Batch [2550/4502], Loss: 0.0111\n",
            "Epoch [5/15], Batch [2600/4502], Loss: 0.0043\n",
            "Epoch [5/15], Batch [2650/4502], Loss: 0.0236\n",
            "Epoch [5/15], Batch [2700/4502], Loss: 0.0030\n",
            "Epoch [5/15], Batch [2750/4502], Loss: 0.0038\n",
            "Epoch [5/15], Batch [2800/4502], Loss: 0.0013\n",
            "Epoch [5/15], Batch [2850/4502], Loss: 0.0047\n",
            "Epoch [5/15], Batch [2900/4502], Loss: 0.0087\n",
            "Epoch [5/15], Batch [2950/4502], Loss: 0.0011\n",
            "Epoch [5/15], Batch [3000/4502], Loss: 0.0367\n",
            "Epoch [5/15], Batch [3050/4502], Loss: 0.0019\n",
            "Epoch [5/15], Batch [3100/4502], Loss: 0.0014\n",
            "Epoch [5/15], Batch [3150/4502], Loss: 0.0017\n",
            "Epoch [5/15], Batch [3200/4502], Loss: 0.0120\n",
            "Epoch [5/15], Batch [3250/4502], Loss: 0.0012\n",
            "Epoch [5/15], Batch [3300/4502], Loss: 0.0044\n",
            "Epoch [5/15], Batch [3350/4502], Loss: 0.0050\n",
            "Epoch [5/15], Batch [3400/4502], Loss: 0.0022\n",
            "Epoch [5/15], Batch [3450/4502], Loss: 0.0097\n",
            "Epoch [5/15], Batch [3500/4502], Loss: 0.0070\n",
            "Epoch [5/15], Batch [3550/4502], Loss: 0.0058\n",
            "Epoch [5/15], Batch [3600/4502], Loss: 0.0240\n",
            "Epoch [5/15], Batch [3650/4502], Loss: 0.0124\n",
            "Epoch [5/15], Batch [3700/4502], Loss: 0.0011\n",
            "Epoch [5/15], Batch [3750/4502], Loss: 0.0454\n",
            "Epoch [5/15], Batch [3800/4502], Loss: 0.0092\n",
            "Epoch [5/15], Batch [3850/4502], Loss: 0.0013\n",
            "Epoch [5/15], Batch [3900/4502], Loss: 0.0027\n",
            "Epoch [5/15], Batch [3950/4502], Loss: 0.0023\n",
            "Epoch [5/15], Batch [4000/4502], Loss: 0.0095\n",
            "Epoch [5/15], Batch [4050/4502], Loss: 0.0003\n",
            "Epoch [5/15], Batch [4100/4502], Loss: 0.0066\n",
            "Epoch [5/15], Batch [4150/4502], Loss: 0.0012\n",
            "Epoch [5/15], Batch [4200/4502], Loss: 0.0006\n",
            "Epoch [5/15], Batch [4250/4502], Loss: 0.0036\n",
            "Epoch [5/15], Batch [4300/4502], Loss: 0.0060\n",
            "Epoch [5/15], Batch [4350/4502], Loss: 0.0090\n",
            "Epoch [5/15], Batch [4400/4502], Loss: 0.0009\n",
            "Epoch [5/15], Batch [4450/4502], Loss: 0.0021\n",
            "Epoch [5/15], Batch [4500/4502], Loss: 0.0016\n",
            "Epoch 5/15, Loss: 0.007536866460638029\n",
            "Updated Learning Rate: [4.641699812555353e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      0.99      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.97      0.99      0.98      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.97      0.99      0.98      7530\n",
            " hall_reverb       0.96      0.97      0.96      4518\n",
            "plate_reverb       0.96      0.99      0.98      3012\n",
            "     octaver       1.00      0.98      0.99      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51957\n",
            "   macro avg       0.99      0.99      0.99     51957\n",
            "weighted avg       0.99      0.99      0.99     51957\n",
            " samples avg       0.96      0.97      0.96     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0089, Accuracy: 0.9660, Precision: 0.9868, Recall: 0.9911, F1-score: 0.9889\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/15], Batch [50/4502], Loss: 0.0028\n",
            "Epoch [6/15], Batch [100/4502], Loss: 0.0043\n",
            "Epoch [6/15], Batch [150/4502], Loss: 0.0075\n",
            "Epoch [6/15], Batch [200/4502], Loss: 0.0037\n",
            "Epoch [6/15], Batch [250/4502], Loss: 0.0188\n",
            "Epoch [6/15], Batch [300/4502], Loss: 0.0029\n",
            "Epoch [6/15], Batch [350/4502], Loss: 0.0030\n",
            "Epoch [6/15], Batch [400/4502], Loss: 0.0122\n",
            "Epoch [6/15], Batch [450/4502], Loss: 0.0004\n",
            "Epoch [6/15], Batch [500/4502], Loss: 0.0022\n",
            "Epoch [6/15], Batch [550/4502], Loss: 0.0008\n",
            "Epoch [6/15], Batch [600/4502], Loss: 0.0048\n",
            "Epoch [6/15], Batch [650/4502], Loss: 0.0022\n",
            "Epoch [6/15], Batch [700/4502], Loss: 0.0275\n",
            "Epoch [6/15], Batch [750/4502], Loss: 0.0035\n",
            "Epoch [6/15], Batch [800/4502], Loss: 0.0051\n",
            "Epoch [6/15], Batch [850/4502], Loss: 0.0178\n",
            "Epoch [6/15], Batch [900/4502], Loss: 0.0079\n",
            "Epoch [6/15], Batch [950/4502], Loss: 0.0058\n",
            "Epoch [6/15], Batch [1000/4502], Loss: 0.0103\n",
            "Epoch [6/15], Batch [1050/4502], Loss: 0.0013\n",
            "Epoch [6/15], Batch [1100/4502], Loss: 0.0018\n",
            "Epoch [6/15], Batch [1150/4502], Loss: 0.0295\n",
            "Epoch [6/15], Batch [1200/4502], Loss: 0.0036\n",
            "Epoch [6/15], Batch [1250/4502], Loss: 0.0197\n",
            "Epoch [6/15], Batch [1300/4502], Loss: 0.0099\n",
            "Epoch [6/15], Batch [1350/4502], Loss: 0.0030\n",
            "Epoch [6/15], Batch [1400/4502], Loss: 0.0030\n",
            "Epoch [6/15], Batch [1450/4502], Loss: 0.0017\n",
            "Epoch [6/15], Batch [1500/4502], Loss: 0.0038\n",
            "Epoch [6/15], Batch [1550/4502], Loss: 0.0118\n",
            "Epoch [6/15], Batch [1600/4502], Loss: 0.0003\n",
            "Epoch [6/15], Batch [1650/4502], Loss: 0.0034\n",
            "Epoch [6/15], Batch [1700/4502], Loss: 0.0014\n",
            "Epoch [6/15], Batch [1750/4502], Loss: 0.0057\n",
            "Epoch [6/15], Batch [1800/4502], Loss: 0.0025\n",
            "Epoch [6/15], Batch [1850/4502], Loss: 0.0002\n",
            "Epoch [6/15], Batch [1900/4502], Loss: 0.0035\n",
            "Epoch [6/15], Batch [1950/4502], Loss: 0.0032\n",
            "Epoch [6/15], Batch [2000/4502], Loss: 0.0145\n",
            "Epoch [6/15], Batch [2050/4502], Loss: 0.0105\n",
            "Epoch [6/15], Batch [2100/4502], Loss: 0.0056\n",
            "Epoch [6/15], Batch [2150/4502], Loss: 0.0006\n",
            "Epoch [6/15], Batch [2200/4502], Loss: 0.0101\n",
            "Epoch [6/15], Batch [2250/4502], Loss: 0.0006\n",
            "Epoch [6/15], Batch [2300/4502], Loss: 0.0015\n",
            "Epoch [6/15], Batch [2350/4502], Loss: 0.0079\n",
            "Epoch [6/15], Batch [2400/4502], Loss: 0.0034\n",
            "Epoch [6/15], Batch [2450/4502], Loss: 0.0049\n",
            "Epoch [6/15], Batch [2500/4502], Loss: 0.0006\n",
            "Epoch [6/15], Batch [2550/4502], Loss: 0.0037\n",
            "Epoch [6/15], Batch [2600/4502], Loss: 0.0048\n",
            "Epoch [6/15], Batch [2650/4502], Loss: 0.0069\n",
            "Epoch [6/15], Batch [2700/4502], Loss: 0.0260\n",
            "Epoch [6/15], Batch [2750/4502], Loss: 0.0012\n",
            "Epoch [6/15], Batch [2800/4502], Loss: 0.0016\n",
            "Epoch [6/15], Batch [2850/4502], Loss: 0.0058\n",
            "Epoch [6/15], Batch [2900/4502], Loss: 0.0023\n",
            "Epoch [6/15], Batch [2950/4502], Loss: 0.0013\n",
            "Epoch [6/15], Batch [3000/4502], Loss: 0.0024\n",
            "Epoch [6/15], Batch [3050/4502], Loss: 0.0010\n",
            "Epoch [6/15], Batch [3100/4502], Loss: 0.0024\n",
            "Epoch [6/15], Batch [3150/4502], Loss: 0.0083\n",
            "Epoch [6/15], Batch [3200/4502], Loss: 0.0032\n",
            "Epoch [6/15], Batch [3250/4502], Loss: 0.0048\n",
            "Epoch [6/15], Batch [3300/4502], Loss: 0.0017\n",
            "Epoch [6/15], Batch [3350/4502], Loss: 0.0033\n",
            "Epoch [6/15], Batch [3400/4502], Loss: 0.0068\n",
            "Epoch [6/15], Batch [3450/4502], Loss: 0.0023\n",
            "Epoch [6/15], Batch [3500/4502], Loss: 0.0155\n",
            "Epoch [6/15], Batch [3550/4502], Loss: 0.0007\n",
            "Epoch [6/15], Batch [3600/4502], Loss: 0.0037\n",
            "Epoch [6/15], Batch [3650/4502], Loss: 0.0204\n",
            "Epoch [6/15], Batch [3700/4502], Loss: 0.0153\n",
            "Epoch [6/15], Batch [3750/4502], Loss: 0.0066\n",
            "Epoch [6/15], Batch [3800/4502], Loss: 0.0205\n",
            "Epoch [6/15], Batch [3850/4502], Loss: 0.0002\n",
            "Epoch [6/15], Batch [3900/4502], Loss: 0.0065\n",
            "Epoch [6/15], Batch [3950/4502], Loss: 0.0046\n",
            "Epoch [6/15], Batch [4000/4502], Loss: 0.0075\n",
            "Epoch [6/15], Batch [4050/4502], Loss: 0.0039\n",
            "Epoch [6/15], Batch [4100/4502], Loss: 0.0019\n",
            "Epoch [6/15], Batch [4150/4502], Loss: 0.0027\n",
            "Epoch [6/15], Batch [4200/4502], Loss: 0.0074\n",
            "Epoch [6/15], Batch [4250/4502], Loss: 0.0081\n",
            "Epoch [6/15], Batch [4300/4502], Loss: 0.0289\n",
            "Epoch [6/15], Batch [4350/4502], Loss: 0.0094\n",
            "Epoch [6/15], Batch [4400/4502], Loss: 0.0039\n",
            "Epoch [6/15], Batch [4450/4502], Loss: 0.0084\n",
            "Epoch [6/15], Batch [4500/4502], Loss: 0.0165\n",
            "Epoch 6/15, Loss: 0.006387951004267615\n",
            "Updated Learning Rate: [3.9811859292287264e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.99      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      0.99      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.98      0.99      0.98      3012\n",
            "      chorus       0.99      1.00      1.00      5271\n",
            "       delay       0.99      0.96      0.98      7530\n",
            " hall_reverb       0.89      0.95      0.91      4518\n",
            "plate_reverb       0.81      1.00      0.89      3012\n",
            "     octaver       1.00      0.99      0.99      2259\n",
            " auto_filter       1.00      0.99      1.00      3765\n",
            "\n",
            "   micro avg       0.97      0.99      0.98     51957\n",
            "   macro avg       0.97      0.99      0.98     51957\n",
            "weighted avg       0.97      0.99      0.98     51957\n",
            " samples avg       0.95      0.96      0.95     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0177, Accuracy: 0.9417, Precision: 0.9705, Recall: 0.9879, F1-score: 0.9783\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/15], Batch [50/4502], Loss: 0.0009\n",
            "Epoch [7/15], Batch [100/4502], Loss: 0.0052\n",
            "Epoch [7/15], Batch [150/4502], Loss: 0.0018\n",
            "Epoch [7/15], Batch [200/4502], Loss: 0.0002\n",
            "Epoch [7/15], Batch [250/4502], Loss: 0.0065\n",
            "Epoch [7/15], Batch [300/4502], Loss: 0.0026\n",
            "Epoch [7/15], Batch [350/4502], Loss: 0.0037\n",
            "Epoch [7/15], Batch [400/4502], Loss: 0.0011\n",
            "Epoch [7/15], Batch [450/4502], Loss: 0.0018\n",
            "Epoch [7/15], Batch [500/4502], Loss: 0.0021\n",
            "Epoch [7/15], Batch [550/4502], Loss: 0.0031\n",
            "Epoch [7/15], Batch [600/4502], Loss: 0.0088\n",
            "Epoch [7/15], Batch [650/4502], Loss: 0.0005\n",
            "Epoch [7/15], Batch [700/4502], Loss: 0.0013\n",
            "Epoch [7/15], Batch [750/4502], Loss: 0.0048\n",
            "Epoch [7/15], Batch [800/4502], Loss: 0.0025\n",
            "Epoch [7/15], Batch [850/4502], Loss: 0.0005\n",
            "Epoch [7/15], Batch [900/4502], Loss: 0.0031\n",
            "Epoch [7/15], Batch [950/4502], Loss: 0.0078\n",
            "Epoch [7/15], Batch [1000/4502], Loss: 0.0173\n",
            "Epoch [7/15], Batch [1050/4502], Loss: 0.0026\n",
            "Epoch [7/15], Batch [1100/4502], Loss: 0.0101\n",
            "Epoch [7/15], Batch [1150/4502], Loss: 0.0032\n",
            "Epoch [7/15], Batch [1200/4502], Loss: 0.0028\n",
            "Epoch [7/15], Batch [1250/4502], Loss: 0.0014\n",
            "Epoch [7/15], Batch [1300/4502], Loss: 0.0083\n",
            "Epoch [7/15], Batch [1350/4502], Loss: 0.0076\n",
            "Epoch [7/15], Batch [1400/4502], Loss: 0.0029\n",
            "Epoch [7/15], Batch [1450/4502], Loss: 0.0069\n",
            "Epoch [7/15], Batch [1500/4502], Loss: 0.0019\n",
            "Epoch [7/15], Batch [1550/4502], Loss: 0.0003\n",
            "Epoch [7/15], Batch [1600/4502], Loss: 0.0019\n",
            "Epoch [7/15], Batch [1650/4502], Loss: 0.0156\n",
            "Epoch [7/15], Batch [1700/4502], Loss: 0.0014\n",
            "Epoch [7/15], Batch [1750/4502], Loss: 0.0041\n",
            "Epoch [7/15], Batch [1800/4502], Loss: 0.0071\n",
            "Epoch [7/15], Batch [1850/4502], Loss: 0.0056\n",
            "Epoch [7/15], Batch [1900/4502], Loss: 0.0012\n",
            "Epoch [7/15], Batch [1950/4502], Loss: 0.0006\n",
            "Epoch [7/15], Batch [2000/4502], Loss: 0.0014\n",
            "Epoch [7/15], Batch [2050/4502], Loss: 0.0006\n",
            "Epoch [7/15], Batch [2100/4502], Loss: 0.0013\n",
            "Epoch [7/15], Batch [2150/4502], Loss: 0.0002\n",
            "Epoch [7/15], Batch [2200/4502], Loss: 0.0119\n",
            "Epoch [7/15], Batch [2250/4502], Loss: 0.0040\n",
            "Epoch [7/15], Batch [2300/4502], Loss: 0.0002\n",
            "Epoch [7/15], Batch [2350/4502], Loss: 0.0067\n",
            "Epoch [7/15], Batch [2400/4502], Loss: 0.0018\n",
            "Epoch [7/15], Batch [2450/4502], Loss: 0.0021\n",
            "Epoch [7/15], Batch [2500/4502], Loss: 0.0060\n",
            "Epoch [7/15], Batch [2550/4502], Loss: 0.0013\n",
            "Epoch [7/15], Batch [2600/4502], Loss: 0.0010\n",
            "Epoch [7/15], Batch [2650/4502], Loss: 0.0057\n",
            "Epoch [7/15], Batch [2700/4502], Loss: 0.0033\n",
            "Epoch [7/15], Batch [2750/4502], Loss: 0.0010\n",
            "Epoch [7/15], Batch [2800/4502], Loss: 0.0011\n",
            "Epoch [7/15], Batch [2850/4502], Loss: 0.0028\n",
            "Epoch [7/15], Batch [2900/4502], Loss: 0.0017\n",
            "Epoch [7/15], Batch [2950/4502], Loss: 0.0066\n",
            "Epoch [7/15], Batch [3000/4502], Loss: 0.0059\n",
            "Epoch [7/15], Batch [3050/4502], Loss: 0.0010\n",
            "Epoch [7/15], Batch [3100/4502], Loss: 0.0018\n",
            "Epoch [7/15], Batch [3150/4502], Loss: 0.0087\n",
            "Epoch [7/15], Batch [3200/4502], Loss: 0.0047\n",
            "Epoch [7/15], Batch [3250/4502], Loss: 0.0142\n",
            "Epoch [7/15], Batch [3300/4502], Loss: 0.0039\n",
            "Epoch [7/15], Batch [3350/4502], Loss: 0.0018\n",
            "Epoch [7/15], Batch [3400/4502], Loss: 0.0003\n",
            "Epoch [7/15], Batch [3450/4502], Loss: 0.0138\n",
            "Epoch [7/15], Batch [3500/4502], Loss: 0.0224\n",
            "Epoch [7/15], Batch [3550/4502], Loss: 0.0039\n",
            "Epoch [7/15], Batch [3600/4502], Loss: 0.0072\n",
            "Epoch [7/15], Batch [3650/4502], Loss: 0.0097\n",
            "Epoch [7/15], Batch [3700/4502], Loss: 0.0238\n",
            "Epoch [7/15], Batch [3750/4502], Loss: 0.0121\n",
            "Epoch [7/15], Batch [3800/4502], Loss: 0.0003\n",
            "Epoch [7/15], Batch [3850/4502], Loss: 0.0048\n",
            "Epoch [7/15], Batch [3900/4502], Loss: 0.0020\n",
            "Epoch [7/15], Batch [3950/4502], Loss: 0.0027\n",
            "Epoch [7/15], Batch [4000/4502], Loss: 0.0026\n",
            "Epoch [7/15], Batch [4050/4502], Loss: 0.0017\n",
            "Epoch [7/15], Batch [4100/4502], Loss: 0.0089\n",
            "Epoch [7/15], Batch [4150/4502], Loss: 0.0033\n",
            "Epoch [7/15], Batch [4200/4502], Loss: 0.0309\n",
            "Epoch [7/15], Batch [4250/4502], Loss: 0.0033\n",
            "Epoch [7/15], Batch [4300/4502], Loss: 0.0025\n",
            "Epoch [7/15], Batch [4350/4502], Loss: 0.0008\n",
            "Epoch [7/15], Batch [4400/4502], Loss: 0.0016\n",
            "Epoch [7/15], Batch [4450/4502], Loss: 0.0008\n",
            "Epoch [7/15], Batch [4500/4502], Loss: 0.0008\n",
            "Epoch 7/15, Loss: 0.00524718830000065\n",
            "Updated Learning Rate: [3.414663171499479e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       0.99      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.90      1.00      0.95      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       1.00      0.96      0.98      7530\n",
            " hall_reverb       0.96      0.98      0.97      4518\n",
            "plate_reverb       0.97      0.98      0.97      3012\n",
            "     octaver       0.93      1.00      0.96      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.98      0.99      0.99     51957\n",
            "   macro avg       0.98      0.99      0.98     51957\n",
            "weighted avg       0.98      0.99      0.99     51957\n",
            " samples avg       0.96      0.96      0.96     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0118, Accuracy: 0.9604, Precision: 0.9786, Recall: 0.9917, F1-score: 0.9847\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/15], Batch [50/4502], Loss: 0.0058\n",
            "Epoch [8/15], Batch [100/4502], Loss: 0.0010\n",
            "Epoch [8/15], Batch [150/4502], Loss: 0.0154\n",
            "Epoch [8/15], Batch [200/4502], Loss: 0.0041\n",
            "Epoch [8/15], Batch [250/4502], Loss: 0.0123\n",
            "Epoch [8/15], Batch [300/4502], Loss: 0.0041\n",
            "Epoch [8/15], Batch [350/4502], Loss: 0.0036\n",
            "Epoch [8/15], Batch [400/4502], Loss: 0.0016\n",
            "Epoch [8/15], Batch [450/4502], Loss: 0.0002\n",
            "Epoch [8/15], Batch [500/4502], Loss: 0.0010\n",
            "Epoch [8/15], Batch [550/4502], Loss: 0.0012\n",
            "Epoch [8/15], Batch [600/4502], Loss: 0.0003\n",
            "Epoch [8/15], Batch [650/4502], Loss: 0.0038\n",
            "Epoch [8/15], Batch [700/4502], Loss: 0.0037\n",
            "Epoch [8/15], Batch [750/4502], Loss: 0.0017\n",
            "Epoch [8/15], Batch [800/4502], Loss: 0.0005\n",
            "Epoch [8/15], Batch [850/4502], Loss: 0.0005\n",
            "Epoch [8/15], Batch [900/4502], Loss: 0.0009\n",
            "Epoch [8/15], Batch [950/4502], Loss: 0.0034\n",
            "Epoch [8/15], Batch [1000/4502], Loss: 0.0077\n",
            "Epoch [8/15], Batch [1050/4502], Loss: 0.0003\n",
            "Epoch [8/15], Batch [1100/4502], Loss: 0.0031\n",
            "Epoch [8/15], Batch [1150/4502], Loss: 0.0061\n",
            "Epoch [8/15], Batch [1200/4502], Loss: 0.0038\n",
            "Epoch [8/15], Batch [1250/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [1300/4502], Loss: 0.0014\n",
            "Epoch [8/15], Batch [1350/4502], Loss: 0.0008\n",
            "Epoch [8/15], Batch [1400/4502], Loss: 0.0003\n",
            "Epoch [8/15], Batch [1450/4502], Loss: 0.0027\n",
            "Epoch [8/15], Batch [1500/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [1550/4502], Loss: 0.0018\n",
            "Epoch [8/15], Batch [1600/4502], Loss: 0.0005\n",
            "Epoch [8/15], Batch [1650/4502], Loss: 0.0029\n",
            "Epoch [8/15], Batch [1700/4502], Loss: 0.0043\n",
            "Epoch [8/15], Batch [1750/4502], Loss: 0.0082\n",
            "Epoch [8/15], Batch [1800/4502], Loss: 0.0014\n",
            "Epoch [8/15], Batch [1850/4502], Loss: 0.0047\n",
            "Epoch [8/15], Batch [1900/4502], Loss: 0.0113\n",
            "Epoch [8/15], Batch [1950/4502], Loss: 0.0014\n",
            "Epoch [8/15], Batch [2000/4502], Loss: 0.0054\n",
            "Epoch [8/15], Batch [2050/4502], Loss: 0.0011\n",
            "Epoch [8/15], Batch [2100/4502], Loss: 0.0047\n",
            "Epoch [8/15], Batch [2150/4502], Loss: 0.0083\n",
            "Epoch [8/15], Batch [2200/4502], Loss: 0.0018\n",
            "Epoch [8/15], Batch [2250/4502], Loss: 0.0103\n",
            "Epoch [8/15], Batch [2300/4502], Loss: 0.0064\n",
            "Epoch [8/15], Batch [2350/4502], Loss: 0.0099\n",
            "Epoch [8/15], Batch [2400/4502], Loss: 0.0007\n",
            "Epoch [8/15], Batch [2450/4502], Loss: 0.0010\n",
            "Epoch [8/15], Batch [2500/4502], Loss: 0.0015\n",
            "Epoch [8/15], Batch [2550/4502], Loss: 0.0018\n",
            "Epoch [8/15], Batch [2600/4502], Loss: 0.0024\n",
            "Epoch [8/15], Batch [2650/4502], Loss: 0.0071\n",
            "Epoch [8/15], Batch [2700/4502], Loss: 0.0001\n",
            "Epoch [8/15], Batch [2750/4502], Loss: 0.0017\n",
            "Epoch [8/15], Batch [2800/4502], Loss: 0.0168\n",
            "Epoch [8/15], Batch [2850/4502], Loss: 0.0104\n",
            "Epoch [8/15], Batch [2900/4502], Loss: 0.0222\n",
            "Epoch [8/15], Batch [2950/4502], Loss: 0.0022\n",
            "Epoch [8/15], Batch [3000/4502], Loss: 0.0078\n",
            "Epoch [8/15], Batch [3050/4502], Loss: 0.0006\n",
            "Epoch [8/15], Batch [3100/4502], Loss: 0.0053\n",
            "Epoch [8/15], Batch [3150/4502], Loss: 0.0011\n",
            "Epoch [8/15], Batch [3200/4502], Loss: 0.0016\n",
            "Epoch [8/15], Batch [3250/4502], Loss: 0.0041\n",
            "Epoch [8/15], Batch [3300/4502], Loss: 0.0141\n",
            "Epoch [8/15], Batch [3350/4502], Loss: 0.0018\n",
            "Epoch [8/15], Batch [3400/4502], Loss: 0.0012\n",
            "Epoch [8/15], Batch [3450/4502], Loss: 0.0054\n",
            "Epoch [8/15], Batch [3500/4502], Loss: 0.0110\n",
            "Epoch [8/15], Batch [3550/4502], Loss: 0.0003\n",
            "Epoch [8/15], Batch [3600/4502], Loss: 0.0009\n",
            "Epoch [8/15], Batch [3650/4502], Loss: 0.0459\n",
            "Epoch [8/15], Batch [3700/4502], Loss: 0.0050\n",
            "Epoch [8/15], Batch [3750/4502], Loss: 0.0141\n",
            "Epoch [8/15], Batch [3800/4502], Loss: 0.0027\n",
            "Epoch [8/15], Batch [3850/4502], Loss: 0.0003\n",
            "Epoch [8/15], Batch [3900/4502], Loss: 0.0034\n",
            "Epoch [8/15], Batch [3950/4502], Loss: 0.0101\n",
            "Epoch [8/15], Batch [4000/4502], Loss: 0.0052\n",
            "Epoch [8/15], Batch [4050/4502], Loss: 0.0066\n",
            "Epoch [8/15], Batch [4100/4502], Loss: 0.0028\n",
            "Epoch [8/15], Batch [4150/4502], Loss: 0.0010\n",
            "Epoch [8/15], Batch [4200/4502], Loss: 0.0071\n",
            "Epoch [8/15], Batch [4250/4502], Loss: 0.0034\n",
            "Epoch [8/15], Batch [4300/4502], Loss: 0.0091\n",
            "Epoch [8/15], Batch [4350/4502], Loss: 0.0003\n",
            "Epoch [8/15], Batch [4400/4502], Loss: 0.0002\n",
            "Epoch [8/15], Batch [4450/4502], Loss: 0.0081\n",
            "Epoch [8/15], Batch [4500/4502], Loss: 0.0029\n",
            "Epoch 8/15, Loss: 0.004675214725428523\n",
            "Updated Learning Rate: [2.9287566021951033e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.98      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.97      0.98      7530\n",
            " hall_reverb       0.80      0.98      0.88      4518\n",
            "plate_reverb       0.88      1.00      0.93      3012\n",
            "     octaver       1.00      0.99      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.97      0.99      0.98     51957\n",
            "   macro avg       0.97      0.99      0.98     51957\n",
            "weighted avg       0.97      0.99      0.98     51957\n",
            " samples avg       0.95      0.97      0.95     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0172, Accuracy: 0.9394, Precision: 0.9721, Recall: 0.9928, F1-score: 0.9813\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/15], Batch [50/4502], Loss: 0.0141\n",
            "Epoch [9/15], Batch [100/4502], Loss: 0.0014\n",
            "Epoch [9/15], Batch [150/4502], Loss: 0.0118\n",
            "Epoch [9/15], Batch [200/4502], Loss: 0.0004\n",
            "Epoch [9/15], Batch [250/4502], Loss: 0.0053\n",
            "Epoch [9/15], Batch [300/4502], Loss: 0.0075\n",
            "Epoch [9/15], Batch [350/4502], Loss: 0.0083\n",
            "Epoch [9/15], Batch [400/4502], Loss: 0.0004\n",
            "Epoch [9/15], Batch [450/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [500/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [550/4502], Loss: 0.0031\n",
            "Epoch [9/15], Batch [600/4502], Loss: 0.0089\n",
            "Epoch [9/15], Batch [650/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [700/4502], Loss: 0.0010\n",
            "Epoch [9/15], Batch [750/4502], Loss: 0.0008\n",
            "Epoch [9/15], Batch [800/4502], Loss: 0.0014\n",
            "Epoch [9/15], Batch [850/4502], Loss: 0.0006\n",
            "Epoch [9/15], Batch [900/4502], Loss: 0.0011\n",
            "Epoch [9/15], Batch [950/4502], Loss: 0.0005\n",
            "Epoch [9/15], Batch [1000/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [1050/4502], Loss: 0.0002\n",
            "Epoch [9/15], Batch [1100/4502], Loss: 0.0020\n",
            "Epoch [9/15], Batch [1150/4502], Loss: 0.0005\n",
            "Epoch [9/15], Batch [1200/4502], Loss: 0.0070\n",
            "Epoch [9/15], Batch [1250/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [1300/4502], Loss: 0.0120\n",
            "Epoch [9/15], Batch [1350/4502], Loss: 0.0098\n",
            "Epoch [9/15], Batch [1400/4502], Loss: 0.0023\n",
            "Epoch [9/15], Batch [1450/4502], Loss: 0.0006\n",
            "Epoch [9/15], Batch [1500/4502], Loss: 0.0223\n",
            "Epoch [9/15], Batch [1550/4502], Loss: 0.0291\n",
            "Epoch [9/15], Batch [1600/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [1650/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [1700/4502], Loss: 0.0038\n",
            "Epoch [9/15], Batch [1750/4502], Loss: 0.0035\n",
            "Epoch [9/15], Batch [1800/4502], Loss: 0.0042\n",
            "Epoch [9/15], Batch [1850/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [1900/4502], Loss: 0.0048\n",
            "Epoch [9/15], Batch [1950/4502], Loss: 0.0148\n",
            "Epoch [9/15], Batch [2000/4502], Loss: 0.0211\n",
            "Epoch [9/15], Batch [2050/4502], Loss: 0.0028\n",
            "Epoch [9/15], Batch [2100/4502], Loss: 0.0009\n",
            "Epoch [9/15], Batch [2150/4502], Loss: 0.0050\n",
            "Epoch [9/15], Batch [2200/4502], Loss: 0.0004\n",
            "Epoch [9/15], Batch [2250/4502], Loss: 0.0015\n",
            "Epoch [9/15], Batch [2300/4502], Loss: 0.0036\n",
            "Epoch [9/15], Batch [2350/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [2400/4502], Loss: 0.0051\n",
            "Epoch [9/15], Batch [2450/4502], Loss: 0.0087\n",
            "Epoch [9/15], Batch [2500/4502], Loss: 0.0038\n",
            "Epoch [9/15], Batch [2550/4502], Loss: 0.0031\n",
            "Epoch [9/15], Batch [2600/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [2650/4502], Loss: 0.0002\n",
            "Epoch [9/15], Batch [2700/4502], Loss: 0.0025\n",
            "Epoch [9/15], Batch [2750/4502], Loss: 0.0047\n",
            "Epoch [9/15], Batch [2800/4502], Loss: 0.0005\n",
            "Epoch [9/15], Batch [2850/4502], Loss: 0.0043\n",
            "Epoch [9/15], Batch [2900/4502], Loss: 0.0014\n",
            "Epoch [9/15], Batch [2950/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [3000/4502], Loss: 0.0041\n",
            "Epoch [9/15], Batch [3050/4502], Loss: 0.0017\n",
            "Epoch [9/15], Batch [3100/4502], Loss: 0.0018\n",
            "Epoch [9/15], Batch [3150/4502], Loss: 0.0014\n",
            "Epoch [9/15], Batch [3200/4502], Loss: 0.0253\n",
            "Epoch [9/15], Batch [3250/4502], Loss: 0.0052\n",
            "Epoch [9/15], Batch [3300/4502], Loss: 0.0005\n",
            "Epoch [9/15], Batch [3350/4502], Loss: 0.0045\n",
            "Epoch [9/15], Batch [3400/4502], Loss: 0.0057\n",
            "Epoch [9/15], Batch [3450/4502], Loss: 0.0006\n",
            "Epoch [9/15], Batch [3500/4502], Loss: 0.0018\n",
            "Epoch [9/15], Batch [3550/4502], Loss: 0.0023\n",
            "Epoch [9/15], Batch [3600/4502], Loss: 0.0004\n",
            "Epoch [9/15], Batch [3650/4502], Loss: 0.0058\n",
            "Epoch [9/15], Batch [3700/4502], Loss: 0.0105\n",
            "Epoch [9/15], Batch [3750/4502], Loss: 0.0042\n",
            "Epoch [9/15], Batch [3800/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [3850/4502], Loss: 0.0137\n",
            "Epoch [9/15], Batch [3900/4502], Loss: 0.0011\n",
            "Epoch [9/15], Batch [3950/4502], Loss: 0.0056\n",
            "Epoch [9/15], Batch [4000/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [4050/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [4100/4502], Loss: 0.0005\n",
            "Epoch [9/15], Batch [4150/4502], Loss: 0.0010\n",
            "Epoch [9/15], Batch [4200/4502], Loss: 0.0102\n",
            "Epoch [9/15], Batch [4250/4502], Loss: 0.0162\n",
            "Epoch [9/15], Batch [4300/4502], Loss: 0.0023\n",
            "Epoch [9/15], Batch [4350/4502], Loss: 0.0194\n",
            "Epoch [9/15], Batch [4400/4502], Loss: 0.0017\n",
            "Epoch [9/15], Batch [4450/4502], Loss: 0.0042\n",
            "Epoch [9/15], Batch [4500/4502], Loss: 0.0003\n",
            "Epoch 9/15, Loss: 0.004045105335762956\n",
            "Updated Learning Rate: [2.5119945377027402e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.97      0.98      7530\n",
            " hall_reverb       0.98      0.96      0.97      4518\n",
            "plate_reverb       0.96      0.99      0.98      3012\n",
            "     octaver       0.97      1.00      0.99      2259\n",
            " auto_filter       0.99      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51957\n",
            "   macro avg       0.99      0.99      0.99     51957\n",
            "weighted avg       0.99      0.99      0.99     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0074, Accuracy: 0.9782, Precision: 0.9912, Recall: 0.9921, F1-score: 0.9916\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/15], Batch [50/4502], Loss: 0.0031\n",
            "Epoch [10/15], Batch [100/4502], Loss: 0.0053\n",
            "Epoch [10/15], Batch [150/4502], Loss: 0.0057\n",
            "Epoch [10/15], Batch [200/4502], Loss: 0.0023\n",
            "Epoch [10/15], Batch [250/4502], Loss: 0.0020\n",
            "Epoch [10/15], Batch [300/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [350/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [400/4502], Loss: 0.0063\n",
            "Epoch [10/15], Batch [450/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [500/4502], Loss: 0.0012\n",
            "Epoch [10/15], Batch [550/4502], Loss: 0.0087\n",
            "Epoch [10/15], Batch [600/4502], Loss: 0.0012\n",
            "Epoch [10/15], Batch [650/4502], Loss: 0.0039\n",
            "Epoch [10/15], Batch [700/4502], Loss: 0.0050\n",
            "Epoch [10/15], Batch [750/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [800/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [850/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [900/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [950/4502], Loss: 0.0002\n",
            "Epoch [10/15], Batch [1000/4502], Loss: 0.0002\n",
            "Epoch [10/15], Batch [1050/4502], Loss: 0.0096\n",
            "Epoch [10/15], Batch [1100/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [1150/4502], Loss: 0.0074\n",
            "Epoch [10/15], Batch [1200/4502], Loss: 0.0140\n",
            "Epoch [10/15], Batch [1250/4502], Loss: 0.0008\n",
            "Epoch [10/15], Batch [1300/4502], Loss: 0.0015\n",
            "Epoch [10/15], Batch [1350/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [1400/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [1450/4502], Loss: 0.0000\n",
            "Epoch [10/15], Batch [1500/4502], Loss: 0.0030\n",
            "Epoch [10/15], Batch [1550/4502], Loss: 0.0005\n",
            "Epoch [10/15], Batch [1600/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [1650/4502], Loss: 0.0020\n",
            "Epoch [10/15], Batch [1700/4502], Loss: 0.0213\n",
            "Epoch [10/15], Batch [1750/4502], Loss: 0.0011\n",
            "Epoch [10/15], Batch [1800/4502], Loss: 0.0049\n",
            "Epoch [10/15], Batch [1850/4502], Loss: 0.0031\n",
            "Epoch [10/15], Batch [1900/4502], Loss: 0.0013\n",
            "Epoch [10/15], Batch [1950/4502], Loss: 0.0014\n",
            "Epoch [10/15], Batch [2000/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [2050/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [2100/4502], Loss: 0.0057\n",
            "Epoch [10/15], Batch [2150/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [2200/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [2250/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [2300/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [2350/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [2400/4502], Loss: 0.0069\n",
            "Epoch [10/15], Batch [2450/4502], Loss: 0.0021\n",
            "Epoch [10/15], Batch [2500/4502], Loss: 0.0011\n",
            "Epoch [10/15], Batch [2550/4502], Loss: 0.0018\n",
            "Epoch [10/15], Batch [2600/4502], Loss: 0.0023\n",
            "Epoch [10/15], Batch [2650/4502], Loss: 0.0121\n",
            "Epoch [10/15], Batch [2700/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [2750/4502], Loss: 0.0039\n",
            "Epoch [10/15], Batch [2800/4502], Loss: 0.0027\n",
            "Epoch [10/15], Batch [2850/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [2900/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [2950/4502], Loss: 0.0041\n",
            "Epoch [10/15], Batch [3000/4502], Loss: 0.0086\n",
            "Epoch [10/15], Batch [3050/4502], Loss: 0.0023\n",
            "Epoch [10/15], Batch [3100/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [3150/4502], Loss: 0.0014\n",
            "Epoch [10/15], Batch [3200/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [3250/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [3300/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [3350/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [3400/4502], Loss: 0.0096\n",
            "Epoch [10/15], Batch [3450/4502], Loss: 0.0045\n",
            "Epoch [10/15], Batch [3500/4502], Loss: 0.0015\n",
            "Epoch [10/15], Batch [3550/4502], Loss: 0.0019\n",
            "Epoch [10/15], Batch [3600/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [3650/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [3700/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [3750/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [3800/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [3850/4502], Loss: 0.0005\n",
            "Epoch [10/15], Batch [3900/4502], Loss: 0.0028\n",
            "Epoch [10/15], Batch [3950/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [4000/4502], Loss: 0.0029\n",
            "Epoch [10/15], Batch [4050/4502], Loss: 0.0002\n",
            "Epoch [10/15], Batch [4100/4502], Loss: 0.0046\n",
            "Epoch [10/15], Batch [4150/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [4200/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [4250/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [4300/4502], Loss: 0.0089\n",
            "Epoch [10/15], Batch [4350/4502], Loss: 0.0181\n",
            "Epoch [10/15], Batch [4400/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [4450/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [4500/4502], Loss: 0.0040\n",
            "Epoch 10/15, Loss: 0.003652154008055414\n",
            "Updated Learning Rate: [2.1545377149876404e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.99      0.99      7530\n",
            " hall_reverb       0.97      0.99      0.98      4518\n",
            "plate_reverb       0.98      0.99      0.98      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      1.00      0.99     51957\n",
            "   macro avg       0.99      1.00      0.99     51957\n",
            "weighted avg       0.99      1.00      0.99     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0042, Accuracy: 0.9840, Precision: 0.9936, Recall: 0.9961, F1-score: 0.9948\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/15], Batch [50/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [100/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [150/4502], Loss: 0.0022\n",
            "Epoch [11/15], Batch [200/4502], Loss: 0.0040\n",
            "Epoch [11/15], Batch [250/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [300/4502], Loss: 0.0007\n",
            "Epoch [11/15], Batch [350/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [400/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [450/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [500/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [550/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [600/4502], Loss: 0.0032\n",
            "Epoch [11/15], Batch [650/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [700/4502], Loss: 0.0026\n",
            "Epoch [11/15], Batch [750/4502], Loss: 0.0010\n",
            "Epoch [11/15], Batch [800/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [850/4502], Loss: 0.0021\n",
            "Epoch [11/15], Batch [900/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [950/4502], Loss: 0.0084\n",
            "Epoch [11/15], Batch [1000/4502], Loss: 0.0022\n",
            "Epoch [11/15], Batch [1050/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [1100/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [1150/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [1200/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [1250/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [1300/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [1350/4502], Loss: 0.0047\n",
            "Epoch [11/15], Batch [1400/4502], Loss: 0.0015\n",
            "Epoch [11/15], Batch [1450/4502], Loss: 0.0116\n",
            "Epoch [11/15], Batch [1500/4502], Loss: 0.0055\n",
            "Epoch [11/15], Batch [1550/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [1600/4502], Loss: 0.0015\n",
            "Epoch [11/15], Batch [1650/4502], Loss: 0.0037\n",
            "Epoch [11/15], Batch [1700/4502], Loss: 0.0037\n",
            "Epoch [11/15], Batch [1750/4502], Loss: 0.0015\n",
            "Epoch [11/15], Batch [1800/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [1850/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [1900/4502], Loss: 0.0058\n",
            "Epoch [11/15], Batch [1950/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [2000/4502], Loss: 0.0024\n",
            "Epoch [11/15], Batch [2050/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [2100/4502], Loss: 0.0034\n",
            "Epoch [11/15], Batch [2150/4502], Loss: 0.0009\n",
            "Epoch [11/15], Batch [2200/4502], Loss: 0.0022\n",
            "Epoch [11/15], Batch [2250/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [2300/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [2350/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [2400/4502], Loss: 0.0018\n",
            "Epoch [11/15], Batch [2450/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [2500/4502], Loss: 0.0028\n",
            "Epoch [11/15], Batch [2550/4502], Loss: 0.0043\n",
            "Epoch [11/15], Batch [2600/4502], Loss: 0.0013\n",
            "Epoch [11/15], Batch [2650/4502], Loss: 0.0077\n",
            "Epoch [11/15], Batch [2700/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [2750/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [2800/4502], Loss: 0.0008\n",
            "Epoch [11/15], Batch [2850/4502], Loss: 0.0016\n",
            "Epoch [11/15], Batch [2900/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [2950/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [3000/4502], Loss: 0.0010\n",
            "Epoch [11/15], Batch [3050/4502], Loss: 0.0035\n",
            "Epoch [11/15], Batch [3100/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [3150/4502], Loss: 0.0033\n",
            "Epoch [11/15], Batch [3200/4502], Loss: 0.0099\n",
            "Epoch [11/15], Batch [3250/4502], Loss: 0.0008\n",
            "Epoch [11/15], Batch [3300/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [3350/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [3400/4502], Loss: 0.0007\n",
            "Epoch [11/15], Batch [3450/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [3500/4502], Loss: 0.0026\n",
            "Epoch [11/15], Batch [3550/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [3600/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [3650/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [3700/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [3750/4502], Loss: 0.0025\n",
            "Epoch [11/15], Batch [3800/4502], Loss: 0.0090\n",
            "Epoch [11/15], Batch [3850/4502], Loss: 0.0024\n",
            "Epoch [11/15], Batch [3900/4502], Loss: 0.0028\n",
            "Epoch [11/15], Batch [3950/4502], Loss: 0.0055\n",
            "Epoch [11/15], Batch [4000/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [4050/4502], Loss: 0.0085\n",
            "Epoch [11/15], Batch [4100/4502], Loss: 0.0050\n",
            "Epoch [11/15], Batch [4150/4502], Loss: 0.0148\n",
            "Epoch [11/15], Batch [4200/4502], Loss: 0.0135\n",
            "Epoch [11/15], Batch [4250/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [4300/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [4350/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [4400/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [4450/4502], Loss: 0.0009\n",
            "Epoch [11/15], Batch [4500/4502], Loss: 0.0038\n",
            "Epoch 11/15, Loss: 0.0033724726156623337\n",
            "Updated Learning Rate: [1.8479469981448992e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.97      0.98      7530\n",
            " hall_reverb       0.97      0.98      0.98      4518\n",
            "plate_reverb       0.96      0.99      0.98      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51957\n",
            "   macro avg       0.99      0.99      0.99     51957\n",
            "weighted avg       0.99      0.99      0.99     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0064, Accuracy: 0.9800, Precision: 0.9927, Recall: 0.9940, F1-score: 0.9933\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/15], Batch [50/4502], Loss: 0.0069\n",
            "Epoch [12/15], Batch [100/4502], Loss: 0.0025\n",
            "Epoch [12/15], Batch [150/4502], Loss: 0.0024\n",
            "Epoch [12/15], Batch [200/4502], Loss: 0.0015\n",
            "Epoch [12/15], Batch [250/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [300/4502], Loss: 0.0016\n",
            "Epoch [12/15], Batch [350/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [400/4502], Loss: 0.0009\n",
            "Epoch [12/15], Batch [450/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [500/4502], Loss: 0.0074\n",
            "Epoch [12/15], Batch [550/4502], Loss: 0.0012\n",
            "Epoch [12/15], Batch [600/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [650/4502], Loss: 0.0122\n",
            "Epoch [12/15], Batch [700/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [750/4502], Loss: 0.0100\n",
            "Epoch [12/15], Batch [800/4502], Loss: 0.0013\n",
            "Epoch [12/15], Batch [850/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [900/4502], Loss: 0.0163\n",
            "Epoch [12/15], Batch [950/4502], Loss: 0.0016\n",
            "Epoch [12/15], Batch [1000/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [1050/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [1100/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [1150/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [1200/4502], Loss: 0.0064\n",
            "Epoch [12/15], Batch [1250/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [1300/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [1350/4502], Loss: 0.0034\n",
            "Epoch [12/15], Batch [1400/4502], Loss: 0.0020\n",
            "Epoch [12/15], Batch [1450/4502], Loss: 0.0047\n",
            "Epoch [12/15], Batch [1500/4502], Loss: 0.0009\n",
            "Epoch [12/15], Batch [1550/4502], Loss: 0.0018\n",
            "Epoch [12/15], Batch [1600/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [1650/4502], Loss: 0.0025\n",
            "Epoch [12/15], Batch [1700/4502], Loss: 0.0120\n",
            "Epoch [12/15], Batch [1750/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [1800/4502], Loss: 0.0028\n",
            "Epoch [12/15], Batch [1850/4502], Loss: 0.0056\n",
            "Epoch [12/15], Batch [1900/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [1950/4502], Loss: 0.0011\n",
            "Epoch [12/15], Batch [2000/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [2050/4502], Loss: 0.0021\n",
            "Epoch [12/15], Batch [2100/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [2150/4502], Loss: 0.0024\n",
            "Epoch [12/15], Batch [2200/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [2250/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [2300/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [2350/4502], Loss: 0.0140\n",
            "Epoch [12/15], Batch [2400/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [2450/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [2500/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [2550/4502], Loss: 0.0019\n",
            "Epoch [12/15], Batch [2600/4502], Loss: 0.0014\n",
            "Epoch [12/15], Batch [2650/4502], Loss: 0.0023\n",
            "Epoch [12/15], Batch [2700/4502], Loss: 0.0090\n",
            "Epoch [12/15], Batch [2750/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [2800/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [2850/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [2900/4502], Loss: 0.0059\n",
            "Epoch [12/15], Batch [2950/4502], Loss: 0.0022\n",
            "Epoch [12/15], Batch [3000/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [3050/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [3100/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [3150/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [3200/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [3250/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [3300/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [3350/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [3400/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [3450/4502], Loss: 0.0021\n",
            "Epoch [12/15], Batch [3500/4502], Loss: 0.0040\n",
            "Epoch [12/15], Batch [3550/4502], Loss: 0.0020\n",
            "Epoch [12/15], Batch [3600/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [3650/4502], Loss: 0.0014\n",
            "Epoch [12/15], Batch [3700/4502], Loss: 0.0006\n",
            "Epoch [12/15], Batch [3750/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [3800/4502], Loss: 0.0022\n",
            "Epoch [12/15], Batch [3850/4502], Loss: 0.0030\n",
            "Epoch [12/15], Batch [3900/4502], Loss: 0.0109\n",
            "Epoch [12/15], Batch [3950/4502], Loss: 0.0161\n",
            "Epoch [12/15], Batch [4000/4502], Loss: 0.0026\n",
            "Epoch [12/15], Batch [4050/4502], Loss: 0.0134\n",
            "Epoch [12/15], Batch [4100/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [4150/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [4200/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [4250/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [4300/4502], Loss: 0.0179\n",
            "Epoch [12/15], Batch [4350/4502], Loss: 0.0006\n",
            "Epoch [12/15], Batch [4400/4502], Loss: 0.0067\n",
            "Epoch [12/15], Batch [4450/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [4500/4502], Loss: 0.0003\n",
            "Epoch 12/15, Loss: 0.0032142128005738305\n",
            "Updated Learning Rate: [1.58498414030888e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       1.00      0.97      0.98      7530\n",
            " hall_reverb       0.98      0.98      0.98      4518\n",
            "plate_reverb       0.95      0.99      0.97      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51957\n",
            "   macro avg       0.99      0.99      0.99     51957\n",
            "weighted avg       0.99      0.99      0.99     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0067, Accuracy: 0.9794, Precision: 0.9914, Recall: 0.9942, F1-score: 0.9927\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/15], Batch [50/4502], Loss: 0.0022\n",
            "Epoch [13/15], Batch [100/4502], Loss: 0.0015\n",
            "Epoch [13/15], Batch [150/4502], Loss: 0.0028\n",
            "Epoch [13/15], Batch [200/4502], Loss: 0.0007\n",
            "Epoch [13/15], Batch [250/4502], Loss: 0.0008\n",
            "Epoch [13/15], Batch [300/4502], Loss: 0.0104\n",
            "Epoch [13/15], Batch [350/4502], Loss: 0.0009\n",
            "Epoch [13/15], Batch [400/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [450/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [500/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [550/4502], Loss: 0.0011\n",
            "Epoch [13/15], Batch [600/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [650/4502], Loss: 0.0010\n",
            "Epoch [13/15], Batch [700/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [750/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [800/4502], Loss: 0.0024\n",
            "Epoch [13/15], Batch [850/4502], Loss: 0.0023\n",
            "Epoch [13/15], Batch [900/4502], Loss: 0.0010\n",
            "Epoch [13/15], Batch [950/4502], Loss: 0.0016\n",
            "Epoch [13/15], Batch [1000/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1050/4502], Loss: 0.0120\n",
            "Epoch [13/15], Batch [1100/4502], Loss: 0.0010\n",
            "Epoch [13/15], Batch [1150/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [1200/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [1250/4502], Loss: 0.0017\n",
            "Epoch [13/15], Batch [1300/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1350/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [1400/4502], Loss: 0.0008\n",
            "Epoch [13/15], Batch [1450/4502], Loss: 0.0131\n",
            "Epoch [13/15], Batch [1500/4502], Loss: 0.0093\n",
            "Epoch [13/15], Batch [1550/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [1600/4502], Loss: 0.0013\n",
            "Epoch [13/15], Batch [1650/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1700/4502], Loss: 0.0086\n",
            "Epoch [13/15], Batch [1750/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [1800/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [1850/4502], Loss: 0.0103\n",
            "Epoch [13/15], Batch [1900/4502], Loss: 0.0128\n",
            "Epoch [13/15], Batch [1950/4502], Loss: 0.0000\n",
            "Epoch [13/15], Batch [2000/4502], Loss: 0.0307\n",
            "Epoch [13/15], Batch [2050/4502], Loss: 0.0121\n",
            "Epoch [13/15], Batch [2100/4502], Loss: 0.0025\n",
            "Epoch [13/15], Batch [2150/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [2200/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [2250/4502], Loss: 0.0007\n",
            "Epoch [13/15], Batch [2300/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [2350/4502], Loss: 0.0023\n",
            "Epoch [13/15], Batch [2400/4502], Loss: 0.0000\n",
            "Epoch [13/15], Batch [2450/4502], Loss: 0.0023\n",
            "Epoch [13/15], Batch [2500/4502], Loss: 0.0223\n",
            "Epoch [13/15], Batch [2550/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [2600/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [2650/4502], Loss: 0.0023\n",
            "Epoch [13/15], Batch [2700/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [2750/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [2800/4502], Loss: 0.0058\n",
            "Epoch [13/15], Batch [2850/4502], Loss: 0.0000\n",
            "Epoch [13/15], Batch [2900/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [2950/4502], Loss: 0.0076\n",
            "Epoch [13/15], Batch [3000/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [3050/4502], Loss: 0.0007\n",
            "Epoch [13/15], Batch [3100/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [3150/4502], Loss: 0.0000\n",
            "Epoch [13/15], Batch [3200/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [3250/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [3300/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [3350/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [3400/4502], Loss: 0.0149\n",
            "Epoch [13/15], Batch [3450/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [3500/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [3550/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [3600/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [3650/4502], Loss: 0.0164\n",
            "Epoch [13/15], Batch [3700/4502], Loss: 0.0018\n",
            "Epoch [13/15], Batch [3750/4502], Loss: 0.0038\n",
            "Epoch [13/15], Batch [3800/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [3850/4502], Loss: 0.0054\n",
            "Epoch [13/15], Batch [3900/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [3950/4502], Loss: 0.0007\n",
            "Epoch [13/15], Batch [4000/4502], Loss: 0.0044\n",
            "Epoch [13/15], Batch [4050/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [4100/4502], Loss: 0.0250\n",
            "Epoch [13/15], Batch [4150/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [4200/4502], Loss: 0.0024\n",
            "Epoch [13/15], Batch [4250/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [4300/4502], Loss: 0.0011\n",
            "Epoch [13/15], Batch [4350/4502], Loss: 0.0004\n",
            "Epoch [13/15], Batch [4400/4502], Loss: 0.0008\n",
            "Epoch [13/15], Batch [4450/4502], Loss: 0.0000\n",
            "Epoch [13/15], Batch [4500/4502], Loss: 0.0012\n",
            "Epoch 13/15, Loss: 0.0028954402515734383\n",
            "Updated Learning Rate: [1.3594408971429265e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.98      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.96      0.99      0.98      7530\n",
            " hall_reverb       0.99      0.96      0.98      4518\n",
            "plate_reverb       0.99      0.98      0.99      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51957\n",
            "   macro avg       0.99      0.99      0.99     51957\n",
            "weighted avg       0.99      0.99      0.99     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0059, Accuracy: 0.9766, Precision: 0.9940, Recall: 0.9926, F1-score: 0.9933\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/15], Batch [50/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [100/4502], Loss: 0.0007\n",
            "Epoch [14/15], Batch [150/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [200/4502], Loss: 0.0092\n",
            "Epoch [14/15], Batch [250/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [300/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [350/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [400/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [450/4502], Loss: 0.0144\n",
            "Epoch [14/15], Batch [500/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [550/4502], Loss: 0.0006\n",
            "Epoch [14/15], Batch [600/4502], Loss: 0.0061\n",
            "Epoch [14/15], Batch [650/4502], Loss: 0.0083\n",
            "Epoch [14/15], Batch [700/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [750/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [800/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [850/4502], Loss: 0.0059\n",
            "Epoch [14/15], Batch [900/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [950/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1000/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [1050/4502], Loss: 0.0011\n",
            "Epoch [14/15], Batch [1100/4502], Loss: 0.0258\n",
            "Epoch [14/15], Batch [1150/4502], Loss: 0.0033\n",
            "Epoch [14/15], Batch [1200/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [1250/4502], Loss: 0.0021\n",
            "Epoch [14/15], Batch [1300/4502], Loss: 0.0183\n",
            "Epoch [14/15], Batch [1350/4502], Loss: 0.0009\n",
            "Epoch [14/15], Batch [1400/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [1450/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1500/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [1550/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [1600/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1650/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1700/4502], Loss: 0.0042\n",
            "Epoch [14/15], Batch [1750/4502], Loss: 0.0019\n",
            "Epoch [14/15], Batch [1800/4502], Loss: 0.0000\n",
            "Epoch [14/15], Batch [1850/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [1900/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1950/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2000/4502], Loss: 0.0006\n",
            "Epoch [14/15], Batch [2050/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [2100/4502], Loss: 0.0020\n",
            "Epoch [14/15], Batch [2150/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2200/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [2250/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2300/4502], Loss: 0.0062\n",
            "Epoch [14/15], Batch [2350/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2400/4502], Loss: 0.0015\n",
            "Epoch [14/15], Batch [2450/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2500/4502], Loss: 0.0045\n",
            "Epoch [14/15], Batch [2550/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [2600/4502], Loss: 0.0054\n",
            "Epoch [14/15], Batch [2650/4502], Loss: 0.0015\n",
            "Epoch [14/15], Batch [2700/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [2750/4502], Loss: 0.0010\n",
            "Epoch [14/15], Batch [2800/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2850/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [2900/4502], Loss: 0.0000\n",
            "Epoch [14/15], Batch [2950/4502], Loss: 0.0000\n",
            "Epoch [14/15], Batch [3000/4502], Loss: 0.0012\n",
            "Epoch [14/15], Batch [3050/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [3100/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [3150/4502], Loss: 0.0010\n",
            "Epoch [14/15], Batch [3200/4502], Loss: 0.0059\n",
            "Epoch [14/15], Batch [3250/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [3300/4502], Loss: 0.0017\n",
            "Epoch [14/15], Batch [3350/4502], Loss: 0.0014\n",
            "Epoch [14/15], Batch [3400/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [3450/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [3500/4502], Loss: 0.0040\n",
            "Epoch [14/15], Batch [3550/4502], Loss: 0.0134\n",
            "Epoch [14/15], Batch [3600/4502], Loss: 0.0059\n",
            "Epoch [14/15], Batch [3650/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [3700/4502], Loss: 0.0327\n",
            "Epoch [14/15], Batch [3750/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [3800/4502], Loss: 0.0068\n",
            "Epoch [14/15], Batch [3850/4502], Loss: 0.0092\n",
            "Epoch [14/15], Batch [3900/4502], Loss: 0.0054\n",
            "Epoch [14/15], Batch [3950/4502], Loss: 0.0009\n",
            "Epoch [14/15], Batch [4000/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [4050/4502], Loss: 0.0007\n",
            "Epoch [14/15], Batch [4100/4502], Loss: 0.0162\n",
            "Epoch [14/15], Batch [4150/4502], Loss: 0.0113\n",
            "Epoch [14/15], Batch [4200/4502], Loss: 0.0009\n",
            "Epoch [14/15], Batch [4250/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [4300/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [4350/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [4400/4502], Loss: 0.0028\n",
            "Epoch [14/15], Batch [4450/4502], Loss: 0.0009\n",
            "Epoch [14/15], Batch [4500/4502], Loss: 0.0000\n",
            "Epoch 14/15, Loss: 0.0026193386070796233\n",
            "Updated Learning Rate: [1.165992457479488e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.99      0.99      7530\n",
            " hall_reverb       0.99      0.98      0.98      4518\n",
            "plate_reverb       0.98      0.99      0.99      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       1.00      0.99      1.00     51957\n",
            "   macro avg       1.00      1.00      1.00     51957\n",
            "weighted avg       1.00      0.99      1.00     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0041, Accuracy: 0.9860, Precision: 0.9953, Recall: 0.9952, F1-score: 0.9953\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/15], Batch [50/4502], Loss: 0.0007\n",
            "Epoch [15/15], Batch [100/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [150/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [200/4502], Loss: 0.0007\n",
            "Epoch [15/15], Batch [250/4502], Loss: 0.0010\n",
            "Epoch [15/15], Batch [300/4502], Loss: 0.0012\n",
            "Epoch [15/15], Batch [350/4502], Loss: 0.0047\n",
            "Epoch [15/15], Batch [400/4502], Loss: 0.0020\n",
            "Epoch [15/15], Batch [450/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [500/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [550/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [600/4502], Loss: 0.0104\n",
            "Epoch [15/15], Batch [650/4502], Loss: 0.0022\n",
            "Epoch [15/15], Batch [700/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [750/4502], Loss: 0.0015\n",
            "Epoch [15/15], Batch [800/4502], Loss: 0.0012\n",
            "Epoch [15/15], Batch [850/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [900/4502], Loss: 0.0041\n",
            "Epoch [15/15], Batch [950/4502], Loss: 0.0007\n",
            "Epoch [15/15], Batch [1000/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1050/4502], Loss: 0.0029\n",
            "Epoch [15/15], Batch [1100/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [1150/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1200/4502], Loss: 0.0030\n",
            "Epoch [15/15], Batch [1250/4502], Loss: 0.0052\n",
            "Epoch [15/15], Batch [1300/4502], Loss: 0.0032\n",
            "Epoch [15/15], Batch [1350/4502], Loss: 0.0011\n",
            "Epoch [15/15], Batch [1400/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [1450/4502], Loss: 0.0070\n",
            "Epoch [15/15], Batch [1500/4502], Loss: 0.0150\n",
            "Epoch [15/15], Batch [1550/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1600/4502], Loss: 0.0010\n",
            "Epoch [15/15], Batch [1650/4502], Loss: 0.0063\n",
            "Epoch [15/15], Batch [1700/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [1750/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [1800/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1850/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [1900/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1950/4502], Loss: 0.0155\n",
            "Epoch [15/15], Batch [2000/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [2050/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [2100/4502], Loss: 0.0016\n",
            "Epoch [15/15], Batch [2150/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [2200/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [2250/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [2300/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2350/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [2400/4502], Loss: 0.0033\n",
            "Epoch [15/15], Batch [2450/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2500/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [2550/4502], Loss: 0.0098\n",
            "Epoch [15/15], Batch [2600/4502], Loss: 0.0014\n",
            "Epoch [15/15], Batch [2650/4502], Loss: 0.0011\n",
            "Epoch [15/15], Batch [2700/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [2750/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [2800/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2850/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [2900/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2950/4502], Loss: 0.0016\n",
            "Epoch [15/15], Batch [3000/4502], Loss: 0.0021\n",
            "Epoch [15/15], Batch [3050/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [3100/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [3150/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [3200/4502], Loss: 0.0026\n",
            "Epoch [15/15], Batch [3250/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [3300/4502], Loss: 0.0030\n",
            "Epoch [15/15], Batch [3350/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [3400/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [3450/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [3500/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [3550/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [3600/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [3650/4502], Loss: 0.0016\n",
            "Epoch [15/15], Batch [3700/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [3750/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [3800/4502], Loss: 0.0025\n",
            "Epoch [15/15], Batch [3850/4502], Loss: 0.0008\n",
            "Epoch [15/15], Batch [3900/4502], Loss: 0.0029\n",
            "Epoch [15/15], Batch [3950/4502], Loss: 0.0041\n",
            "Epoch [15/15], Batch [4000/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [4050/4502], Loss: 0.0007\n",
            "Epoch [15/15], Batch [4100/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [4150/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [4200/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [4250/4502], Loss: 0.0035\n",
            "Epoch [15/15], Batch [4300/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [4350/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [4400/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [4450/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [4500/4502], Loss: 0.0077\n",
            "Epoch 15/15, Loss: 0.002395975591725207\n",
            "Updated Learning Rate: [1.0000717307801568e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5271\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.99      0.99      7530\n",
            " hall_reverb       0.97      0.99      0.98      4518\n",
            "plate_reverb       0.98      0.99      0.99      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      1.00      0.99     51957\n",
            "   macro avg       0.99      1.00      0.99     51957\n",
            "weighted avg       0.99      1.00      0.99     51957\n",
            " samples avg       0.97      0.97      0.97     51957\n",
            "\n",
            "\n",
            "Validation Loss: 0.0045, Accuracy: 0.9840, Precision: 0.9936, Recall: 0.9960, F1-score: 0.9948\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt8.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=6, pin_memory=True)\n",
        "\n",
        "dsp_feature_dim = train_dataset.dsp_features.shape[1]\n",
        "num_classes = len(train_dataset.label_map)\n",
        "\n",
        "model = spectrogramCNN(num_classes, dsp_feature_dim).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8577)  # 0.0001  0.00001 over 15 epochs\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "print_freq = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (spectrograms, dsp_features, labels) in enumerate(train_loader):\n",
        "        spectrograms, dsp_features, labels = spectrograms.to(device), dsp_features.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(spectrograms, dsp_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    print(f\"Updated Learning Rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrograms, dsp_features, labels in val_loader:\n",
        "            spectrograms, dsp_features, labels = spectrograms.to(device), dsp_features.to(device), labels.to(device)\n",
        "            outputs = model(spectrograms, dsp_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
        "\n",
        "            # Store for metric computation\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Convert lists to numpy arrays for metric calculations\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Print classification report\n",
        "    class_names = train_dataset.label_map\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(f\"\\nValidation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "train_losses = np.array(train_losses)\n",
        "val_losses = np.array(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\", marker=\"s\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs. Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "uY2IFHDsktce",
        "outputId": "0768421a-c111-4a42-bd41-0224953e918c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "CGiHpYYikrqv",
        "outputId": "9ec38608-f4a4-40f2-fd27-39020770e8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: NaN values found in DSP features. Replacing with 0.\n",
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Caught KeyError in DataLoader worker process 10.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-11-74b1d14380c9>\", line 75, in __getitem__\n    spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n                               ~~~~~~~~~~~~~~^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/usr/local/lib/python3.11/dist-packages/h5py/_hl/group.py\", line 357, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 257, in h5py.h5o.open\nKeyError: \"Unable to synchronously open object (object 'final_validate_DLY_180ms.wav_seg_1' doesn't exist)\"\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0d7bf83ace34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsp_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsp_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectrograms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsp_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsp_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1453\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 10.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-11-74b1d14380c9>\", line 75, in __getitem__\n    spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n                               ~~~~~~~~~~~~~~^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/usr/local/lib/python3.11/dist-packages/h5py/_hl/group.py\", line 357, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 257, in h5py.h5o.open\nKeyError: \"Unable to synchronously open object (object 'final_validate_DLY_180ms.wav_seg_1' doesn't exist)\"\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "h5_test_path = \"/content/final_datasets/test_extra_TRM_DLY.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_test.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_altX.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "dsp_feature_dim = train_dataset.dsp_features.shape[1]\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes, dsp_feature_dim).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, dsp_features, labels in test_loader:\n",
        "        spectrograms, dsp_features, labels = spectrograms.to(device), dsp_features.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms, dsp_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4M37nJkp9Vq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6979384-af7d-47bd-fe46-db4a18ada933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: NaN values found in DSP features. Replacing with 0.\n",
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n",
            "\n",
            "Test Loss: 0.1079, Accuracy: 0.7873, Precision: 0.8930, Recall: 0.9310, F1-score: 0.9055\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.75      0.86      0.80      3432\n",
            "  distortion       0.99      0.98      0.98      5148\n",
            "        fuzz       1.00      0.95      0.97      6006\n",
            "     tremolo       0.84      0.98      0.90      4290\n",
            "      phaser       1.00      0.92      0.96      5148\n",
            "     flanger       0.99      0.76      0.86      3432\n",
            "      chorus       0.92      0.96      0.94      6006\n",
            "       delay       0.95      0.94      0.94      7722\n",
            " hall_reverb       0.92      0.93      0.92      5148\n",
            "plate_reverb       0.89      0.97      0.93      3432\n",
            "     octaver       0.59      0.99      0.74      2574\n",
            " auto_filter       0.88      0.93      0.90      4290\n",
            "\n",
            "   micro avg       0.90      0.93      0.92     56628\n",
            "   macro avg       0.89      0.93      0.91     56628\n",
            "weighted avg       0.91      0.93      0.92     56628\n",
            " samples avg       0.87      0.89      0.87     56628\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "# h5_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.h5\"\n",
        "# csv_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.csv\"\n",
        "\n",
        "h5_test_path = \"/content/final_datasets/final_real.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_real.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_altX.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "dsp_feature_dim = test_dataset.dsp_features.shape[1]\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes, dsp_feature_dim).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, dsp_features, labels in test_loader:\n",
        "        spectrograms, dsp_features, labels = spectrograms.to(device), dsp_features.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms, dsp_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "v-liaT6r8uXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-QKEy3qc2hc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}