{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbdionne/toneclone/blob/main/spectrogramCNN_alt4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mIYygHe1JGIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa75dd95-b4fc-4d56-e89d-27b75a3588b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/final_datasets\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/test_extra_TRM_DLY.h5\" \"/content/final_datasets/test_extra_TRM_DLY.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/test_extra_TRM_DLY.csv\" \"/content/final_datasets/test_extra_TRM_DLY.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/validate_extra_TRM_DLY.h5\" \"/content/final_datasets/validate_extra_TRM_DLY.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/validate_extra_TRM_DLY.csv\" \"/content/final_datasets/validate_extra_TRM_DLY.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/train_extra_TRM_DLY.h5\" \"/content/final_datasets/train_extra_TRM_DLY.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/train_extra_TRM_DLY.csv\" \"/content/final_datasets/train_extra_TRM_DLY.csv\""
      ],
      "metadata": {
        "id": "7QQiSvhn9atV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hvDlTB85Sraj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import torchaudio.transforms as T\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Downcasting object dtype arrays on .fillna\")\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for spectrogram data with data augmentation.\n",
        "    Includes:\n",
        "    - Random Gaussian noise\n",
        "    - Pitch shifting using torch.roll() with zero-padding (prevents wrapping)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, csv_file, augment=True, noise_level=0.03, pitch_shift_range=(-0.5, 0.5)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_file (str): Path to the HDF5 file containing spectrograms.\n",
        "            csv_file (str): Path to CSV file with labels.\n",
        "            augment (bool): Whether to apply data augmentation.\n",
        "            noise_level (float): Standard deviation of Gaussian noise to add.\n",
        "            pitch_shift_range (tuple): Min/max semitones for pitch shifting.\n",
        "        \"\"\"\n",
        "        self.hdf5_file_path = hdf5_file\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.label_map = self.labels.columns[1:].tolist()  # Get effect label names\n",
        "        self.hdf5_file = None  # Open HDF5 file once per worker\n",
        "\n",
        "        self.augment = augment\n",
        "        self.noise_level = noise_level\n",
        "        self.pitch_shift_range = pitch_shift_range\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Open HDF5 file per worker to avoid threading issues\n",
        "        if self.hdf5_file is None:\n",
        "            self.hdf5_file = h5py.File(self.hdf5_file_path, \"r\", swmr=True)\n",
        "\n",
        "        # Retrieve spectrogram\n",
        "        key = self.labels.iloc[idx]['key']\n",
        "        spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Retrieve labels\n",
        "        label_values = self.labels.iloc[idx][1:].fillna(0).astype(float).values\n",
        "        label = torch.tensor(label_values, dtype=torch.float32)\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            spectrogram = self.add_noise(spectrogram)\n",
        "            spectrogram = self.pitch_shift(spectrogram)\n",
        "\n",
        "        return spectrogram, label\n",
        "\n",
        "    def add_noise(self, spectrogram):\n",
        "        \"\"\"Adds Gaussian noise where noise level is randomly chosen between 0 and self.noise_level.\"\"\"\n",
        "        noise_level = random.uniform(0, self.noise_level)  # Random noise per sample\n",
        "        noise = torch.randn_like(spectrogram) * noise_level  # Scale noise\n",
        "        return spectrogram + noise\n",
        "\n",
        "    def pitch_shift(self, spectrogram):\n",
        "        \"\"\"Shifts spectrogram frequency bins using torch.roll() with zero padding.\"\"\"\n",
        "        semitone_shift = random.uniform(*self.pitch_shift_range)  # Random shift between min/max\n",
        "        shift_bins = int(semitone_shift / 12 * spectrogram.shape[-2])  # Convert semitone shift to frequency bins\n",
        "\n",
        "        # Apply frequency bin shift using torch.roll() with zero-padding\n",
        "        shifted = torch.roll(spectrogram, shifts=shift_bins, dims=-2)  # Shift along frequency axis\n",
        "\n",
        "        if shift_bins > 0:  # Shift up (higher pitch)\n",
        "            shifted[..., :shift_bins, :] = 0  # Zero-pad low frequencies\n",
        "        elif shift_bins < 0:  # Shift down (lower pitch)\n",
        "            shifted[..., shift_bins:, :] = 0  # Zero-pad high frequencies\n",
        "\n",
        "        return shifted\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.hdf5_file is not None:\n",
        "            self.hdf5_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pVGFYMDhDnI9"
      },
      "outputs": [],
      "source": [
        "class spectrogramCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(spectrogramCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x) # Dropout\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p3BlietYUQpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "85fe2ec1-81db-44cc-c3a7-b61667e8e14d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/final_datasets/train_extra_TRM_DLY.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-143dd1346f6c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt4.mod\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpectrogramDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_train_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpectrogramDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_val_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_val_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9bc02692e0f3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hdf5_file, csv_file, augment, noise_level, pitch_shift_range)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \"\"\"\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get effect label names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Open HDF5 file once per worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/final_datasets/train_extra_TRM_DLY.csv'"
          ]
        }
      ],
      "source": [
        "# Initialize dataset from HD5F and csv file\n",
        "\n",
        "h5_train_path = '/content/final_datasets/train_extra_TRM_DLY.h5'\n",
        "csv_train_path = '/content/final_datasets/train_extra_TRM_DLY.csv'\n",
        "\n",
        "h5_val_path = '/content/final_datasets/validate_extra_TRM_DLY.h5'\n",
        "csv_val_path = '/content/final_datasets/validate_extra_TRM_DLY.csv'\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt4.mod\"\n",
        "\n",
        "train_dataset = SpectrogramDataset(h5_train_path, csv_train_path)\n",
        "val_dataset = SpectrogramDataset(h5_val_path, csv_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUEbBB7wX05C"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=6, pin_memory=True)\n",
        "\n",
        "num_classes = len(train_dataset.label_map)\n",
        "\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.63)  # 0.0001 â†’ 0.00001 over 5 epochs\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "print_freq = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    print(f\"Updated Learning Rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrograms, labels in val_loader:\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            outputs = model(spectrograms)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
        "\n",
        "            # Store for metric computation\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Convert lists to numpy arrays for metric calculations\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Print classification report\n",
        "    class_names = train_dataset.label_map\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(f\"\\nValidation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGiHpYYikrqv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "# h5_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_test.h5\"\n",
        "# csv_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_test.csv\"\n",
        "\n",
        "h5_test_path = \"/content/final_datasets/final_test.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_test.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M37nJkp9Vq1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "# h5_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.h5\"\n",
        "# csv_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.csv\"\n",
        "\n",
        "h5_test_path = \"/content/final_datasets/final_real.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_real.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}