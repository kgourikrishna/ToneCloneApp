{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mIYygHe1JGIV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets\" \"/content/final_datasets\""
      ],
      "metadata": {
        "id": "7QQiSvhn9atV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hvDlTB85Sraj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import torchaudio.transforms as T\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Downcasting object dtype arrays on .fillna\")\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for spectrogram data with data augmentation.\n",
        "    Includes:\n",
        "    - Random Gaussian noise\n",
        "    - Pitch shifting using torch.roll() with zero-padding (prevents wrapping)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, csv_file, augment=True, noise_level=0.03, pitch_shift_range=(-0.5, 0.5)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_file (str): Path to the HDF5 file containing spectrograms.\n",
        "            csv_file (str): Path to CSV file with labels.\n",
        "            augment (bool): Whether to apply data augmentation.\n",
        "            noise_level (float): Standard deviation of Gaussian noise to add.\n",
        "            pitch_shift_range (tuple): Min/max semitones for pitch shifting.\n",
        "        \"\"\"\n",
        "        self.hdf5_file_path = hdf5_file\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.label_map = self.labels.columns[1:].tolist()  # Get effect label names\n",
        "        self.hdf5_file = None  # Open HDF5 file once per worker\n",
        "\n",
        "        self.augment = augment\n",
        "        self.noise_level = noise_level\n",
        "        self.pitch_shift_range = pitch_shift_range\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Open HDF5 file per worker to avoid threading issues\n",
        "        if self.hdf5_file is None:\n",
        "            self.hdf5_file = h5py.File(self.hdf5_file_path, \"r\", swmr=True)\n",
        "\n",
        "        # Retrieve spectrogram\n",
        "        key = self.labels.iloc[idx]['key']\n",
        "        spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Retrieve labels\n",
        "        label_values = self.labels.iloc[idx][1:].fillna(0).astype(float).values\n",
        "        label = torch.tensor(label_values, dtype=torch.float32)\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            spectrogram = self.add_noise(spectrogram)\n",
        "            spectrogram = self.pitch_shift(spectrogram)\n",
        "\n",
        "        return spectrogram, label\n",
        "\n",
        "    def add_noise(self, spectrogram):\n",
        "        \"\"\"Adds Gaussian noise where noise level is randomly chosen between 0 and self.noise_level.\"\"\"\n",
        "        noise_level = random.uniform(0, self.noise_level)  # Random noise per sample\n",
        "        noise = torch.randn_like(spectrogram) * noise_level  # Scale noise\n",
        "        return spectrogram + noise\n",
        "\n",
        "    def pitch_shift(self, spectrogram):\n",
        "        \"\"\"Shifts spectrogram frequency bins using torch.roll() with zero padding.\"\"\"\n",
        "        semitone_shift = random.uniform(*self.pitch_shift_range)  # Random shift between min/max\n",
        "        shift_bins = int(semitone_shift / 12 * spectrogram.shape[-2])  # Convert semitone shift to frequency bins\n",
        "\n",
        "        # Apply frequency bin shift using torch.roll() with zero-padding\n",
        "        shifted = torch.roll(spectrogram, shifts=shift_bins, dims=-2)  # Shift along frequency axis\n",
        "\n",
        "        if shift_bins > 0:  # Shift up (higher pitch)\n",
        "            shifted[..., :shift_bins, :] = 0  # Zero-pad low frequencies\n",
        "        elif shift_bins < 0:  # Shift down (lower pitch)\n",
        "            shifted[..., shift_bins:, :] = 0  # Zero-pad high frequencies\n",
        "\n",
        "        return shifted\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.hdf5_file is not None:\n",
        "            self.hdf5_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pVGFYMDhDnI9"
      },
      "outputs": [],
      "source": [
        "class spectrogramCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(spectrogramCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x) # Dropout\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p3BlietYUQpv"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset from HD5F and csv file\n",
        "\n",
        "# h5_train_path = '/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_train.h5'\n",
        "# csv_train_path = '/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_train.csv'\n",
        "\n",
        "# h5_val_path = '/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_validate.h5'\n",
        "# csv_val_path = '/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_validate.csv'\n",
        "\n",
        "h5_train_path = '/content/final_datasets/final_train.h5'\n",
        "csv_train_path = '/content/final_datasets/final_train.csv'\n",
        "\n",
        "h5_val_path = '/content/final_datasets/final_validate.h5'\n",
        "csv_val_path = '/content/final_datasets/final_validate.csv'\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\"\n",
        "\n",
        "train_dataset = SpectrogramDataset(h5_train_path, csv_train_path)\n",
        "val_dataset = SpectrogramDataset(h5_val_path, csv_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUEbBB7wX05C",
        "outputId": "a3d97f86-5138-4787-c46c-27788a9ae1d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [10/4062], Loss: 0.5428\n",
            "Epoch [1/5], Batch [20/4062], Loss: 0.4420\n",
            "Epoch [1/5], Batch [30/4062], Loss: 0.3951\n",
            "Epoch [1/5], Batch [40/4062], Loss: 0.3476\n",
            "Epoch [1/5], Batch [50/4062], Loss: 0.3533\n",
            "Epoch [1/5], Batch [60/4062], Loss: 0.3141\n",
            "Epoch [1/5], Batch [70/4062], Loss: 0.3459\n",
            "Epoch [1/5], Batch [80/4062], Loss: 0.3117\n",
            "Epoch [1/5], Batch [90/4062], Loss: 0.3360\n",
            "Epoch [1/5], Batch [100/4062], Loss: 0.3009\n",
            "Epoch [1/5], Batch [110/4062], Loss: 0.3153\n",
            "Epoch [1/5], Batch [120/4062], Loss: 0.2936\n",
            "Epoch [1/5], Batch [130/4062], Loss: 0.2716\n",
            "Epoch [1/5], Batch [140/4062], Loss: 0.2954\n",
            "Epoch [1/5], Batch [150/4062], Loss: 0.2756\n",
            "Epoch [1/5], Batch [160/4062], Loss: 0.2774\n",
            "Epoch [1/5], Batch [170/4062], Loss: 0.2627\n",
            "Epoch [1/5], Batch [180/4062], Loss: 0.2464\n",
            "Epoch [1/5], Batch [190/4062], Loss: 0.2336\n",
            "Epoch [1/5], Batch [200/4062], Loss: 0.1985\n",
            "Epoch [1/5], Batch [210/4062], Loss: 0.2280\n",
            "Epoch [1/5], Batch [220/4062], Loss: 0.2142\n",
            "Epoch [1/5], Batch [230/4062], Loss: 0.2045\n",
            "Epoch [1/5], Batch [240/4062], Loss: 0.1995\n",
            "Epoch [1/5], Batch [250/4062], Loss: 0.2062\n",
            "Epoch [1/5], Batch [260/4062], Loss: 0.2127\n",
            "Epoch [1/5], Batch [270/4062], Loss: 0.2199\n",
            "Epoch [1/5], Batch [280/4062], Loss: 0.1609\n",
            "Epoch [1/5], Batch [290/4062], Loss: 0.1735\n",
            "Epoch [1/5], Batch [300/4062], Loss: 0.1808\n",
            "Epoch [1/5], Batch [310/4062], Loss: 0.1850\n",
            "Epoch [1/5], Batch [320/4062], Loss: 0.1729\n",
            "Epoch [1/5], Batch [330/4062], Loss: 0.1752\n",
            "Epoch [1/5], Batch [340/4062], Loss: 0.1842\n",
            "Epoch [1/5], Batch [350/4062], Loss: 0.1839\n",
            "Epoch [1/5], Batch [360/4062], Loss: 0.1611\n",
            "Epoch [1/5], Batch [370/4062], Loss: 0.1460\n",
            "Epoch [1/5], Batch [380/4062], Loss: 0.1459\n",
            "Epoch [1/5], Batch [390/4062], Loss: 0.1948\n",
            "Epoch [1/5], Batch [400/4062], Loss: 0.1802\n",
            "Epoch [1/5], Batch [410/4062], Loss: 0.1544\n",
            "Epoch [1/5], Batch [420/4062], Loss: 0.1634\n",
            "Epoch [1/5], Batch [430/4062], Loss: 0.1316\n",
            "Epoch [1/5], Batch [440/4062], Loss: 0.1506\n",
            "Epoch [1/5], Batch [450/4062], Loss: 0.1524\n",
            "Epoch [1/5], Batch [460/4062], Loss: 0.1367\n",
            "Epoch [1/5], Batch [470/4062], Loss: 0.1375\n",
            "Epoch [1/5], Batch [480/4062], Loss: 0.1066\n",
            "Epoch [1/5], Batch [490/4062], Loss: 0.1439\n",
            "Epoch [1/5], Batch [500/4062], Loss: 0.1097\n",
            "Epoch [1/5], Batch [510/4062], Loss: 0.1264\n",
            "Epoch [1/5], Batch [520/4062], Loss: 0.1226\n",
            "Epoch [1/5], Batch [530/4062], Loss: 0.1371\n",
            "Epoch [1/5], Batch [540/4062], Loss: 0.1110\n",
            "Epoch [1/5], Batch [550/4062], Loss: 0.1217\n",
            "Epoch [1/5], Batch [560/4062], Loss: 0.1046\n",
            "Epoch [1/5], Batch [570/4062], Loss: 0.1091\n",
            "Epoch [1/5], Batch [580/4062], Loss: 0.1203\n",
            "Epoch [1/5], Batch [590/4062], Loss: 0.1415\n",
            "Epoch [1/5], Batch [600/4062], Loss: 0.0997\n",
            "Epoch [1/5], Batch [610/4062], Loss: 0.0919\n",
            "Epoch [1/5], Batch [620/4062], Loss: 0.1082\n",
            "Epoch [1/5], Batch [630/4062], Loss: 0.1301\n",
            "Epoch [1/5], Batch [640/4062], Loss: 0.1128\n",
            "Epoch [1/5], Batch [650/4062], Loss: 0.1021\n",
            "Epoch [1/5], Batch [660/4062], Loss: 0.1232\n",
            "Epoch [1/5], Batch [670/4062], Loss: 0.1230\n",
            "Epoch [1/5], Batch [680/4062], Loss: 0.0923\n",
            "Epoch [1/5], Batch [690/4062], Loss: 0.1487\n",
            "Epoch [1/5], Batch [700/4062], Loss: 0.1367\n",
            "Epoch [1/5], Batch [710/4062], Loss: 0.1006\n",
            "Epoch [1/5], Batch [720/4062], Loss: 0.1082\n",
            "Epoch [1/5], Batch [730/4062], Loss: 0.1207\n",
            "Epoch [1/5], Batch [740/4062], Loss: 0.1038\n",
            "Epoch [1/5], Batch [750/4062], Loss: 0.1368\n",
            "Epoch [1/5], Batch [760/4062], Loss: 0.1265\n",
            "Epoch [1/5], Batch [770/4062], Loss: 0.0950\n",
            "Epoch [1/5], Batch [780/4062], Loss: 0.0723\n",
            "Epoch [1/5], Batch [790/4062], Loss: 0.1217\n",
            "Epoch [1/5], Batch [800/4062], Loss: 0.0984\n",
            "Epoch [1/5], Batch [810/4062], Loss: 0.1052\n",
            "Epoch [1/5], Batch [820/4062], Loss: 0.0917\n",
            "Epoch [1/5], Batch [830/4062], Loss: 0.1028\n",
            "Epoch [1/5], Batch [840/4062], Loss: 0.0958\n",
            "Epoch [1/5], Batch [850/4062], Loss: 0.1032\n",
            "Epoch [1/5], Batch [860/4062], Loss: 0.1109\n",
            "Epoch [1/5], Batch [870/4062], Loss: 0.0942\n",
            "Epoch [1/5], Batch [880/4062], Loss: 0.1065\n",
            "Epoch [1/5], Batch [890/4062], Loss: 0.1055\n",
            "Epoch [1/5], Batch [900/4062], Loss: 0.0958\n",
            "Epoch [1/5], Batch [910/4062], Loss: 0.0745\n",
            "Epoch [1/5], Batch [920/4062], Loss: 0.0865\n",
            "Epoch [1/5], Batch [930/4062], Loss: 0.0791\n",
            "Epoch [1/5], Batch [940/4062], Loss: 0.0838\n",
            "Epoch [1/5], Batch [950/4062], Loss: 0.1209\n",
            "Epoch [1/5], Batch [960/4062], Loss: 0.0829\n",
            "Epoch [1/5], Batch [970/4062], Loss: 0.0866\n",
            "Epoch [1/5], Batch [980/4062], Loss: 0.1136\n",
            "Epoch [1/5], Batch [990/4062], Loss: 0.0748\n",
            "Epoch [1/5], Batch [1000/4062], Loss: 0.1001\n",
            "Epoch [1/5], Batch [1010/4062], Loss: 0.0996\n",
            "Epoch [1/5], Batch [1020/4062], Loss: 0.1199\n",
            "Epoch [1/5], Batch [1030/4062], Loss: 0.0825\n",
            "Epoch [1/5], Batch [1040/4062], Loss: 0.0795\n",
            "Epoch [1/5], Batch [1050/4062], Loss: 0.1025\n",
            "Epoch [1/5], Batch [1060/4062], Loss: 0.0723\n",
            "Epoch [1/5], Batch [1070/4062], Loss: 0.0633\n",
            "Epoch [1/5], Batch [1080/4062], Loss: 0.0958\n",
            "Epoch [1/5], Batch [1090/4062], Loss: 0.0705\n",
            "Epoch [1/5], Batch [1100/4062], Loss: 0.0836\n",
            "Epoch [1/5], Batch [1110/4062], Loss: 0.0704\n",
            "Epoch [1/5], Batch [1120/4062], Loss: 0.0644\n",
            "Epoch [1/5], Batch [1130/4062], Loss: 0.0674\n",
            "Epoch [1/5], Batch [1140/4062], Loss: 0.0921\n",
            "Epoch [1/5], Batch [1150/4062], Loss: 0.0993\n",
            "Epoch [1/5], Batch [1160/4062], Loss: 0.0753\n",
            "Epoch [1/5], Batch [1170/4062], Loss: 0.0617\n",
            "Epoch [1/5], Batch [1180/4062], Loss: 0.0621\n",
            "Epoch [1/5], Batch [1190/4062], Loss: 0.0730\n",
            "Epoch [1/5], Batch [1200/4062], Loss: 0.0763\n",
            "Epoch [1/5], Batch [1210/4062], Loss: 0.0541\n",
            "Epoch [1/5], Batch [1220/4062], Loss: 0.0670\n",
            "Epoch [1/5], Batch [1230/4062], Loss: 0.0669\n",
            "Epoch [1/5], Batch [1240/4062], Loss: 0.0691\n",
            "Epoch [1/5], Batch [1250/4062], Loss: 0.0733\n",
            "Epoch [1/5], Batch [1260/4062], Loss: 0.0608\n",
            "Epoch [1/5], Batch [1270/4062], Loss: 0.0704\n",
            "Epoch [1/5], Batch [1280/4062], Loss: 0.1082\n",
            "Epoch [1/5], Batch [1290/4062], Loss: 0.0503\n",
            "Epoch [1/5], Batch [1300/4062], Loss: 0.0572\n",
            "Epoch [1/5], Batch [1310/4062], Loss: 0.0499\n",
            "Epoch [1/5], Batch [1320/4062], Loss: 0.0989\n",
            "Epoch [1/5], Batch [1330/4062], Loss: 0.0755\n",
            "Epoch [1/5], Batch [1340/4062], Loss: 0.0673\n",
            "Epoch [1/5], Batch [1350/4062], Loss: 0.0606\n",
            "Epoch [1/5], Batch [1360/4062], Loss: 0.0854\n",
            "Epoch [1/5], Batch [1370/4062], Loss: 0.0617\n",
            "Epoch [1/5], Batch [1380/4062], Loss: 0.0596\n",
            "Epoch [1/5], Batch [1390/4062], Loss: 0.0649\n",
            "Epoch [1/5], Batch [1400/4062], Loss: 0.0528\n",
            "Epoch [1/5], Batch [1410/4062], Loss: 0.0805\n",
            "Epoch [1/5], Batch [1420/4062], Loss: 0.0747\n",
            "Epoch [1/5], Batch [1430/4062], Loss: 0.1052\n",
            "Epoch [1/5], Batch [1440/4062], Loss: 0.0760\n",
            "Epoch [1/5], Batch [1450/4062], Loss: 0.0684\n",
            "Epoch [1/5], Batch [1460/4062], Loss: 0.0689\n",
            "Epoch [1/5], Batch [1470/4062], Loss: 0.0654\n",
            "Epoch [1/5], Batch [1480/4062], Loss: 0.0528\n",
            "Epoch [1/5], Batch [1490/4062], Loss: 0.0549\n",
            "Epoch [1/5], Batch [1500/4062], Loss: 0.0628\n",
            "Epoch [1/5], Batch [1510/4062], Loss: 0.0627\n",
            "Epoch [1/5], Batch [1520/4062], Loss: 0.0590\n",
            "Epoch [1/5], Batch [1530/4062], Loss: 0.0293\n",
            "Epoch [1/5], Batch [1540/4062], Loss: 0.0804\n",
            "Epoch [1/5], Batch [1550/4062], Loss: 0.0759\n",
            "Epoch [1/5], Batch [1560/4062], Loss: 0.0401\n",
            "Epoch [1/5], Batch [1570/4062], Loss: 0.0574\n",
            "Epoch [1/5], Batch [1580/4062], Loss: 0.0704\n",
            "Epoch [1/5], Batch [1590/4062], Loss: 0.0677\n",
            "Epoch [1/5], Batch [1600/4062], Loss: 0.0668\n",
            "Epoch [1/5], Batch [1610/4062], Loss: 0.0737\n",
            "Epoch [1/5], Batch [1620/4062], Loss: 0.0644\n",
            "Epoch [1/5], Batch [1630/4062], Loss: 0.0849\n",
            "Epoch [1/5], Batch [1640/4062], Loss: 0.0293\n",
            "Epoch [1/5], Batch [1650/4062], Loss: 0.0657\n",
            "Epoch [1/5], Batch [1660/4062], Loss: 0.0397\n",
            "Epoch [1/5], Batch [1670/4062], Loss: 0.0468\n",
            "Epoch [1/5], Batch [1680/4062], Loss: 0.0392\n",
            "Epoch [1/5], Batch [1690/4062], Loss: 0.0572\n",
            "Epoch [1/5], Batch [1700/4062], Loss: 0.0494\n",
            "Epoch [1/5], Batch [1710/4062], Loss: 0.0601\n",
            "Epoch [1/5], Batch [1720/4062], Loss: 0.0571\n",
            "Epoch [1/5], Batch [1730/4062], Loss: 0.0580\n",
            "Epoch [1/5], Batch [1740/4062], Loss: 0.0637\n",
            "Epoch [1/5], Batch [1750/4062], Loss: 0.0516\n",
            "Epoch [1/5], Batch [1760/4062], Loss: 0.0516\n",
            "Epoch [1/5], Batch [1770/4062], Loss: 0.0525\n",
            "Epoch [1/5], Batch [1780/4062], Loss: 0.0802\n",
            "Epoch [1/5], Batch [1790/4062], Loss: 0.0539\n",
            "Epoch [1/5], Batch [1800/4062], Loss: 0.0605\n",
            "Epoch [1/5], Batch [1810/4062], Loss: 0.0552\n",
            "Epoch [1/5], Batch [1820/4062], Loss: 0.0562\n",
            "Epoch [1/5], Batch [1830/4062], Loss: 0.0488\n",
            "Epoch [1/5], Batch [1840/4062], Loss: 0.0614\n",
            "Epoch [1/5], Batch [1850/4062], Loss: 0.0516\n",
            "Epoch [1/5], Batch [1860/4062], Loss: 0.0461\n",
            "Epoch [1/5], Batch [1870/4062], Loss: 0.0720\n",
            "Epoch [1/5], Batch [1880/4062], Loss: 0.0494\n",
            "Epoch [1/5], Batch [1890/4062], Loss: 0.0463\n",
            "Epoch [1/5], Batch [1900/4062], Loss: 0.0330\n",
            "Epoch [1/5], Batch [1910/4062], Loss: 0.0761\n",
            "Epoch [1/5], Batch [1920/4062], Loss: 0.0539\n",
            "Epoch [1/5], Batch [1930/4062], Loss: 0.0894\n",
            "Epoch [1/5], Batch [1940/4062], Loss: 0.0450\n",
            "Epoch [1/5], Batch [1950/4062], Loss: 0.0397\n",
            "Epoch [1/5], Batch [1960/4062], Loss: 0.0609\n",
            "Epoch [1/5], Batch [1970/4062], Loss: 0.0601\n",
            "Epoch [1/5], Batch [1980/4062], Loss: 0.0678\n",
            "Epoch [1/5], Batch [1990/4062], Loss: 0.0463\n",
            "Epoch [1/5], Batch [2000/4062], Loss: 0.0561\n",
            "Epoch [1/5], Batch [2010/4062], Loss: 0.0444\n",
            "Epoch [1/5], Batch [2020/4062], Loss: 0.0542\n",
            "Epoch [1/5], Batch [2030/4062], Loss: 0.0447\n",
            "Epoch [1/5], Batch [2040/4062], Loss: 0.0368\n",
            "Epoch [1/5], Batch [2050/4062], Loss: 0.0561\n",
            "Epoch [1/5], Batch [2060/4062], Loss: 0.0761\n",
            "Epoch [1/5], Batch [2070/4062], Loss: 0.0477\n",
            "Epoch [1/5], Batch [2080/4062], Loss: 0.0584\n",
            "Epoch [1/5], Batch [2090/4062], Loss: 0.0403\n",
            "Epoch [1/5], Batch [2100/4062], Loss: 0.0488\n",
            "Epoch [1/5], Batch [2110/4062], Loss: 0.0629\n",
            "Epoch [1/5], Batch [2120/4062], Loss: 0.0287\n",
            "Epoch [1/5], Batch [2130/4062], Loss: 0.0334\n",
            "Epoch [1/5], Batch [2140/4062], Loss: 0.0605\n",
            "Epoch [1/5], Batch [2150/4062], Loss: 0.0557\n",
            "Epoch [1/5], Batch [2160/4062], Loss: 0.0359\n",
            "Epoch [1/5], Batch [2170/4062], Loss: 0.0421\n",
            "Epoch [1/5], Batch [2180/4062], Loss: 0.0297\n",
            "Epoch [1/5], Batch [2190/4062], Loss: 0.0474\n",
            "Epoch [1/5], Batch [2200/4062], Loss: 0.0475\n",
            "Epoch [1/5], Batch [2210/4062], Loss: 0.0446\n",
            "Epoch [1/5], Batch [2220/4062], Loss: 0.0746\n",
            "Epoch [1/5], Batch [2230/4062], Loss: 0.0384\n",
            "Epoch [1/5], Batch [2240/4062], Loss: 0.0345\n",
            "Epoch [1/5], Batch [2250/4062], Loss: 0.0270\n",
            "Epoch [1/5], Batch [2260/4062], Loss: 0.0366\n",
            "Epoch [1/5], Batch [2270/4062], Loss: 0.0395\n",
            "Epoch [1/5], Batch [2280/4062], Loss: 0.0610\n",
            "Epoch [1/5], Batch [2290/4062], Loss: 0.0650\n",
            "Epoch [1/5], Batch [2300/4062], Loss: 0.0394\n",
            "Epoch [1/5], Batch [2310/4062], Loss: 0.0525\n",
            "Epoch [1/5], Batch [2320/4062], Loss: 0.0459\n",
            "Epoch [1/5], Batch [2330/4062], Loss: 0.0271\n",
            "Epoch [1/5], Batch [2340/4062], Loss: 0.0423\n",
            "Epoch [1/5], Batch [2350/4062], Loss: 0.0471\n",
            "Epoch [1/5], Batch [2360/4062], Loss: 0.0239\n",
            "Epoch [1/5], Batch [2370/4062], Loss: 0.0630\n",
            "Epoch [1/5], Batch [2380/4062], Loss: 0.0494\n",
            "Epoch [1/5], Batch [2390/4062], Loss: 0.0446\n",
            "Epoch [1/5], Batch [2400/4062], Loss: 0.0345\n",
            "Epoch [1/5], Batch [2410/4062], Loss: 0.0329\n",
            "Epoch [1/5], Batch [2420/4062], Loss: 0.0334\n",
            "Epoch [1/5], Batch [2430/4062], Loss: 0.0553\n",
            "Epoch [1/5], Batch [2440/4062], Loss: 0.0454\n",
            "Epoch [1/5], Batch [2450/4062], Loss: 0.0705\n",
            "Epoch [1/5], Batch [2460/4062], Loss: 0.0449\n",
            "Epoch [1/5], Batch [2470/4062], Loss: 0.0411\n",
            "Epoch [1/5], Batch [2480/4062], Loss: 0.0324\n",
            "Epoch [1/5], Batch [2490/4062], Loss: 0.0450\n",
            "Epoch [1/5], Batch [2500/4062], Loss: 0.0470\n",
            "Epoch [1/5], Batch [2510/4062], Loss: 0.0389\n",
            "Epoch [1/5], Batch [2520/4062], Loss: 0.0401\n",
            "Epoch [1/5], Batch [2530/4062], Loss: 0.0494\n",
            "Epoch [1/5], Batch [2540/4062], Loss: 0.0381\n",
            "Epoch [1/5], Batch [2550/4062], Loss: 0.0263\n",
            "Epoch [1/5], Batch [2560/4062], Loss: 0.0539\n",
            "Epoch [1/5], Batch [2570/4062], Loss: 0.0424\n",
            "Epoch [1/5], Batch [2580/4062], Loss: 0.0420\n",
            "Epoch [1/5], Batch [2590/4062], Loss: 0.0398\n",
            "Epoch [1/5], Batch [2600/4062], Loss: 0.0244\n",
            "Epoch [1/5], Batch [2610/4062], Loss: 0.0333\n",
            "Epoch [1/5], Batch [2620/4062], Loss: 0.0452\n",
            "Epoch [1/5], Batch [2630/4062], Loss: 0.0575\n",
            "Epoch [1/5], Batch [2640/4062], Loss: 0.0386\n",
            "Epoch [1/5], Batch [2650/4062], Loss: 0.0405\n",
            "Epoch [1/5], Batch [2660/4062], Loss: 0.0295\n",
            "Epoch [1/5], Batch [2670/4062], Loss: 0.0473\n",
            "Epoch [1/5], Batch [2680/4062], Loss: 0.0332\n",
            "Epoch [1/5], Batch [2690/4062], Loss: 0.0482\n",
            "Epoch [1/5], Batch [2700/4062], Loss: 0.0458\n",
            "Epoch [1/5], Batch [2710/4062], Loss: 0.0406\n",
            "Epoch [1/5], Batch [2720/4062], Loss: 0.0568\n",
            "Epoch [1/5], Batch [2730/4062], Loss: 0.0355\n",
            "Epoch [1/5], Batch [2740/4062], Loss: 0.0478\n",
            "Epoch [1/5], Batch [2750/4062], Loss: 0.0374\n",
            "Epoch [1/5], Batch [2760/4062], Loss: 0.0346\n",
            "Epoch [1/5], Batch [2770/4062], Loss: 0.0481\n",
            "Epoch [1/5], Batch [2780/4062], Loss: 0.0261\n",
            "Epoch [1/5], Batch [2790/4062], Loss: 0.0371\n",
            "Epoch [1/5], Batch [2800/4062], Loss: 0.0583\n",
            "Epoch [1/5], Batch [2810/4062], Loss: 0.0376\n",
            "Epoch [1/5], Batch [2820/4062], Loss: 0.0364\n",
            "Epoch [1/5], Batch [2830/4062], Loss: 0.0266\n",
            "Epoch [1/5], Batch [2840/4062], Loss: 0.0398\n",
            "Epoch [1/5], Batch [2850/4062], Loss: 0.0439\n",
            "Epoch [1/5], Batch [2860/4062], Loss: 0.0323\n",
            "Epoch [1/5], Batch [2870/4062], Loss: 0.0588\n",
            "Epoch [1/5], Batch [2880/4062], Loss: 0.0377\n",
            "Epoch [1/5], Batch [2890/4062], Loss: 0.0360\n",
            "Epoch [1/5], Batch [2900/4062], Loss: 0.0187\n",
            "Epoch [1/5], Batch [2910/4062], Loss: 0.0227\n",
            "Epoch [1/5], Batch [2920/4062], Loss: 0.0487\n",
            "Epoch [1/5], Batch [2930/4062], Loss: 0.0215\n",
            "Epoch [1/5], Batch [2940/4062], Loss: 0.0392\n",
            "Epoch [1/5], Batch [2950/4062], Loss: 0.0568\n",
            "Epoch [1/5], Batch [2960/4062], Loss: 0.0258\n",
            "Epoch [1/5], Batch [2970/4062], Loss: 0.0491\n",
            "Epoch [1/5], Batch [2980/4062], Loss: 0.0283\n",
            "Epoch [1/5], Batch [2990/4062], Loss: 0.0922\n",
            "Epoch [1/5], Batch [3000/4062], Loss: 0.0286\n",
            "Epoch [1/5], Batch [3010/4062], Loss: 0.0367\n",
            "Epoch [1/5], Batch [3020/4062], Loss: 0.0265\n",
            "Epoch [1/5], Batch [3030/4062], Loss: 0.0431\n",
            "Epoch [1/5], Batch [3040/4062], Loss: 0.0608\n",
            "Epoch [1/5], Batch [3050/4062], Loss: 0.0344\n",
            "Epoch [1/5], Batch [3060/4062], Loss: 0.0243\n",
            "Epoch [1/5], Batch [3070/4062], Loss: 0.0414\n",
            "Epoch [1/5], Batch [3080/4062], Loss: 0.0322\n",
            "Epoch [1/5], Batch [3090/4062], Loss: 0.0420\n",
            "Epoch [1/5], Batch [3100/4062], Loss: 0.0243\n",
            "Epoch [1/5], Batch [3110/4062], Loss: 0.0375\n",
            "Epoch [1/5], Batch [3120/4062], Loss: 0.0247\n",
            "Epoch [1/5], Batch [3130/4062], Loss: 0.0339\n",
            "Epoch [1/5], Batch [3140/4062], Loss: 0.0282\n",
            "Epoch [1/5], Batch [3150/4062], Loss: 0.0246\n",
            "Epoch [1/5], Batch [3160/4062], Loss: 0.0243\n",
            "Epoch [1/5], Batch [3170/4062], Loss: 0.0355\n",
            "Epoch [1/5], Batch [3180/4062], Loss: 0.0421\n",
            "Epoch [1/5], Batch [3190/4062], Loss: 0.0266\n",
            "Epoch [1/5], Batch [3200/4062], Loss: 0.0185\n",
            "Epoch [1/5], Batch [3210/4062], Loss: 0.0440\n",
            "Epoch [1/5], Batch [3220/4062], Loss: 0.0226\n",
            "Epoch [1/5], Batch [3230/4062], Loss: 0.0302\n",
            "Epoch [1/5], Batch [3240/4062], Loss: 0.0245\n",
            "Epoch [1/5], Batch [3250/4062], Loss: 0.0343\n",
            "Epoch [1/5], Batch [3260/4062], Loss: 0.0382\n",
            "Epoch [1/5], Batch [3270/4062], Loss: 0.0537\n",
            "Epoch [1/5], Batch [3280/4062], Loss: 0.0371\n",
            "Epoch [1/5], Batch [3290/4062], Loss: 0.0750\n",
            "Epoch [1/5], Batch [3300/4062], Loss: 0.0326\n",
            "Epoch [1/5], Batch [3310/4062], Loss: 0.0328\n",
            "Epoch [1/5], Batch [3320/4062], Loss: 0.0458\n",
            "Epoch [1/5], Batch [3330/4062], Loss: 0.0183\n",
            "Epoch [1/5], Batch [3340/4062], Loss: 0.0187\n",
            "Epoch [1/5], Batch [3350/4062], Loss: 0.0302\n",
            "Epoch [1/5], Batch [3360/4062], Loss: 0.0153\n",
            "Epoch [1/5], Batch [3370/4062], Loss: 0.0356\n",
            "Epoch [1/5], Batch [3380/4062], Loss: 0.0351\n",
            "Epoch [1/5], Batch [3390/4062], Loss: 0.0217\n",
            "Epoch [1/5], Batch [3400/4062], Loss: 0.0228\n",
            "Epoch [1/5], Batch [3410/4062], Loss: 0.0496\n",
            "Epoch [1/5], Batch [3420/4062], Loss: 0.0204\n",
            "Epoch [1/5], Batch [3430/4062], Loss: 0.0418\n",
            "Epoch [1/5], Batch [3440/4062], Loss: 0.0241\n",
            "Epoch [1/5], Batch [3450/4062], Loss: 0.0200\n",
            "Epoch [1/5], Batch [3460/4062], Loss: 0.0220\n",
            "Epoch [1/5], Batch [3470/4062], Loss: 0.0432\n",
            "Epoch [1/5], Batch [3480/4062], Loss: 0.0188\n",
            "Epoch [1/5], Batch [3490/4062], Loss: 0.0198\n",
            "Epoch [1/5], Batch [3500/4062], Loss: 0.0253\n",
            "Epoch [1/5], Batch [3510/4062], Loss: 0.0224\n",
            "Epoch [1/5], Batch [3520/4062], Loss: 0.0445\n",
            "Epoch [1/5], Batch [3530/4062], Loss: 0.0386\n",
            "Epoch [1/5], Batch [3540/4062], Loss: 0.0315\n",
            "Epoch [1/5], Batch [3550/4062], Loss: 0.0628\n",
            "Epoch [1/5], Batch [3560/4062], Loss: 0.0404\n",
            "Epoch [1/5], Batch [3570/4062], Loss: 0.0479\n",
            "Epoch [1/5], Batch [3580/4062], Loss: 0.0351\n",
            "Epoch [1/5], Batch [3590/4062], Loss: 0.0145\n",
            "Epoch [1/5], Batch [3600/4062], Loss: 0.0331\n",
            "Epoch [1/5], Batch [3610/4062], Loss: 0.0165\n",
            "Epoch [1/5], Batch [3620/4062], Loss: 0.0179\n",
            "Epoch [1/5], Batch [3630/4062], Loss: 0.0266\n",
            "Epoch [1/5], Batch [3640/4062], Loss: 0.0472\n",
            "Epoch [1/5], Batch [3650/4062], Loss: 0.0392\n",
            "Epoch [1/5], Batch [3660/4062], Loss: 0.0408\n",
            "Epoch [1/5], Batch [3670/4062], Loss: 0.0479\n",
            "Epoch [1/5], Batch [3680/4062], Loss: 0.0296\n",
            "Epoch [1/5], Batch [3690/4062], Loss: 0.0182\n",
            "Epoch [1/5], Batch [3700/4062], Loss: 0.0153\n",
            "Epoch [1/5], Batch [3710/4062], Loss: 0.0158\n",
            "Epoch [1/5], Batch [3720/4062], Loss: 0.0230\n",
            "Epoch [1/5], Batch [3730/4062], Loss: 0.0204\n",
            "Epoch [1/5], Batch [3740/4062], Loss: 0.0720\n",
            "Epoch [1/5], Batch [3750/4062], Loss: 0.0312\n",
            "Epoch [1/5], Batch [3760/4062], Loss: 0.0310\n",
            "Epoch [1/5], Batch [3770/4062], Loss: 0.0306\n",
            "Epoch [1/5], Batch [3780/4062], Loss: 0.0294\n",
            "Epoch [1/5], Batch [3790/4062], Loss: 0.0549\n",
            "Epoch [1/5], Batch [3800/4062], Loss: 0.0231\n",
            "Epoch [1/5], Batch [3810/4062], Loss: 0.0435\n",
            "Epoch [1/5], Batch [3820/4062], Loss: 0.0327\n",
            "Epoch [1/5], Batch [3830/4062], Loss: 0.0765\n",
            "Epoch [1/5], Batch [3840/4062], Loss: 0.0272\n",
            "Epoch [1/5], Batch [3850/4062], Loss: 0.0551\n",
            "Epoch [1/5], Batch [3860/4062], Loss: 0.0103\n",
            "Epoch [1/5], Batch [3870/4062], Loss: 0.0244\n",
            "Epoch [1/5], Batch [3880/4062], Loss: 0.0229\n",
            "Epoch [1/5], Batch [3890/4062], Loss: 0.0400\n",
            "Epoch [1/5], Batch [3900/4062], Loss: 0.0327\n",
            "Epoch [1/5], Batch [3910/4062], Loss: 0.0260\n",
            "Epoch [1/5], Batch [3920/4062], Loss: 0.0308\n",
            "Epoch [1/5], Batch [3930/4062], Loss: 0.0233\n",
            "Epoch [1/5], Batch [3940/4062], Loss: 0.0293\n",
            "Epoch [1/5], Batch [3950/4062], Loss: 0.0234\n",
            "Epoch [1/5], Batch [3960/4062], Loss: 0.0301\n",
            "Epoch [1/5], Batch [3970/4062], Loss: 0.0431\n",
            "Epoch [1/5], Batch [3980/4062], Loss: 0.0287\n",
            "Epoch [1/5], Batch [3990/4062], Loss: 0.0200\n",
            "Epoch [1/5], Batch [4000/4062], Loss: 0.0556\n",
            "Epoch [1/5], Batch [4010/4062], Loss: 0.0644\n",
            "Epoch [1/5], Batch [4020/4062], Loss: 0.0171\n",
            "Epoch [1/5], Batch [4030/4062], Loss: 0.0197\n",
            "Epoch [1/5], Batch [4040/4062], Loss: 0.0305\n",
            "Epoch [1/5], Batch [4050/4062], Loss: 0.0202\n",
            "Epoch [1/5], Batch [4060/4062], Loss: 0.0332\n",
            "Epoch 1/5, Loss: 0.07820892570987857\n",
            "Updated Learning Rate: [6.3e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.92      0.77      0.84       753\n",
            "   overdrive       0.98      0.98      0.98      3012\n",
            "  distortion       0.99      0.99      0.99      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      0.95      0.98      3765\n",
            "      phaser       0.98      1.00      0.99      4518\n",
            "     flanger       0.75      0.99      0.86      3012\n",
            "      chorus       0.99      1.00      0.99      5271\n",
            "       delay       0.92      0.98      0.95      6777\n",
            " hall_reverb       0.98      0.70      0.82      4518\n",
            "plate_reverb       0.78      0.98      0.87      3012\n",
            "     octaver       1.00      0.82      0.90      2259\n",
            " auto_filter       1.00      0.84      0.92      3765\n",
            "\n",
            "   micro avg       0.95      0.94      0.94     50451\n",
            "   macro avg       0.95      0.92      0.93     50451\n",
            "weighted avg       0.95      0.94      0.94     50451\n",
            " samples avg       0.93      0.93      0.92     50451\n",
            "\n",
            "\n",
            "Validation Loss: 0.0413, Accuracy: 0.8317, Precision: 0.9469, Recall: 0.9237, F1-score: 0.9295\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\n",
            "Epoch [2/5], Batch [10/4062], Loss: 0.0161\n",
            "Epoch [2/5], Batch [20/4062], Loss: 0.0211\n",
            "Epoch [2/5], Batch [30/4062], Loss: 0.0379\n",
            "Epoch [2/5], Batch [40/4062], Loss: 0.0299\n",
            "Epoch [2/5], Batch [50/4062], Loss: 0.0219\n",
            "Epoch [2/5], Batch [60/4062], Loss: 0.0229\n",
            "Epoch [2/5], Batch [70/4062], Loss: 0.0192\n",
            "Epoch [2/5], Batch [80/4062], Loss: 0.0167\n",
            "Epoch [2/5], Batch [90/4062], Loss: 0.0228\n",
            "Epoch [2/5], Batch [100/4062], Loss: 0.0275\n",
            "Epoch [2/5], Batch [110/4062], Loss: 0.0264\n",
            "Epoch [2/5], Batch [120/4062], Loss: 0.0220\n",
            "Epoch [2/5], Batch [130/4062], Loss: 0.0134\n",
            "Epoch [2/5], Batch [140/4062], Loss: 0.0224\n",
            "Epoch [2/5], Batch [150/4062], Loss: 0.0137\n",
            "Epoch [2/5], Batch [160/4062], Loss: 0.0299\n",
            "Epoch [2/5], Batch [170/4062], Loss: 0.0124\n",
            "Epoch [2/5], Batch [180/4062], Loss: 0.0203\n",
            "Epoch [2/5], Batch [190/4062], Loss: 0.0299\n",
            "Epoch [2/5], Batch [200/4062], Loss: 0.0288\n",
            "Epoch [2/5], Batch [210/4062], Loss: 0.0165\n",
            "Epoch [2/5], Batch [220/4062], Loss: 0.0119\n",
            "Epoch [2/5], Batch [230/4062], Loss: 0.0146\n",
            "Epoch [2/5], Batch [240/4062], Loss: 0.0291\n",
            "Epoch [2/5], Batch [250/4062], Loss: 0.0164\n",
            "Epoch [2/5], Batch [260/4062], Loss: 0.0193\n",
            "Epoch [2/5], Batch [270/4062], Loss: 0.0221\n",
            "Epoch [2/5], Batch [280/4062], Loss: 0.0126\n",
            "Epoch [2/5], Batch [290/4062], Loss: 0.0061\n",
            "Epoch [2/5], Batch [300/4062], Loss: 0.0133\n",
            "Epoch [2/5], Batch [310/4062], Loss: 0.0210\n",
            "Epoch [2/5], Batch [320/4062], Loss: 0.0361\n",
            "Epoch [2/5], Batch [330/4062], Loss: 0.0222\n",
            "Epoch [2/5], Batch [340/4062], Loss: 0.0221\n",
            "Epoch [2/5], Batch [350/4062], Loss: 0.0283\n",
            "Epoch [2/5], Batch [360/4062], Loss: 0.0180\n",
            "Epoch [2/5], Batch [370/4062], Loss: 0.0149\n",
            "Epoch [2/5], Batch [380/4062], Loss: 0.0136\n",
            "Epoch [2/5], Batch [390/4062], Loss: 0.0089\n",
            "Epoch [2/5], Batch [400/4062], Loss: 0.0321\n",
            "Epoch [2/5], Batch [410/4062], Loss: 0.0154\n",
            "Epoch [2/5], Batch [420/4062], Loss: 0.0151\n",
            "Epoch [2/5], Batch [430/4062], Loss: 0.0169\n",
            "Epoch [2/5], Batch [440/4062], Loss: 0.0229\n",
            "Epoch [2/5], Batch [450/4062], Loss: 0.0258\n",
            "Epoch [2/5], Batch [460/4062], Loss: 0.0086\n",
            "Epoch [2/5], Batch [470/4062], Loss: 0.0264\n",
            "Epoch [2/5], Batch [480/4062], Loss: 0.0263\n",
            "Epoch [2/5], Batch [490/4062], Loss: 0.0262\n",
            "Epoch [2/5], Batch [500/4062], Loss: 0.0093\n",
            "Epoch [2/5], Batch [510/4062], Loss: 0.0169\n",
            "Epoch [2/5], Batch [520/4062], Loss: 0.0386\n",
            "Epoch [2/5], Batch [530/4062], Loss: 0.0139\n",
            "Epoch [2/5], Batch [540/4062], Loss: 0.0055\n",
            "Epoch [2/5], Batch [550/4062], Loss: 0.0191\n",
            "Epoch [2/5], Batch [560/4062], Loss: 0.0115\n",
            "Epoch [2/5], Batch [570/4062], Loss: 0.0118\n",
            "Epoch [2/5], Batch [580/4062], Loss: 0.0192\n",
            "Epoch [2/5], Batch [590/4062], Loss: 0.0180\n",
            "Epoch [2/5], Batch [600/4062], Loss: 0.0242\n",
            "Epoch [2/5], Batch [610/4062], Loss: 0.0172\n",
            "Epoch [2/5], Batch [620/4062], Loss: 0.0339\n",
            "Epoch [2/5], Batch [630/4062], Loss: 0.0182\n",
            "Epoch [2/5], Batch [640/4062], Loss: 0.0124\n",
            "Epoch [2/5], Batch [650/4062], Loss: 0.0222\n",
            "Epoch [2/5], Batch [660/4062], Loss: 0.0279\n",
            "Epoch [2/5], Batch [670/4062], Loss: 0.0123\n",
            "Epoch [2/5], Batch [680/4062], Loss: 0.0253\n",
            "Epoch [2/5], Batch [690/4062], Loss: 0.0267\n",
            "Epoch [2/5], Batch [700/4062], Loss: 0.0292\n",
            "Epoch [2/5], Batch [710/4062], Loss: 0.0151\n",
            "Epoch [2/5], Batch [720/4062], Loss: 0.0271\n",
            "Epoch [2/5], Batch [730/4062], Loss: 0.0244\n",
            "Epoch [2/5], Batch [740/4062], Loss: 0.0247\n",
            "Epoch [2/5], Batch [750/4062], Loss: 0.0143\n",
            "Epoch [2/5], Batch [760/4062], Loss: 0.0155\n",
            "Epoch [2/5], Batch [770/4062], Loss: 0.0105\n",
            "Epoch [2/5], Batch [780/4062], Loss: 0.0242\n",
            "Epoch [2/5], Batch [790/4062], Loss: 0.0109\n",
            "Epoch [2/5], Batch [800/4062], Loss: 0.0407\n",
            "Epoch [2/5], Batch [810/4062], Loss: 0.0165\n",
            "Epoch [2/5], Batch [820/4062], Loss: 0.0143\n",
            "Epoch [2/5], Batch [830/4062], Loss: 0.0325\n",
            "Epoch [2/5], Batch [840/4062], Loss: 0.0050\n",
            "Epoch [2/5], Batch [850/4062], Loss: 0.0199\n",
            "Epoch [2/5], Batch [860/4062], Loss: 0.0182\n",
            "Epoch [2/5], Batch [870/4062], Loss: 0.0120\n",
            "Epoch [2/5], Batch [880/4062], Loss: 0.0278\n",
            "Epoch [2/5], Batch [890/4062], Loss: 0.0286\n",
            "Epoch [2/5], Batch [900/4062], Loss: 0.0166\n",
            "Epoch [2/5], Batch [910/4062], Loss: 0.0140\n",
            "Epoch [2/5], Batch [920/4062], Loss: 0.0159\n",
            "Epoch [2/5], Batch [930/4062], Loss: 0.0355\n",
            "Epoch [2/5], Batch [940/4062], Loss: 0.0144\n",
            "Epoch [2/5], Batch [950/4062], Loss: 0.0283\n",
            "Epoch [2/5], Batch [960/4062], Loss: 0.0215\n",
            "Epoch [2/5], Batch [970/4062], Loss: 0.0730\n",
            "Epoch [2/5], Batch [980/4062], Loss: 0.0133\n",
            "Epoch [2/5], Batch [990/4062], Loss: 0.0178\n",
            "Epoch [2/5], Batch [1000/4062], Loss: 0.0092\n",
            "Epoch [2/5], Batch [1010/4062], Loss: 0.0273\n",
            "Epoch [2/5], Batch [1020/4062], Loss: 0.0135\n",
            "Epoch [2/5], Batch [1030/4062], Loss: 0.0271\n",
            "Epoch [2/5], Batch [1040/4062], Loss: 0.0101\n",
            "Epoch [2/5], Batch [1050/4062], Loss: 0.0090\n",
            "Epoch [2/5], Batch [1060/4062], Loss: 0.0139\n",
            "Epoch [2/5], Batch [1070/4062], Loss: 0.0408\n",
            "Epoch [2/5], Batch [1080/4062], Loss: 0.0200\n",
            "Epoch [2/5], Batch [1090/4062], Loss: 0.0126\n",
            "Epoch [2/5], Batch [1100/4062], Loss: 0.0173\n",
            "Epoch [2/5], Batch [1110/4062], Loss: 0.0434\n",
            "Epoch [2/5], Batch [1120/4062], Loss: 0.0180\n",
            "Epoch [2/5], Batch [1130/4062], Loss: 0.0241\n",
            "Epoch [2/5], Batch [1140/4062], Loss: 0.0319\n",
            "Epoch [2/5], Batch [1150/4062], Loss: 0.0109\n",
            "Epoch [2/5], Batch [1160/4062], Loss: 0.0095\n",
            "Epoch [2/5], Batch [1170/4062], Loss: 0.0155\n",
            "Epoch [2/5], Batch [1180/4062], Loss: 0.0085\n",
            "Epoch [2/5], Batch [1190/4062], Loss: 0.0231\n",
            "Epoch [2/5], Batch [1200/4062], Loss: 0.0285\n",
            "Epoch [2/5], Batch [1210/4062], Loss: 0.0372\n",
            "Epoch [2/5], Batch [1220/4062], Loss: 0.0144\n",
            "Epoch [2/5], Batch [1230/4062], Loss: 0.0144\n",
            "Epoch [2/5], Batch [1240/4062], Loss: 0.0062\n",
            "Epoch [2/5], Batch [1250/4062], Loss: 0.0283\n",
            "Epoch [2/5], Batch [1260/4062], Loss: 0.0314\n",
            "Epoch [2/5], Batch [1270/4062], Loss: 0.0264\n",
            "Epoch [2/5], Batch [1280/4062], Loss: 0.0085\n",
            "Epoch [2/5], Batch [1290/4062], Loss: 0.0157\n",
            "Epoch [2/5], Batch [1300/4062], Loss: 0.0159\n",
            "Epoch [2/5], Batch [1310/4062], Loss: 0.0230\n",
            "Epoch [2/5], Batch [1320/4062], Loss: 0.0222\n",
            "Epoch [2/5], Batch [1330/4062], Loss: 0.0153\n",
            "Epoch [2/5], Batch [1340/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [1350/4062], Loss: 0.0573\n",
            "Epoch [2/5], Batch [1360/4062], Loss: 0.0226\n",
            "Epoch [2/5], Batch [1370/4062], Loss: 0.0434\n",
            "Epoch [2/5], Batch [1380/4062], Loss: 0.0298\n",
            "Epoch [2/5], Batch [1390/4062], Loss: 0.0126\n",
            "Epoch [2/5], Batch [1400/4062], Loss: 0.0161\n",
            "Epoch [2/5], Batch [1410/4062], Loss: 0.0248\n",
            "Epoch [2/5], Batch [1420/4062], Loss: 0.0146\n",
            "Epoch [2/5], Batch [1430/4062], Loss: 0.0063\n",
            "Epoch [2/5], Batch [1440/4062], Loss: 0.0443\n",
            "Epoch [2/5], Batch [1450/4062], Loss: 0.0161\n",
            "Epoch [2/5], Batch [1460/4062], Loss: 0.0135\n",
            "Epoch [2/5], Batch [1470/4062], Loss: 0.0161\n",
            "Epoch [2/5], Batch [1480/4062], Loss: 0.0183\n",
            "Epoch [2/5], Batch [1490/4062], Loss: 0.0225\n",
            "Epoch [2/5], Batch [1500/4062], Loss: 0.0247\n",
            "Epoch [2/5], Batch [1510/4062], Loss: 0.0135\n",
            "Epoch [2/5], Batch [1520/4062], Loss: 0.0105\n",
            "Epoch [2/5], Batch [1530/4062], Loss: 0.0132\n",
            "Epoch [2/5], Batch [1540/4062], Loss: 0.0305\n",
            "Epoch [2/5], Batch [1550/4062], Loss: 0.0103\n",
            "Epoch [2/5], Batch [1560/4062], Loss: 0.0074\n",
            "Epoch [2/5], Batch [1570/4062], Loss: 0.0424\n",
            "Epoch [2/5], Batch [1580/4062], Loss: 0.0161\n",
            "Epoch [2/5], Batch [1590/4062], Loss: 0.0323\n",
            "Epoch [2/5], Batch [1600/4062], Loss: 0.0194\n",
            "Epoch [2/5], Batch [1610/4062], Loss: 0.0221\n",
            "Epoch [2/5], Batch [1620/4062], Loss: 0.0333\n",
            "Epoch [2/5], Batch [1630/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [1640/4062], Loss: 0.0094\n",
            "Epoch [2/5], Batch [1650/4062], Loss: 0.0088\n",
            "Epoch [2/5], Batch [1660/4062], Loss: 0.0140\n",
            "Epoch [2/5], Batch [1670/4062], Loss: 0.0211\n",
            "Epoch [2/5], Batch [1680/4062], Loss: 0.0189\n",
            "Epoch [2/5], Batch [1690/4062], Loss: 0.0275\n",
            "Epoch [2/5], Batch [1700/4062], Loss: 0.0167\n",
            "Epoch [2/5], Batch [1710/4062], Loss: 0.0124\n",
            "Epoch [2/5], Batch [1720/4062], Loss: 0.0250\n",
            "Epoch [2/5], Batch [1730/4062], Loss: 0.0121\n",
            "Epoch [2/5], Batch [1740/4062], Loss: 0.0139\n",
            "Epoch [2/5], Batch [1750/4062], Loss: 0.0094\n",
            "Epoch [2/5], Batch [1760/4062], Loss: 0.0086\n",
            "Epoch [2/5], Batch [1770/4062], Loss: 0.0065\n",
            "Epoch [2/5], Batch [1780/4062], Loss: 0.0107\n",
            "Epoch [2/5], Batch [1790/4062], Loss: 0.0142\n",
            "Epoch [2/5], Batch [1800/4062], Loss: 0.0449\n",
            "Epoch [2/5], Batch [1810/4062], Loss: 0.0115\n",
            "Epoch [2/5], Batch [1820/4062], Loss: 0.0057\n",
            "Epoch [2/5], Batch [1830/4062], Loss: 0.0107\n",
            "Epoch [2/5], Batch [1840/4062], Loss: 0.0094\n",
            "Epoch [2/5], Batch [1850/4062], Loss: 0.0195\n",
            "Epoch [2/5], Batch [1860/4062], Loss: 0.0116\n",
            "Epoch [2/5], Batch [1870/4062], Loss: 0.0127\n",
            "Epoch [2/5], Batch [1880/4062], Loss: 0.0130\n",
            "Epoch [2/5], Batch [1890/4062], Loss: 0.0169\n",
            "Epoch [2/5], Batch [1900/4062], Loss: 0.0322\n",
            "Epoch [2/5], Batch [1910/4062], Loss: 0.0471\n",
            "Epoch [2/5], Batch [1920/4062], Loss: 0.0209\n",
            "Epoch [2/5], Batch [1930/4062], Loss: 0.0135\n",
            "Epoch [2/5], Batch [1940/4062], Loss: 0.0170\n",
            "Epoch [2/5], Batch [1950/4062], Loss: 0.0442\n",
            "Epoch [2/5], Batch [1960/4062], Loss: 0.0250\n",
            "Epoch [2/5], Batch [1970/4062], Loss: 0.0242\n",
            "Epoch [2/5], Batch [1980/4062], Loss: 0.0166\n",
            "Epoch [2/5], Batch [1990/4062], Loss: 0.0142\n",
            "Epoch [2/5], Batch [2000/4062], Loss: 0.0144\n",
            "Epoch [2/5], Batch [2010/4062], Loss: 0.0373\n",
            "Epoch [2/5], Batch [2020/4062], Loss: 0.0199\n",
            "Epoch [2/5], Batch [2030/4062], Loss: 0.0063\n",
            "Epoch [2/5], Batch [2040/4062], Loss: 0.0108\n",
            "Epoch [2/5], Batch [2050/4062], Loss: 0.0096\n",
            "Epoch [2/5], Batch [2060/4062], Loss: 0.0129\n",
            "Epoch [2/5], Batch [2070/4062], Loss: 0.0332\n",
            "Epoch [2/5], Batch [2080/4062], Loss: 0.0091\n",
            "Epoch [2/5], Batch [2090/4062], Loss: 0.0276\n",
            "Epoch [2/5], Batch [2100/4062], Loss: 0.0432\n",
            "Epoch [2/5], Batch [2110/4062], Loss: 0.0709\n",
            "Epoch [2/5], Batch [2120/4062], Loss: 0.0292\n",
            "Epoch [2/5], Batch [2130/4062], Loss: 0.0086\n",
            "Epoch [2/5], Batch [2140/4062], Loss: 0.0207\n",
            "Epoch [2/5], Batch [2150/4062], Loss: 0.0075\n",
            "Epoch [2/5], Batch [2160/4062], Loss: 0.0387\n",
            "Epoch [2/5], Batch [2170/4062], Loss: 0.0154\n",
            "Epoch [2/5], Batch [2180/4062], Loss: 0.0178\n",
            "Epoch [2/5], Batch [2190/4062], Loss: 0.0181\n",
            "Epoch [2/5], Batch [2200/4062], Loss: 0.0112\n",
            "Epoch [2/5], Batch [2210/4062], Loss: 0.0296\n",
            "Epoch [2/5], Batch [2220/4062], Loss: 0.0602\n",
            "Epoch [2/5], Batch [2230/4062], Loss: 0.0207\n",
            "Epoch [2/5], Batch [2240/4062], Loss: 0.0102\n",
            "Epoch [2/5], Batch [2250/4062], Loss: 0.0155\n",
            "Epoch [2/5], Batch [2260/4062], Loss: 0.0166\n",
            "Epoch [2/5], Batch [2270/4062], Loss: 0.0208\n",
            "Epoch [2/5], Batch [2280/4062], Loss: 0.0177\n",
            "Epoch [2/5], Batch [2290/4062], Loss: 0.0216\n",
            "Epoch [2/5], Batch [2300/4062], Loss: 0.0168\n",
            "Epoch [2/5], Batch [2310/4062], Loss: 0.0374\n",
            "Epoch [2/5], Batch [2320/4062], Loss: 0.0114\n",
            "Epoch [2/5], Batch [2330/4062], Loss: 0.0326\n",
            "Epoch [2/5], Batch [2340/4062], Loss: 0.0276\n",
            "Epoch [2/5], Batch [2350/4062], Loss: 0.0299\n",
            "Epoch [2/5], Batch [2360/4062], Loss: 0.0236\n",
            "Epoch [2/5], Batch [2370/4062], Loss: 0.0048\n",
            "Epoch [2/5], Batch [2380/4062], Loss: 0.0176\n",
            "Epoch [2/5], Batch [2390/4062], Loss: 0.0118\n",
            "Epoch [2/5], Batch [2400/4062], Loss: 0.0050\n",
            "Epoch [2/5], Batch [2410/4062], Loss: 0.0588\n",
            "Epoch [2/5], Batch [2420/4062], Loss: 0.0251\n",
            "Epoch [2/5], Batch [2430/4062], Loss: 0.0223\n",
            "Epoch [2/5], Batch [2440/4062], Loss: 0.0168\n",
            "Epoch [2/5], Batch [2450/4062], Loss: 0.0267\n",
            "Epoch [2/5], Batch [2460/4062], Loss: 0.0197\n",
            "Epoch [2/5], Batch [2470/4062], Loss: 0.0107\n",
            "Epoch [2/5], Batch [2480/4062], Loss: 0.0245\n",
            "Epoch [2/5], Batch [2490/4062], Loss: 0.0146\n",
            "Epoch [2/5], Batch [2500/4062], Loss: 0.0118\n",
            "Epoch [2/5], Batch [2510/4062], Loss: 0.0125\n",
            "Epoch [2/5], Batch [2520/4062], Loss: 0.0200\n",
            "Epoch [2/5], Batch [2530/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [2540/4062], Loss: 0.0178\n",
            "Epoch [2/5], Batch [2550/4062], Loss: 0.0110\n",
            "Epoch [2/5], Batch [2560/4062], Loss: 0.0468\n",
            "Epoch [2/5], Batch [2570/4062], Loss: 0.0107\n",
            "Epoch [2/5], Batch [2580/4062], Loss: 0.0512\n",
            "Epoch [2/5], Batch [2590/4062], Loss: 0.0206\n",
            "Epoch [2/5], Batch [2600/4062], Loss: 0.0278\n",
            "Epoch [2/5], Batch [2610/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [2620/4062], Loss: 0.0118\n",
            "Epoch [2/5], Batch [2630/4062], Loss: 0.0126\n",
            "Epoch [2/5], Batch [2640/4062], Loss: 0.0156\n",
            "Epoch [2/5], Batch [2650/4062], Loss: 0.0210\n",
            "Epoch [2/5], Batch [2660/4062], Loss: 0.0067\n",
            "Epoch [2/5], Batch [2670/4062], Loss: 0.0111\n",
            "Epoch [2/5], Batch [2680/4062], Loss: 0.0232\n",
            "Epoch [2/5], Batch [2690/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [2700/4062], Loss: 0.0170\n",
            "Epoch [2/5], Batch [2710/4062], Loss: 0.0103\n",
            "Epoch [2/5], Batch [2720/4062], Loss: 0.0088\n",
            "Epoch [2/5], Batch [2730/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [2740/4062], Loss: 0.0150\n",
            "Epoch [2/5], Batch [2750/4062], Loss: 0.0078\n",
            "Epoch [2/5], Batch [2760/4062], Loss: 0.0062\n",
            "Epoch [2/5], Batch [2770/4062], Loss: 0.0233\n",
            "Epoch [2/5], Batch [2780/4062], Loss: 0.0079\n",
            "Epoch [2/5], Batch [2790/4062], Loss: 0.0190\n",
            "Epoch [2/5], Batch [2800/4062], Loss: 0.0209\n",
            "Epoch [2/5], Batch [2810/4062], Loss: 0.0084\n",
            "Epoch [2/5], Batch [2820/4062], Loss: 0.0089\n",
            "Epoch [2/5], Batch [2830/4062], Loss: 0.0065\n",
            "Epoch [2/5], Batch [2840/4062], Loss: 0.0191\n",
            "Epoch [2/5], Batch [2850/4062], Loss: 0.0152\n",
            "Epoch [2/5], Batch [2860/4062], Loss: 0.0101\n",
            "Epoch [2/5], Batch [2870/4062], Loss: 0.0138\n",
            "Epoch [2/5], Batch [2880/4062], Loss: 0.0348\n",
            "Epoch [2/5], Batch [2890/4062], Loss: 0.0328\n",
            "Epoch [2/5], Batch [2900/4062], Loss: 0.0159\n",
            "Epoch [2/5], Batch [2910/4062], Loss: 0.0209\n",
            "Epoch [2/5], Batch [2920/4062], Loss: 0.0169\n",
            "Epoch [2/5], Batch [2930/4062], Loss: 0.0134\n",
            "Epoch [2/5], Batch [2940/4062], Loss: 0.0174\n",
            "Epoch [2/5], Batch [2950/4062], Loss: 0.0305\n",
            "Epoch [2/5], Batch [2960/4062], Loss: 0.0102\n",
            "Epoch [2/5], Batch [2970/4062], Loss: 0.0138\n",
            "Epoch [2/5], Batch [2980/4062], Loss: 0.0085\n",
            "Epoch [2/5], Batch [2990/4062], Loss: 0.0291\n",
            "Epoch [2/5], Batch [3000/4062], Loss: 0.0238\n",
            "Epoch [2/5], Batch [3010/4062], Loss: 0.0161\n",
            "Epoch [2/5], Batch [3020/4062], Loss: 0.0274\n",
            "Epoch [2/5], Batch [3030/4062], Loss: 0.0103\n",
            "Epoch [2/5], Batch [3040/4062], Loss: 0.0145\n",
            "Epoch [2/5], Batch [3050/4062], Loss: 0.0140\n",
            "Epoch [2/5], Batch [3060/4062], Loss: 0.0262\n",
            "Epoch [2/5], Batch [3070/4062], Loss: 0.0160\n",
            "Epoch [2/5], Batch [3080/4062], Loss: 0.0328\n",
            "Epoch [2/5], Batch [3090/4062], Loss: 0.0235\n",
            "Epoch [2/5], Batch [3100/4062], Loss: 0.0113\n",
            "Epoch [2/5], Batch [3110/4062], Loss: 0.0299\n",
            "Epoch [2/5], Batch [3120/4062], Loss: 0.0150\n",
            "Epoch [2/5], Batch [3130/4062], Loss: 0.0091\n",
            "Epoch [2/5], Batch [3140/4062], Loss: 0.0055\n",
            "Epoch [2/5], Batch [3150/4062], Loss: 0.0048\n",
            "Epoch [2/5], Batch [3160/4062], Loss: 0.0105\n",
            "Epoch [2/5], Batch [3170/4062], Loss: 0.0086\n",
            "Epoch [2/5], Batch [3180/4062], Loss: 0.0155\n",
            "Epoch [2/5], Batch [3190/4062], Loss: 0.0097\n",
            "Epoch [2/5], Batch [3200/4062], Loss: 0.0134\n",
            "Epoch [2/5], Batch [3210/4062], Loss: 0.0146\n",
            "Epoch [2/5], Batch [3220/4062], Loss: 0.0171\n",
            "Epoch [2/5], Batch [3230/4062], Loss: 0.0064\n",
            "Epoch [2/5], Batch [3240/4062], Loss: 0.0048\n",
            "Epoch [2/5], Batch [3250/4062], Loss: 0.0475\n",
            "Epoch [2/5], Batch [3260/4062], Loss: 0.0082\n",
            "Epoch [2/5], Batch [3270/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [3280/4062], Loss: 0.0035\n",
            "Epoch [2/5], Batch [3290/4062], Loss: 0.0179\n",
            "Epoch [2/5], Batch [3300/4062], Loss: 0.0245\n",
            "Epoch [2/5], Batch [3310/4062], Loss: 0.0366\n",
            "Epoch [2/5], Batch [3320/4062], Loss: 0.0075\n",
            "Epoch [2/5], Batch [3330/4062], Loss: 0.0067\n",
            "Epoch [2/5], Batch [3340/4062], Loss: 0.0220\n",
            "Epoch [2/5], Batch [3350/4062], Loss: 0.0076\n",
            "Epoch [2/5], Batch [3360/4062], Loss: 0.0076\n",
            "Epoch [2/5], Batch [3370/4062], Loss: 0.0206\n",
            "Epoch [2/5], Batch [3380/4062], Loss: 0.0117\n",
            "Epoch [2/5], Batch [3390/4062], Loss: 0.0133\n",
            "Epoch [2/5], Batch [3400/4062], Loss: 0.0139\n",
            "Epoch [2/5], Batch [3410/4062], Loss: 0.0476\n",
            "Epoch [2/5], Batch [3420/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [3430/4062], Loss: 0.0166\n",
            "Epoch [2/5], Batch [3440/4062], Loss: 0.0356\n",
            "Epoch [2/5], Batch [3450/4062], Loss: 0.0193\n",
            "Epoch [2/5], Batch [3460/4062], Loss: 0.0208\n",
            "Epoch [2/5], Batch [3470/4062], Loss: 0.0163\n",
            "Epoch [2/5], Batch [3480/4062], Loss: 0.0148\n",
            "Epoch [2/5], Batch [3490/4062], Loss: 0.0101\n",
            "Epoch [2/5], Batch [3500/4062], Loss: 0.0182\n",
            "Epoch [2/5], Batch [3510/4062], Loss: 0.0093\n",
            "Epoch [2/5], Batch [3520/4062], Loss: 0.0387\n",
            "Epoch [2/5], Batch [3530/4062], Loss: 0.0173\n",
            "Epoch [2/5], Batch [3540/4062], Loss: 0.0457\n",
            "Epoch [2/5], Batch [3550/4062], Loss: 0.0133\n",
            "Epoch [2/5], Batch [3560/4062], Loss: 0.0155\n",
            "Epoch [2/5], Batch [3570/4062], Loss: 0.0126\n",
            "Epoch [2/5], Batch [3580/4062], Loss: 0.0113\n",
            "Epoch [2/5], Batch [3590/4062], Loss: 0.0102\n",
            "Epoch [2/5], Batch [3600/4062], Loss: 0.0187\n",
            "Epoch [2/5], Batch [3610/4062], Loss: 0.0157\n",
            "Epoch [2/5], Batch [3620/4062], Loss: 0.0404\n",
            "Epoch [2/5], Batch [3630/4062], Loss: 0.0250\n",
            "Epoch [2/5], Batch [3640/4062], Loss: 0.0031\n",
            "Epoch [2/5], Batch [3650/4062], Loss: 0.0128\n",
            "Epoch [2/5], Batch [3660/4062], Loss: 0.0424\n",
            "Epoch [2/5], Batch [3670/4062], Loss: 0.0160\n",
            "Epoch [2/5], Batch [3680/4062], Loss: 0.0259\n",
            "Epoch [2/5], Batch [3690/4062], Loss: 0.0120\n",
            "Epoch [2/5], Batch [3700/4062], Loss: 0.0104\n",
            "Epoch [2/5], Batch [3710/4062], Loss: 0.0315\n",
            "Epoch [2/5], Batch [3720/4062], Loss: 0.0115\n",
            "Epoch [2/5], Batch [3730/4062], Loss: 0.0229\n",
            "Epoch [2/5], Batch [3740/4062], Loss: 0.0057\n",
            "Epoch [2/5], Batch [3750/4062], Loss: 0.0067\n",
            "Epoch [2/5], Batch [3760/4062], Loss: 0.0243\n",
            "Epoch [2/5], Batch [3770/4062], Loss: 0.0273\n",
            "Epoch [2/5], Batch [3780/4062], Loss: 0.0068\n",
            "Epoch [2/5], Batch [3790/4062], Loss: 0.0137\n",
            "Epoch [2/5], Batch [3800/4062], Loss: 0.0329\n",
            "Epoch [2/5], Batch [3810/4062], Loss: 0.0301\n",
            "Epoch [2/5], Batch [3820/4062], Loss: 0.0140\n",
            "Epoch [2/5], Batch [3830/4062], Loss: 0.0098\n",
            "Epoch [2/5], Batch [3840/4062], Loss: 0.0295\n",
            "Epoch [2/5], Batch [3850/4062], Loss: 0.0119\n",
            "Epoch [2/5], Batch [3860/4062], Loss: 0.0157\n",
            "Epoch [2/5], Batch [3870/4062], Loss: 0.0357\n",
            "Epoch [2/5], Batch [3880/4062], Loss: 0.0152\n",
            "Epoch [2/5], Batch [3890/4062], Loss: 0.0214\n",
            "Epoch [2/5], Batch [3900/4062], Loss: 0.0156\n",
            "Epoch [2/5], Batch [3910/4062], Loss: 0.0068\n",
            "Epoch [2/5], Batch [3920/4062], Loss: 0.0192\n",
            "Epoch [2/5], Batch [3930/4062], Loss: 0.0304\n",
            "Epoch [2/5], Batch [3940/4062], Loss: 0.0157\n",
            "Epoch [2/5], Batch [3950/4062], Loss: 0.0162\n",
            "Epoch [2/5], Batch [3960/4062], Loss: 0.0111\n",
            "Epoch [2/5], Batch [3970/4062], Loss: 0.0207\n",
            "Epoch [2/5], Batch [3980/4062], Loss: 0.0074\n",
            "Epoch [2/5], Batch [3990/4062], Loss: 0.0116\n",
            "Epoch [2/5], Batch [4000/4062], Loss: 0.0096\n",
            "Epoch [2/5], Batch [4010/4062], Loss: 0.0204\n",
            "Epoch [2/5], Batch [4020/4062], Loss: 0.0358\n",
            "Epoch [2/5], Batch [4030/4062], Loss: 0.0153\n",
            "Epoch [2/5], Batch [4040/4062], Loss: 0.0191\n",
            "Epoch [2/5], Batch [4050/4062], Loss: 0.0208\n",
            "Epoch [2/5], Batch [4060/4062], Loss: 0.0186\n",
            "Epoch 2/5, Loss: 0.019514721865040777\n",
            "Updated Learning Rate: [3.969e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       1.00      0.71      0.83       753\n",
            "   overdrive       0.91      1.00      0.95      3012\n",
            "  distortion       1.00      0.96      0.98      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      0.99      0.99      3765\n",
            "      phaser       1.00      0.99      1.00      4518\n",
            "     flanger       1.00      0.93      0.96      3012\n",
            "      chorus       1.00      0.99      0.99      5271\n",
            "       delay       0.96      0.98      0.97      6777\n",
            " hall_reverb       0.95      0.86      0.90      4518\n",
            "plate_reverb       0.83      0.97      0.90      3012\n",
            "     octaver       0.89      1.00      0.94      2259\n",
            " auto_filter       0.99      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.97      0.97      0.97     50451\n",
            "   macro avg       0.96      0.95      0.95     50451\n",
            "weighted avg       0.97      0.97      0.97     50451\n",
            " samples avg       0.95      0.96      0.95     50451\n",
            "\n",
            "\n",
            "Validation Loss: 0.0267, Accuracy: 0.9075, Precision: 0.9642, Recall: 0.9512, F1-score: 0.9546\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5], Batch [10/4062], Loss: 0.0155\n",
            "Epoch [3/5], Batch [20/4062], Loss: 0.0082\n",
            "Epoch [3/5], Batch [30/4062], Loss: 0.0145\n",
            "Epoch [3/5], Batch [40/4062], Loss: 0.0105\n",
            "Epoch [3/5], Batch [50/4062], Loss: 0.0130\n",
            "Epoch [3/5], Batch [60/4062], Loss: 0.0114\n",
            "Epoch [3/5], Batch [70/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [80/4062], Loss: 0.0182\n",
            "Epoch [3/5], Batch [90/4062], Loss: 0.0053\n",
            "Epoch [3/5], Batch [100/4062], Loss: 0.0046\n",
            "Epoch [3/5], Batch [110/4062], Loss: 0.0140\n",
            "Epoch [3/5], Batch [120/4062], Loss: 0.0157\n",
            "Epoch [3/5], Batch [130/4062], Loss: 0.0146\n",
            "Epoch [3/5], Batch [140/4062], Loss: 0.0083\n",
            "Epoch [3/5], Batch [150/4062], Loss: 0.0123\n",
            "Epoch [3/5], Batch [160/4062], Loss: 0.0269\n",
            "Epoch [3/5], Batch [170/4062], Loss: 0.0081\n",
            "Epoch [3/5], Batch [180/4062], Loss: 0.0181\n",
            "Epoch [3/5], Batch [190/4062], Loss: 0.0128\n",
            "Epoch [3/5], Batch [200/4062], Loss: 0.0115\n",
            "Epoch [3/5], Batch [210/4062], Loss: 0.0103\n",
            "Epoch [3/5], Batch [220/4062], Loss: 0.0139\n",
            "Epoch [3/5], Batch [230/4062], Loss: 0.0108\n",
            "Epoch [3/5], Batch [240/4062], Loss: 0.0210\n",
            "Epoch [3/5], Batch [250/4062], Loss: 0.0147\n",
            "Epoch [3/5], Batch [260/4062], Loss: 0.0057\n",
            "Epoch [3/5], Batch [270/4062], Loss: 0.0162\n",
            "Epoch [3/5], Batch [280/4062], Loss: 0.0036\n",
            "Epoch [3/5], Batch [290/4062], Loss: 0.0086\n",
            "Epoch [3/5], Batch [300/4062], Loss: 0.0186\n",
            "Epoch [3/5], Batch [310/4062], Loss: 0.0086\n",
            "Epoch [3/5], Batch [320/4062], Loss: 0.0142\n",
            "Epoch [3/5], Batch [330/4062], Loss: 0.0135\n",
            "Epoch [3/5], Batch [340/4062], Loss: 0.0088\n",
            "Epoch [3/5], Batch [350/4062], Loss: 0.0127\n",
            "Epoch [3/5], Batch [360/4062], Loss: 0.0082\n",
            "Epoch [3/5], Batch [370/4062], Loss: 0.0057\n",
            "Epoch [3/5], Batch [380/4062], Loss: 0.0121\n",
            "Epoch [3/5], Batch [390/4062], Loss: 0.0120\n",
            "Epoch [3/5], Batch [400/4062], Loss: 0.0205\n",
            "Epoch [3/5], Batch [410/4062], Loss: 0.0075\n",
            "Epoch [3/5], Batch [420/4062], Loss: 0.0064\n",
            "Epoch [3/5], Batch [430/4062], Loss: 0.0213\n",
            "Epoch [3/5], Batch [440/4062], Loss: 0.0050\n",
            "Epoch [3/5], Batch [450/4062], Loss: 0.0160\n",
            "Epoch [3/5], Batch [460/4062], Loss: 0.0127\n",
            "Epoch [3/5], Batch [470/4062], Loss: 0.0147\n",
            "Epoch [3/5], Batch [480/4062], Loss: 0.0247\n",
            "Epoch [3/5], Batch [490/4062], Loss: 0.0037\n",
            "Epoch [3/5], Batch [500/4062], Loss: 0.0148\n",
            "Epoch [3/5], Batch [510/4062], Loss: 0.0106\n",
            "Epoch [3/5], Batch [520/4062], Loss: 0.0036\n",
            "Epoch [3/5], Batch [530/4062], Loss: 0.0136\n",
            "Epoch [3/5], Batch [540/4062], Loss: 0.0118\n",
            "Epoch [3/5], Batch [550/4062], Loss: 0.0047\n",
            "Epoch [3/5], Batch [560/4062], Loss: 0.0066\n",
            "Epoch [3/5], Batch [570/4062], Loss: 0.0079\n",
            "Epoch [3/5], Batch [580/4062], Loss: 0.0157\n",
            "Epoch [3/5], Batch [590/4062], Loss: 0.0188\n",
            "Epoch [3/5], Batch [600/4062], Loss: 0.0116\n",
            "Epoch [3/5], Batch [610/4062], Loss: 0.0193\n",
            "Epoch [3/5], Batch [620/4062], Loss: 0.0048\n",
            "Epoch [3/5], Batch [630/4062], Loss: 0.0117\n",
            "Epoch [3/5], Batch [640/4062], Loss: 0.0047\n",
            "Epoch [3/5], Batch [650/4062], Loss: 0.0148\n",
            "Epoch [3/5], Batch [660/4062], Loss: 0.0268\n",
            "Epoch [3/5], Batch [670/4062], Loss: 0.0098\n",
            "Epoch [3/5], Batch [680/4062], Loss: 0.0113\n",
            "Epoch [3/5], Batch [690/4062], Loss: 0.0060\n",
            "Epoch [3/5], Batch [700/4062], Loss: 0.0223\n",
            "Epoch [3/5], Batch [710/4062], Loss: 0.0112\n",
            "Epoch [3/5], Batch [720/4062], Loss: 0.0198\n",
            "Epoch [3/5], Batch [730/4062], Loss: 0.0091\n",
            "Epoch [3/5], Batch [740/4062], Loss: 0.0087\n",
            "Epoch [3/5], Batch [750/4062], Loss: 0.0053\n",
            "Epoch [3/5], Batch [760/4062], Loss: 0.0121\n",
            "Epoch [3/5], Batch [770/4062], Loss: 0.0123\n",
            "Epoch [3/5], Batch [780/4062], Loss: 0.0129\n",
            "Epoch [3/5], Batch [790/4062], Loss: 0.0132\n",
            "Epoch [3/5], Batch [800/4062], Loss: 0.0092\n",
            "Epoch [3/5], Batch [810/4062], Loss: 0.0139\n",
            "Epoch [3/5], Batch [820/4062], Loss: 0.0110\n",
            "Epoch [3/5], Batch [830/4062], Loss: 0.0069\n",
            "Epoch [3/5], Batch [840/4062], Loss: 0.0083\n",
            "Epoch [3/5], Batch [850/4062], Loss: 0.0171\n",
            "Epoch [3/5], Batch [860/4062], Loss: 0.0115\n",
            "Epoch [3/5], Batch [870/4062], Loss: 0.0146\n",
            "Epoch [3/5], Batch [880/4062], Loss: 0.0098\n",
            "Epoch [3/5], Batch [890/4062], Loss: 0.0152\n",
            "Epoch [3/5], Batch [900/4062], Loss: 0.0101\n",
            "Epoch [3/5], Batch [910/4062], Loss: 0.0083\n",
            "Epoch [3/5], Batch [920/4062], Loss: 0.0216\n",
            "Epoch [3/5], Batch [930/4062], Loss: 0.0147\n",
            "Epoch [3/5], Batch [940/4062], Loss: 0.0066\n",
            "Epoch [3/5], Batch [950/4062], Loss: 0.0084\n",
            "Epoch [3/5], Batch [960/4062], Loss: 0.0113\n",
            "Epoch [3/5], Batch [970/4062], Loss: 0.0047\n",
            "Epoch [3/5], Batch [980/4062], Loss: 0.0144\n",
            "Epoch [3/5], Batch [990/4062], Loss: 0.0091\n",
            "Epoch [3/5], Batch [1000/4062], Loss: 0.0126\n",
            "Epoch [3/5], Batch [1010/4062], Loss: 0.0176\n",
            "Epoch [3/5], Batch [1020/4062], Loss: 0.0230\n",
            "Epoch [3/5], Batch [1030/4062], Loss: 0.0200\n",
            "Epoch [3/5], Batch [1040/4062], Loss: 0.0163\n",
            "Epoch [3/5], Batch [1050/4062], Loss: 0.0117\n",
            "Epoch [3/5], Batch [1060/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [1070/4062], Loss: 0.0122\n",
            "Epoch [3/5], Batch [1080/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [1090/4062], Loss: 0.0227\n",
            "Epoch [3/5], Batch [1100/4062], Loss: 0.0250\n",
            "Epoch [3/5], Batch [1110/4062], Loss: 0.0082\n",
            "Epoch [3/5], Batch [1120/4062], Loss: 0.0090\n",
            "Epoch [3/5], Batch [1130/4062], Loss: 0.0040\n",
            "Epoch [3/5], Batch [1140/4062], Loss: 0.0082\n",
            "Epoch [3/5], Batch [1150/4062], Loss: 0.0165\n",
            "Epoch [3/5], Batch [1160/4062], Loss: 0.0170\n",
            "Epoch [3/5], Batch [1170/4062], Loss: 0.0051\n",
            "Epoch [3/5], Batch [1180/4062], Loss: 0.0048\n",
            "Epoch [3/5], Batch [1190/4062], Loss: 0.0229\n",
            "Epoch [3/5], Batch [1200/4062], Loss: 0.0063\n",
            "Epoch [3/5], Batch [1210/4062], Loss: 0.0049\n",
            "Epoch [3/5], Batch [1220/4062], Loss: 0.0067\n",
            "Epoch [3/5], Batch [1230/4062], Loss: 0.0049\n",
            "Epoch [3/5], Batch [1240/4062], Loss: 0.0096\n",
            "Epoch [3/5], Batch [1250/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [1260/4062], Loss: 0.0257\n",
            "Epoch [3/5], Batch [1270/4062], Loss: 0.0136\n",
            "Epoch [3/5], Batch [1280/4062], Loss: 0.0109\n",
            "Epoch [3/5], Batch [1290/4062], Loss: 0.0188\n",
            "Epoch [3/5], Batch [1300/4062], Loss: 0.0082\n",
            "Epoch [3/5], Batch [1310/4062], Loss: 0.0341\n",
            "Epoch [3/5], Batch [1320/4062], Loss: 0.0143\n",
            "Epoch [3/5], Batch [1330/4062], Loss: 0.0056\n",
            "Epoch [3/5], Batch [1340/4062], Loss: 0.0064\n",
            "Epoch [3/5], Batch [1350/4062], Loss: 0.0075\n",
            "Epoch [3/5], Batch [1360/4062], Loss: 0.0080\n",
            "Epoch [3/5], Batch [1370/4062], Loss: 0.0144\n",
            "Epoch [3/5], Batch [1380/4062], Loss: 0.0112\n",
            "Epoch [3/5], Batch [1390/4062], Loss: 0.0067\n",
            "Epoch [3/5], Batch [1400/4062], Loss: 0.0129\n",
            "Epoch [3/5], Batch [1410/4062], Loss: 0.0193\n",
            "Epoch [3/5], Batch [1420/4062], Loss: 0.0090\n",
            "Epoch [3/5], Batch [1430/4062], Loss: 0.0222\n",
            "Epoch [3/5], Batch [1440/4062], Loss: 0.0138\n",
            "Epoch [3/5], Batch [1450/4062], Loss: 0.0108\n",
            "Epoch [3/5], Batch [1460/4062], Loss: 0.0248\n",
            "Epoch [3/5], Batch [1470/4062], Loss: 0.0058\n",
            "Epoch [3/5], Batch [1480/4062], Loss: 0.0272\n",
            "Epoch [3/5], Batch [1490/4062], Loss: 0.0046\n",
            "Epoch [3/5], Batch [1500/4062], Loss: 0.0063\n",
            "Epoch [3/5], Batch [1510/4062], Loss: 0.0240\n",
            "Epoch [3/5], Batch [1520/4062], Loss: 0.0070\n",
            "Epoch [3/5], Batch [1530/4062], Loss: 0.0128\n",
            "Epoch [3/5], Batch [1540/4062], Loss: 0.0108\n",
            "Epoch [3/5], Batch [1550/4062], Loss: 0.0144\n",
            "Epoch [3/5], Batch [1560/4062], Loss: 0.0093\n",
            "Epoch [3/5], Batch [1570/4062], Loss: 0.0061\n",
            "Epoch [3/5], Batch [1580/4062], Loss: 0.0094\n",
            "Epoch [3/5], Batch [1590/4062], Loss: 0.0146\n",
            "Epoch [3/5], Batch [1600/4062], Loss: 0.0112\n",
            "Epoch [3/5], Batch [1610/4062], Loss: 0.0254\n",
            "Epoch [3/5], Batch [1620/4062], Loss: 0.0087\n",
            "Epoch [3/5], Batch [1630/4062], Loss: 0.0127\n",
            "Epoch [3/5], Batch [1640/4062], Loss: 0.0039\n",
            "Epoch [3/5], Batch [1650/4062], Loss: 0.0042\n",
            "Epoch [3/5], Batch [1660/4062], Loss: 0.0106\n",
            "Epoch [3/5], Batch [1670/4062], Loss: 0.0113\n",
            "Epoch [3/5], Batch [1680/4062], Loss: 0.0105\n",
            "Epoch [3/5], Batch [1690/4062], Loss: 0.0201\n",
            "Epoch [3/5], Batch [1700/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [1710/4062], Loss: 0.0239\n",
            "Epoch [3/5], Batch [1720/4062], Loss: 0.0158\n",
            "Epoch [3/5], Batch [1730/4062], Loss: 0.0072\n",
            "Epoch [3/5], Batch [1740/4062], Loss: 0.0043\n",
            "Epoch [3/5], Batch [1750/4062], Loss: 0.0076\n",
            "Epoch [3/5], Batch [1760/4062], Loss: 0.0113\n",
            "Epoch [3/5], Batch [1770/4062], Loss: 0.0065\n",
            "Epoch [3/5], Batch [1780/4062], Loss: 0.0076\n",
            "Epoch [3/5], Batch [1790/4062], Loss: 0.0210\n",
            "Epoch [3/5], Batch [1800/4062], Loss: 0.0170\n",
            "Epoch [3/5], Batch [1810/4062], Loss: 0.0024\n",
            "Epoch [3/5], Batch [1820/4062], Loss: 0.0068\n",
            "Epoch [3/5], Batch [1830/4062], Loss: 0.0359\n",
            "Epoch [3/5], Batch [1840/4062], Loss: 0.0223\n",
            "Epoch [3/5], Batch [1850/4062], Loss: 0.0189\n",
            "Epoch [3/5], Batch [1860/4062], Loss: 0.0267\n",
            "Epoch [3/5], Batch [1870/4062], Loss: 0.0084\n",
            "Epoch [3/5], Batch [1880/4062], Loss: 0.0109\n",
            "Epoch [3/5], Batch [1890/4062], Loss: 0.0135\n",
            "Epoch [3/5], Batch [1900/4062], Loss: 0.0314\n",
            "Epoch [3/5], Batch [1910/4062], Loss: 0.0152\n",
            "Epoch [3/5], Batch [1920/4062], Loss: 0.0121\n",
            "Epoch [3/5], Batch [1930/4062], Loss: 0.0105\n",
            "Epoch [3/5], Batch [1940/4062], Loss: 0.0192\n",
            "Epoch [3/5], Batch [1950/4062], Loss: 0.0060\n",
            "Epoch [3/5], Batch [1960/4062], Loss: 0.0190\n",
            "Epoch [3/5], Batch [1970/4062], Loss: 0.0105\n",
            "Epoch [3/5], Batch [1980/4062], Loss: 0.0222\n",
            "Epoch [3/5], Batch [1990/4062], Loss: 0.0052\n",
            "Epoch [3/5], Batch [2000/4062], Loss: 0.0064\n",
            "Epoch [3/5], Batch [2010/4062], Loss: 0.0289\n",
            "Epoch [3/5], Batch [2020/4062], Loss: 0.0054\n",
            "Epoch [3/5], Batch [2030/4062], Loss: 0.0060\n",
            "Epoch [3/5], Batch [2040/4062], Loss: 0.0113\n",
            "Epoch [3/5], Batch [2050/4062], Loss: 0.0092\n",
            "Epoch [3/5], Batch [2060/4062], Loss: 0.0133\n",
            "Epoch [3/5], Batch [2070/4062], Loss: 0.0256\n",
            "Epoch [3/5], Batch [2080/4062], Loss: 0.0151\n",
            "Epoch [3/5], Batch [2090/4062], Loss: 0.0157\n",
            "Epoch [3/5], Batch [2100/4062], Loss: 0.0161\n",
            "Epoch [3/5], Batch [2110/4062], Loss: 0.0060\n",
            "Epoch [3/5], Batch [2120/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [2130/4062], Loss: 0.0247\n",
            "Epoch [3/5], Batch [2140/4062], Loss: 0.0094\n",
            "Epoch [3/5], Batch [2150/4062], Loss: 0.0333\n",
            "Epoch [3/5], Batch [2160/4062], Loss: 0.0159\n",
            "Epoch [3/5], Batch [2170/4062], Loss: 0.0154\n",
            "Epoch [3/5], Batch [2180/4062], Loss: 0.0027\n",
            "Epoch [3/5], Batch [2190/4062], Loss: 0.0033\n",
            "Epoch [3/5], Batch [2200/4062], Loss: 0.0064\n",
            "Epoch [3/5], Batch [2210/4062], Loss: 0.0040\n",
            "Epoch [3/5], Batch [2220/4062], Loss: 0.0150\n",
            "Epoch [3/5], Batch [2230/4062], Loss: 0.0160\n",
            "Epoch [3/5], Batch [2240/4062], Loss: 0.0062\n",
            "Epoch [3/5], Batch [2250/4062], Loss: 0.0044\n",
            "Epoch [3/5], Batch [2260/4062], Loss: 0.0164\n",
            "Epoch [3/5], Batch [2270/4062], Loss: 0.0044\n",
            "Epoch [3/5], Batch [2280/4062], Loss: 0.0203\n",
            "Epoch [3/5], Batch [2290/4062], Loss: 0.0167\n",
            "Epoch [3/5], Batch [2300/4062], Loss: 0.0265\n",
            "Epoch [3/5], Batch [2310/4062], Loss: 0.0140\n",
            "Epoch [3/5], Batch [2320/4062], Loss: 0.0072\n",
            "Epoch [3/5], Batch [2330/4062], Loss: 0.0106\n",
            "Epoch [3/5], Batch [2340/4062], Loss: 0.0084\n",
            "Epoch [3/5], Batch [2350/4062], Loss: 0.0125\n",
            "Epoch [3/5], Batch [2360/4062], Loss: 0.0105\n",
            "Epoch [3/5], Batch [2370/4062], Loss: 0.0054\n",
            "Epoch [3/5], Batch [2380/4062], Loss: 0.0057\n",
            "Epoch [3/5], Batch [2390/4062], Loss: 0.0128\n",
            "Epoch [3/5], Batch [2400/4062], Loss: 0.0097\n",
            "Epoch [3/5], Batch [2410/4062], Loss: 0.0077\n",
            "Epoch [3/5], Batch [2420/4062], Loss: 0.0122\n",
            "Epoch [3/5], Batch [2430/4062], Loss: 0.0160\n",
            "Epoch [3/5], Batch [2440/4062], Loss: 0.0049\n",
            "Epoch [3/5], Batch [2450/4062], Loss: 0.0081\n",
            "Epoch [3/5], Batch [2460/4062], Loss: 0.0014\n",
            "Epoch [3/5], Batch [2470/4062], Loss: 0.0105\n",
            "Epoch [3/5], Batch [2480/4062], Loss: 0.0129\n",
            "Epoch [3/5], Batch [2490/4062], Loss: 0.0189\n",
            "Epoch [3/5], Batch [2500/4062], Loss: 0.0093\n",
            "Epoch [3/5], Batch [2510/4062], Loss: 0.0036\n",
            "Epoch [3/5], Batch [2520/4062], Loss: 0.0040\n",
            "Epoch [3/5], Batch [2530/4062], Loss: 0.0020\n",
            "Epoch [3/5], Batch [2540/4062], Loss: 0.0060\n",
            "Epoch [3/5], Batch [2550/4062], Loss: 0.0108\n",
            "Epoch [3/5], Batch [2560/4062], Loss: 0.0025\n",
            "Epoch [3/5], Batch [2570/4062], Loss: 0.0190\n",
            "Epoch [3/5], Batch [2580/4062], Loss: 0.0039\n",
            "Epoch [3/5], Batch [2590/4062], Loss: 0.0109\n",
            "Epoch [3/5], Batch [2600/4062], Loss: 0.0181\n",
            "Epoch [3/5], Batch [2610/4062], Loss: 0.0066\n",
            "Epoch [3/5], Batch [2620/4062], Loss: 0.0048\n",
            "Epoch [3/5], Batch [2630/4062], Loss: 0.0269\n",
            "Epoch [3/5], Batch [2640/4062], Loss: 0.0055\n",
            "Epoch [3/5], Batch [2650/4062], Loss: 0.0076\n",
            "Epoch [3/5], Batch [2660/4062], Loss: 0.0132\n",
            "Epoch [3/5], Batch [2670/4062], Loss: 0.0206\n",
            "Epoch [3/5], Batch [2680/4062], Loss: 0.0129\n",
            "Epoch [3/5], Batch [2690/4062], Loss: 0.0190\n",
            "Epoch [3/5], Batch [2700/4062], Loss: 0.0080\n",
            "Epoch [3/5], Batch [2710/4062], Loss: 0.0095\n",
            "Epoch [3/5], Batch [2720/4062], Loss: 0.0266\n",
            "Epoch [3/5], Batch [2730/4062], Loss: 0.0202\n",
            "Epoch [3/5], Batch [2740/4062], Loss: 0.0047\n",
            "Epoch [3/5], Batch [2750/4062], Loss: 0.0143\n",
            "Epoch [3/5], Batch [2760/4062], Loss: 0.0157\n",
            "Epoch [3/5], Batch [2770/4062], Loss: 0.0120\n",
            "Epoch [3/5], Batch [2780/4062], Loss: 0.0054\n",
            "Epoch [3/5], Batch [2790/4062], Loss: 0.0113\n",
            "Epoch [3/5], Batch [2800/4062], Loss: 0.0024\n",
            "Epoch [3/5], Batch [2810/4062], Loss: 0.0079\n",
            "Epoch [3/5], Batch [2820/4062], Loss: 0.0202\n",
            "Epoch [3/5], Batch [2830/4062], Loss: 0.0061\n",
            "Epoch [3/5], Batch [2840/4062], Loss: 0.0076\n",
            "Epoch [3/5], Batch [2850/4062], Loss: 0.0087\n",
            "Epoch [3/5], Batch [2860/4062], Loss: 0.0031\n",
            "Epoch [3/5], Batch [2870/4062], Loss: 0.0148\n",
            "Epoch [3/5], Batch [2880/4062], Loss: 0.0055\n",
            "Epoch [3/5], Batch [2890/4062], Loss: 0.0072\n",
            "Epoch [3/5], Batch [2900/4062], Loss: 0.0137\n",
            "Epoch [3/5], Batch [2910/4062], Loss: 0.0149\n",
            "Epoch [3/5], Batch [2920/4062], Loss: 0.0093\n",
            "Epoch [3/5], Batch [2930/4062], Loss: 0.0251\n",
            "Epoch [3/5], Batch [2940/4062], Loss: 0.0319\n",
            "Epoch [3/5], Batch [2950/4062], Loss: 0.0388\n",
            "Epoch [3/5], Batch [2960/4062], Loss: 0.0098\n",
            "Epoch [3/5], Batch [2970/4062], Loss: 0.0125\n",
            "Epoch [3/5], Batch [2980/4062], Loss: 0.0083\n",
            "Epoch [3/5], Batch [2990/4062], Loss: 0.0069\n",
            "Epoch [3/5], Batch [3000/4062], Loss: 0.0091\n",
            "Epoch [3/5], Batch [3010/4062], Loss: 0.0133\n",
            "Epoch [3/5], Batch [3020/4062], Loss: 0.0200\n",
            "Epoch [3/5], Batch [3030/4062], Loss: 0.0069\n",
            "Epoch [3/5], Batch [3040/4062], Loss: 0.0182\n",
            "Epoch [3/5], Batch [3050/4062], Loss: 0.0521\n",
            "Epoch [3/5], Batch [3060/4062], Loss: 0.0307\n",
            "Epoch [3/5], Batch [3070/4062], Loss: 0.0081\n",
            "Epoch [3/5], Batch [3080/4062], Loss: 0.0100\n",
            "Epoch [3/5], Batch [3090/4062], Loss: 0.0033\n",
            "Epoch [3/5], Batch [3100/4062], Loss: 0.0077\n",
            "Epoch [3/5], Batch [3110/4062], Loss: 0.0071\n",
            "Epoch [3/5], Batch [3120/4062], Loss: 0.0209\n",
            "Epoch [3/5], Batch [3130/4062], Loss: 0.0117\n",
            "Epoch [3/5], Batch [3140/4062], Loss: 0.0127\n",
            "Epoch [3/5], Batch [3150/4062], Loss: 0.0059\n",
            "Epoch [3/5], Batch [3160/4062], Loss: 0.0115\n",
            "Epoch [3/5], Batch [3170/4062], Loss: 0.0102\n",
            "Epoch [3/5], Batch [3180/4062], Loss: 0.0071\n",
            "Epoch [3/5], Batch [3190/4062], Loss: 0.0025\n",
            "Epoch [3/5], Batch [3200/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [3210/4062], Loss: 0.0125\n",
            "Epoch [3/5], Batch [3220/4062], Loss: 0.0024\n",
            "Epoch [3/5], Batch [3230/4062], Loss: 0.0102\n",
            "Epoch [3/5], Batch [3240/4062], Loss: 0.0092\n",
            "Epoch [3/5], Batch [3250/4062], Loss: 0.0055\n",
            "Epoch [3/5], Batch [3260/4062], Loss: 0.0096\n",
            "Epoch [3/5], Batch [3270/4062], Loss: 0.0172\n",
            "Epoch [3/5], Batch [3280/4062], Loss: 0.0153\n",
            "Epoch [3/5], Batch [3290/4062], Loss: 0.0038\n",
            "Epoch [3/5], Batch [3300/4062], Loss: 0.0141\n",
            "Epoch [3/5], Batch [3310/4062], Loss: 0.0054\n",
            "Epoch [3/5], Batch [3320/4062], Loss: 0.0064\n",
            "Epoch [3/5], Batch [3330/4062], Loss: 0.0129\n",
            "Epoch [3/5], Batch [3340/4062], Loss: 0.0086\n",
            "Epoch [3/5], Batch [3350/4062], Loss: 0.0135\n",
            "Epoch [3/5], Batch [3360/4062], Loss: 0.0104\n",
            "Epoch [3/5], Batch [3370/4062], Loss: 0.0114\n",
            "Epoch [3/5], Batch [3380/4062], Loss: 0.0082\n",
            "Epoch [3/5], Batch [3390/4062], Loss: 0.0133\n",
            "Epoch [3/5], Batch [3400/4062], Loss: 0.0052\n",
            "Epoch [3/5], Batch [3410/4062], Loss: 0.0050\n",
            "Epoch [3/5], Batch [3420/4062], Loss: 0.0102\n",
            "Epoch [3/5], Batch [3430/4062], Loss: 0.0080\n",
            "Epoch [3/5], Batch [3440/4062], Loss: 0.0196\n",
            "Epoch [3/5], Batch [3450/4062], Loss: 0.0077\n",
            "Epoch [3/5], Batch [3460/4062], Loss: 0.0077\n",
            "Epoch [3/5], Batch [3470/4062], Loss: 0.0216\n",
            "Epoch [3/5], Batch [3480/4062], Loss: 0.0120\n",
            "Epoch [3/5], Batch [3490/4062], Loss: 0.0160\n",
            "Epoch [3/5], Batch [3500/4062], Loss: 0.0066\n",
            "Epoch [3/5], Batch [3510/4062], Loss: 0.0110\n",
            "Epoch [3/5], Batch [3520/4062], Loss: 0.0057\n",
            "Epoch [3/5], Batch [3530/4062], Loss: 0.0070\n",
            "Epoch [3/5], Batch [3540/4062], Loss: 0.0156\n",
            "Epoch [3/5], Batch [3550/4062], Loss: 0.0023\n",
            "Epoch [3/5], Batch [3560/4062], Loss: 0.0128\n",
            "Epoch [3/5], Batch [3570/4062], Loss: 0.0093\n",
            "Epoch [3/5], Batch [3580/4062], Loss: 0.0074\n",
            "Epoch [3/5], Batch [3590/4062], Loss: 0.0110\n",
            "Epoch [3/5], Batch [3600/4062], Loss: 0.0085\n",
            "Epoch [3/5], Batch [3610/4062], Loss: 0.0109\n",
            "Epoch [3/5], Batch [3620/4062], Loss: 0.0278\n",
            "Epoch [3/5], Batch [3630/4062], Loss: 0.0117\n",
            "Epoch [3/5], Batch [3640/4062], Loss: 0.0128\n",
            "Epoch [3/5], Batch [3650/4062], Loss: 0.0308\n",
            "Epoch [3/5], Batch [3660/4062], Loss: 0.0143\n",
            "Epoch [3/5], Batch [3670/4062], Loss: 0.0300\n",
            "Epoch [3/5], Batch [3680/4062], Loss: 0.0090\n",
            "Epoch [3/5], Batch [3690/4062], Loss: 0.0163\n",
            "Epoch [3/5], Batch [3700/4062], Loss: 0.0191\n",
            "Epoch [3/5], Batch [3710/4062], Loss: 0.0090\n",
            "Epoch [3/5], Batch [3720/4062], Loss: 0.0101\n",
            "Epoch [3/5], Batch [3730/4062], Loss: 0.0087\n",
            "Epoch [3/5], Batch [3740/4062], Loss: 0.0040\n",
            "Epoch [3/5], Batch [3750/4062], Loss: 0.0197\n",
            "Epoch [3/5], Batch [3760/4062], Loss: 0.0307\n",
            "Epoch [3/5], Batch [3770/4062], Loss: 0.0128\n",
            "Epoch [3/5], Batch [3780/4062], Loss: 0.0065\n",
            "Epoch [3/5], Batch [3790/4062], Loss: 0.0092\n",
            "Epoch [3/5], Batch [3800/4062], Loss: 0.0101\n",
            "Epoch [3/5], Batch [3810/4062], Loss: 0.0143\n",
            "Epoch [3/5], Batch [3820/4062], Loss: 0.0478\n",
            "Epoch [3/5], Batch [3830/4062], Loss: 0.0039\n",
            "Epoch [3/5], Batch [3840/4062], Loss: 0.0028\n",
            "Epoch [3/5], Batch [3850/4062], Loss: 0.0084\n",
            "Epoch [3/5], Batch [3860/4062], Loss: 0.0146\n",
            "Epoch [3/5], Batch [3870/4062], Loss: 0.0148\n",
            "Epoch [3/5], Batch [3880/4062], Loss: 0.0410\n",
            "Epoch [3/5], Batch [3890/4062], Loss: 0.0112\n",
            "Epoch [3/5], Batch [3900/4062], Loss: 0.0156\n",
            "Epoch [3/5], Batch [3910/4062], Loss: 0.0094\n",
            "Epoch [3/5], Batch [3920/4062], Loss: 0.0087\n",
            "Epoch [3/5], Batch [3930/4062], Loss: 0.0109\n",
            "Epoch [3/5], Batch [3940/4062], Loss: 0.0050\n",
            "Epoch [3/5], Batch [3950/4062], Loss: 0.0130\n",
            "Epoch [3/5], Batch [3960/4062], Loss: 0.0033\n",
            "Epoch [3/5], Batch [3970/4062], Loss: 0.0088\n",
            "Epoch [3/5], Batch [3980/4062], Loss: 0.0326\n",
            "Epoch [3/5], Batch [3990/4062], Loss: 0.0067\n",
            "Epoch [3/5], Batch [4000/4062], Loss: 0.0120\n",
            "Epoch [3/5], Batch [4010/4062], Loss: 0.0067\n",
            "Epoch [3/5], Batch [4020/4062], Loss: 0.0058\n",
            "Epoch [3/5], Batch [4030/4062], Loss: 0.0059\n",
            "Epoch [3/5], Batch [4040/4062], Loss: 0.0043\n",
            "Epoch [3/5], Batch [4050/4062], Loss: 0.0096\n",
            "Epoch [3/5], Batch [4060/4062], Loss: 0.0082\n",
            "Epoch 3/5, Loss: 0.012269105155089506\n",
            "Updated Learning Rate: [2.50047e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.98      0.90      0.94       753\n",
            "   overdrive       1.00      0.96      0.98      3012\n",
            "  distortion       1.00      0.99      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      3765\n",
            "      phaser       0.99      1.00      1.00      4518\n",
            "     flanger       0.95      1.00      0.97      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       1.00      0.89      0.94      6777\n",
            " hall_reverb       0.99      0.87      0.93      4518\n",
            "plate_reverb       0.96      0.96      0.96      3012\n",
            "     octaver       0.98      1.00      0.99      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.97      0.98     50451\n",
            "   macro avg       0.99      0.97      0.98     50451\n",
            "weighted avg       0.99      0.97      0.98     50451\n",
            " samples avg       0.98      0.96      0.97     50451\n",
            "\n",
            "\n",
            "Validation Loss: 0.0161, Accuracy: 0.9320, Precision: 0.9883, Recall: 0.9665, F1-score: 0.9767\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5], Batch [10/4062], Loss: 0.0071\n",
            "Epoch [4/5], Batch [20/4062], Loss: 0.0054\n",
            "Epoch [4/5], Batch [30/4062], Loss: 0.0101\n",
            "Epoch [4/5], Batch [40/4062], Loss: 0.0080\n",
            "Epoch [4/5], Batch [50/4062], Loss: 0.0093\n",
            "Epoch [4/5], Batch [60/4062], Loss: 0.0071\n",
            "Epoch [4/5], Batch [70/4062], Loss: 0.0092\n",
            "Epoch [4/5], Batch [80/4062], Loss: 0.0100\n",
            "Epoch [4/5], Batch [90/4062], Loss: 0.0135\n",
            "Epoch [4/5], Batch [100/4062], Loss: 0.0209\n",
            "Epoch [4/5], Batch [110/4062], Loss: 0.0060\n",
            "Epoch [4/5], Batch [120/4062], Loss: 0.0083\n",
            "Epoch [4/5], Batch [130/4062], Loss: 0.0088\n",
            "Epoch [4/5], Batch [140/4062], Loss: 0.0058\n",
            "Epoch [4/5], Batch [150/4062], Loss: 0.0080\n",
            "Epoch [4/5], Batch [160/4062], Loss: 0.0123\n",
            "Epoch [4/5], Batch [170/4062], Loss: 0.0065\n",
            "Epoch [4/5], Batch [180/4062], Loss: 0.0099\n",
            "Epoch [4/5], Batch [190/4062], Loss: 0.0095\n",
            "Epoch [4/5], Batch [200/4062], Loss: 0.0095\n",
            "Epoch [4/5], Batch [210/4062], Loss: 0.0062\n",
            "Epoch [4/5], Batch [220/4062], Loss: 0.0121\n",
            "Epoch [4/5], Batch [230/4062], Loss: 0.0080\n",
            "Epoch [4/5], Batch [240/4062], Loss: 0.0085\n",
            "Epoch [4/5], Batch [250/4062], Loss: 0.0072\n",
            "Epoch [4/5], Batch [260/4062], Loss: 0.0073\n",
            "Epoch [4/5], Batch [270/4062], Loss: 0.0130\n",
            "Epoch [4/5], Batch [280/4062], Loss: 0.0053\n",
            "Epoch [4/5], Batch [290/4062], Loss: 0.0106\n",
            "Epoch [4/5], Batch [300/4062], Loss: 0.0027\n",
            "Epoch [4/5], Batch [310/4062], Loss: 0.0137\n",
            "Epoch [4/5], Batch [320/4062], Loss: 0.0128\n",
            "Epoch [4/5], Batch [330/4062], Loss: 0.0136\n",
            "Epoch [4/5], Batch [340/4062], Loss: 0.0062\n",
            "Epoch [4/5], Batch [350/4062], Loss: 0.0122\n",
            "Epoch [4/5], Batch [360/4062], Loss: 0.0087\n",
            "Epoch [4/5], Batch [370/4062], Loss: 0.0051\n",
            "Epoch [4/5], Batch [380/4062], Loss: 0.0259\n",
            "Epoch [4/5], Batch [390/4062], Loss: 0.0040\n",
            "Epoch [4/5], Batch [400/4062], Loss: 0.0061\n",
            "Epoch [4/5], Batch [410/4062], Loss: 0.0314\n",
            "Epoch [4/5], Batch [420/4062], Loss: 0.0096\n",
            "Epoch [4/5], Batch [430/4062], Loss: 0.0152\n",
            "Epoch [4/5], Batch [440/4062], Loss: 0.0055\n",
            "Epoch [4/5], Batch [450/4062], Loss: 0.0164\n",
            "Epoch [4/5], Batch [460/4062], Loss: 0.0031\n",
            "Epoch [4/5], Batch [470/4062], Loss: 0.0146\n",
            "Epoch [4/5], Batch [480/4062], Loss: 0.0320\n",
            "Epoch [4/5], Batch [490/4062], Loss: 0.0072\n",
            "Epoch [4/5], Batch [500/4062], Loss: 0.0023\n",
            "Epoch [4/5], Batch [510/4062], Loss: 0.0136\n",
            "Epoch [4/5], Batch [520/4062], Loss: 0.0123\n",
            "Epoch [4/5], Batch [530/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [540/4062], Loss: 0.0097\n",
            "Epoch [4/5], Batch [550/4062], Loss: 0.0016\n",
            "Epoch [4/5], Batch [560/4062], Loss: 0.0122\n",
            "Epoch [4/5], Batch [570/4062], Loss: 0.0019\n",
            "Epoch [4/5], Batch [580/4062], Loss: 0.0183\n",
            "Epoch [4/5], Batch [590/4062], Loss: 0.0052\n",
            "Epoch [4/5], Batch [600/4062], Loss: 0.0094\n",
            "Epoch [4/5], Batch [610/4062], Loss: 0.0220\n",
            "Epoch [4/5], Batch [620/4062], Loss: 0.0095\n",
            "Epoch [4/5], Batch [630/4062], Loss: 0.0054\n",
            "Epoch [4/5], Batch [640/4062], Loss: 0.0035\n",
            "Epoch [4/5], Batch [650/4062], Loss: 0.0318\n",
            "Epoch [4/5], Batch [660/4062], Loss: 0.0225\n",
            "Epoch [4/5], Batch [670/4062], Loss: 0.0223\n",
            "Epoch [4/5], Batch [680/4062], Loss: 0.0097\n",
            "Epoch [4/5], Batch [690/4062], Loss: 0.0261\n",
            "Epoch [4/5], Batch [700/4062], Loss: 0.0130\n",
            "Epoch [4/5], Batch [710/4062], Loss: 0.0126\n",
            "Epoch [4/5], Batch [720/4062], Loss: 0.0113\n",
            "Epoch [4/5], Batch [730/4062], Loss: 0.0028\n",
            "Epoch [4/5], Batch [740/4062], Loss: 0.0046\n",
            "Epoch [4/5], Batch [750/4062], Loss: 0.0092\n",
            "Epoch [4/5], Batch [760/4062], Loss: 0.0014\n",
            "Epoch [4/5], Batch [770/4062], Loss: 0.0102\n",
            "Epoch [4/5], Batch [780/4062], Loss: 0.0074\n",
            "Epoch [4/5], Batch [790/4062], Loss: 0.0066\n",
            "Epoch [4/5], Batch [800/4062], Loss: 0.0099\n",
            "Epoch [4/5], Batch [810/4062], Loss: 0.0056\n",
            "Epoch [4/5], Batch [820/4062], Loss: 0.0096\n",
            "Epoch [4/5], Batch [830/4062], Loss: 0.0042\n",
            "Epoch [4/5], Batch [840/4062], Loss: 0.0053\n",
            "Epoch [4/5], Batch [850/4062], Loss: 0.0140\n",
            "Epoch [4/5], Batch [860/4062], Loss: 0.0196\n",
            "Epoch [4/5], Batch [870/4062], Loss: 0.0082\n",
            "Epoch [4/5], Batch [880/4062], Loss: 0.0073\n",
            "Epoch [4/5], Batch [890/4062], Loss: 0.0144\n",
            "Epoch [4/5], Batch [900/4062], Loss: 0.0065\n",
            "Epoch [4/5], Batch [910/4062], Loss: 0.0097\n",
            "Epoch [4/5], Batch [920/4062], Loss: 0.0032\n",
            "Epoch [4/5], Batch [930/4062], Loss: 0.0023\n",
            "Epoch [4/5], Batch [940/4062], Loss: 0.0020\n",
            "Epoch [4/5], Batch [950/4062], Loss: 0.0076\n",
            "Epoch [4/5], Batch [960/4062], Loss: 0.0037\n",
            "Epoch [4/5], Batch [970/4062], Loss: 0.0034\n",
            "Epoch [4/5], Batch [980/4062], Loss: 0.0125\n",
            "Epoch [4/5], Batch [990/4062], Loss: 0.0062\n",
            "Epoch [4/5], Batch [1000/4062], Loss: 0.0078\n",
            "Epoch [4/5], Batch [1010/4062], Loss: 0.0047\n",
            "Epoch [4/5], Batch [1020/4062], Loss: 0.0025\n",
            "Epoch [4/5], Batch [1030/4062], Loss: 0.0078\n",
            "Epoch [4/5], Batch [1040/4062], Loss: 0.0433\n",
            "Epoch [4/5], Batch [1050/4062], Loss: 0.0086\n",
            "Epoch [4/5], Batch [1060/4062], Loss: 0.0119\n",
            "Epoch [4/5], Batch [1070/4062], Loss: 0.0064\n",
            "Epoch [4/5], Batch [1080/4062], Loss: 0.0135\n",
            "Epoch [4/5], Batch [1090/4062], Loss: 0.0088\n",
            "Epoch [4/5], Batch [1100/4062], Loss: 0.0098\n",
            "Epoch [4/5], Batch [1110/4062], Loss: 0.0056\n",
            "Epoch [4/5], Batch [1120/4062], Loss: 0.0047\n",
            "Epoch [4/5], Batch [1130/4062], Loss: 0.0020\n",
            "Epoch [4/5], Batch [1140/4062], Loss: 0.0018\n",
            "Epoch [4/5], Batch [1150/4062], Loss: 0.0028\n",
            "Epoch [4/5], Batch [1160/4062], Loss: 0.0024\n",
            "Epoch [4/5], Batch [1170/4062], Loss: 0.0063\n",
            "Epoch [4/5], Batch [1180/4062], Loss: 0.0143\n",
            "Epoch [4/5], Batch [1190/4062], Loss: 0.0134\n",
            "Epoch [4/5], Batch [1200/4062], Loss: 0.0079\n",
            "Epoch [4/5], Batch [1210/4062], Loss: 0.0047\n",
            "Epoch [4/5], Batch [1220/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [1230/4062], Loss: 0.0226\n",
            "Epoch [4/5], Batch [1240/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [1250/4062], Loss: 0.0118\n",
            "Epoch [4/5], Batch [1260/4062], Loss: 0.0167\n",
            "Epoch [4/5], Batch [1270/4062], Loss: 0.0042\n",
            "Epoch [4/5], Batch [1280/4062], Loss: 0.0117\n",
            "Epoch [4/5], Batch [1290/4062], Loss: 0.0257\n",
            "Epoch [4/5], Batch [1300/4062], Loss: 0.0147\n",
            "Epoch [4/5], Batch [1310/4062], Loss: 0.0122\n",
            "Epoch [4/5], Batch [1320/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [1330/4062], Loss: 0.0038\n",
            "Epoch [4/5], Batch [1340/4062], Loss: 0.0051\n",
            "Epoch [4/5], Batch [1350/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [1360/4062], Loss: 0.0185\n",
            "Epoch [4/5], Batch [1370/4062], Loss: 0.0145\n",
            "Epoch [4/5], Batch [1380/4062], Loss: 0.0103\n",
            "Epoch [4/5], Batch [1390/4062], Loss: 0.0168\n",
            "Epoch [4/5], Batch [1400/4062], Loss: 0.0024\n",
            "Epoch [4/5], Batch [1410/4062], Loss: 0.0122\n",
            "Epoch [4/5], Batch [1420/4062], Loss: 0.0137\n",
            "Epoch [4/5], Batch [1430/4062], Loss: 0.0023\n",
            "Epoch [4/5], Batch [1440/4062], Loss: 0.0124\n",
            "Epoch [4/5], Batch [1450/4062], Loss: 0.0053\n",
            "Epoch [4/5], Batch [1460/4062], Loss: 0.0150\n",
            "Epoch [4/5], Batch [1470/4062], Loss: 0.0030\n",
            "Epoch [4/5], Batch [1480/4062], Loss: 0.0064\n",
            "Epoch [4/5], Batch [1490/4062], Loss: 0.0022\n",
            "Epoch [4/5], Batch [1500/4062], Loss: 0.0047\n",
            "Epoch [4/5], Batch [1510/4062], Loss: 0.0118\n",
            "Epoch [4/5], Batch [1520/4062], Loss: 0.0108\n",
            "Epoch [4/5], Batch [1530/4062], Loss: 0.0092\n",
            "Epoch [4/5], Batch [1540/4062], Loss: 0.0017\n",
            "Epoch [4/5], Batch [1550/4062], Loss: 0.0030\n",
            "Epoch [4/5], Batch [1560/4062], Loss: 0.0039\n",
            "Epoch [4/5], Batch [1570/4062], Loss: 0.0276\n",
            "Epoch [4/5], Batch [1580/4062], Loss: 0.0079\n",
            "Epoch [4/5], Batch [1590/4062], Loss: 0.0115\n",
            "Epoch [4/5], Batch [1600/4062], Loss: 0.0021\n",
            "Epoch [4/5], Batch [1610/4062], Loss: 0.0094\n",
            "Epoch [4/5], Batch [1620/4062], Loss: 0.0069\n",
            "Epoch [4/5], Batch [1630/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [1640/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [1650/4062], Loss: 0.0108\n",
            "Epoch [4/5], Batch [1660/4062], Loss: 0.0060\n",
            "Epoch [4/5], Batch [1670/4062], Loss: 0.0248\n",
            "Epoch [4/5], Batch [1680/4062], Loss: 0.0043\n",
            "Epoch [4/5], Batch [1690/4062], Loss: 0.0140\n",
            "Epoch [4/5], Batch [1700/4062], Loss: 0.0064\n",
            "Epoch [4/5], Batch [1710/4062], Loss: 0.0064\n",
            "Epoch [4/5], Batch [1720/4062], Loss: 0.0033\n",
            "Epoch [4/5], Batch [1730/4062], Loss: 0.0096\n",
            "Epoch [4/5], Batch [1740/4062], Loss: 0.0132\n",
            "Epoch [4/5], Batch [1750/4062], Loss: 0.0037\n",
            "Epoch [4/5], Batch [1760/4062], Loss: 0.0034\n",
            "Epoch [4/5], Batch [1770/4062], Loss: 0.0059\n",
            "Epoch [4/5], Batch [1780/4062], Loss: 0.0131\n",
            "Epoch [4/5], Batch [1790/4062], Loss: 0.0080\n",
            "Epoch [4/5], Batch [1800/4062], Loss: 0.0150\n",
            "Epoch [4/5], Batch [1810/4062], Loss: 0.0284\n",
            "Epoch [4/5], Batch [1820/4062], Loss: 0.0065\n",
            "Epoch [4/5], Batch [1830/4062], Loss: 0.0088\n",
            "Epoch [4/5], Batch [1840/4062], Loss: 0.0101\n",
            "Epoch [4/5], Batch [1850/4062], Loss: 0.0064\n",
            "Epoch [4/5], Batch [1860/4062], Loss: 0.0131\n",
            "Epoch [4/5], Batch [1870/4062], Loss: 0.0084\n",
            "Epoch [4/5], Batch [1880/4062], Loss: 0.0095\n",
            "Epoch [4/5], Batch [1890/4062], Loss: 0.0212\n",
            "Epoch [4/5], Batch [1900/4062], Loss: 0.0031\n",
            "Epoch [4/5], Batch [1910/4062], Loss: 0.0134\n",
            "Epoch [4/5], Batch [1920/4062], Loss: 0.0035\n",
            "Epoch [4/5], Batch [1930/4062], Loss: 0.0134\n",
            "Epoch [4/5], Batch [1940/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [1950/4062], Loss: 0.0084\n",
            "Epoch [4/5], Batch [1960/4062], Loss: 0.0108\n",
            "Epoch [4/5], Batch [1970/4062], Loss: 0.0124\n",
            "Epoch [4/5], Batch [1980/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [1990/4062], Loss: 0.0035\n",
            "Epoch [4/5], Batch [2000/4062], Loss: 0.0032\n",
            "Epoch [4/5], Batch [2010/4062], Loss: 0.0147\n",
            "Epoch [4/5], Batch [2020/4062], Loss: 0.0057\n",
            "Epoch [4/5], Batch [2030/4062], Loss: 0.0058\n",
            "Epoch [4/5], Batch [2040/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [2050/4062], Loss: 0.0049\n",
            "Epoch [4/5], Batch [2060/4062], Loss: 0.0080\n",
            "Epoch [4/5], Batch [2070/4062], Loss: 0.0039\n",
            "Epoch [4/5], Batch [2080/4062], Loss: 0.0110\n",
            "Epoch [4/5], Batch [2090/4062], Loss: 0.0079\n",
            "Epoch [4/5], Batch [2100/4062], Loss: 0.0263\n",
            "Epoch [4/5], Batch [2110/4062], Loss: 0.0035\n",
            "Epoch [4/5], Batch [2120/4062], Loss: 0.0090\n",
            "Epoch [4/5], Batch [2130/4062], Loss: 0.0026\n",
            "Epoch [4/5], Batch [2140/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [2150/4062], Loss: 0.0076\n",
            "Epoch [4/5], Batch [2160/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [2170/4062], Loss: 0.0030\n",
            "Epoch [4/5], Batch [2180/4062], Loss: 0.0078\n",
            "Epoch [4/5], Batch [2190/4062], Loss: 0.0034\n",
            "Epoch [4/5], Batch [2200/4062], Loss: 0.0013\n",
            "Epoch [4/5], Batch [2210/4062], Loss: 0.0055\n",
            "Epoch [4/5], Batch [2220/4062], Loss: 0.0025\n",
            "Epoch [4/5], Batch [2230/4062], Loss: 0.0060\n",
            "Epoch [4/5], Batch [2240/4062], Loss: 0.0056\n",
            "Epoch [4/5], Batch [2250/4062], Loss: 0.0222\n",
            "Epoch [4/5], Batch [2260/4062], Loss: 0.0079\n",
            "Epoch [4/5], Batch [2270/4062], Loss: 0.0165\n",
            "Epoch [4/5], Batch [2280/4062], Loss: 0.0067\n",
            "Epoch [4/5], Batch [2290/4062], Loss: 0.0084\n",
            "Epoch [4/5], Batch [2300/4062], Loss: 0.0044\n",
            "Epoch [4/5], Batch [2310/4062], Loss: 0.0031\n",
            "Epoch [4/5], Batch [2320/4062], Loss: 0.0259\n",
            "Epoch [4/5], Batch [2330/4062], Loss: 0.0070\n",
            "Epoch [4/5], Batch [2340/4062], Loss: 0.0017\n",
            "Epoch [4/5], Batch [2350/4062], Loss: 0.0046\n",
            "Epoch [4/5], Batch [2360/4062], Loss: 0.0033\n",
            "Epoch [4/5], Batch [2370/4062], Loss: 0.0036\n",
            "Epoch [4/5], Batch [2380/4062], Loss: 0.0023\n",
            "Epoch [4/5], Batch [2390/4062], Loss: 0.0104\n",
            "Epoch [4/5], Batch [2400/4062], Loss: 0.0025\n",
            "Epoch [4/5], Batch [2410/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [2420/4062], Loss: 0.0018\n",
            "Epoch [4/5], Batch [2430/4062], Loss: 0.0052\n",
            "Epoch [4/5], Batch [2440/4062], Loss: 0.0067\n",
            "Epoch [4/5], Batch [2450/4062], Loss: 0.0305\n",
            "Epoch [4/5], Batch [2460/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [2470/4062], Loss: 0.0036\n",
            "Epoch [4/5], Batch [2480/4062], Loss: 0.0082\n",
            "Epoch [4/5], Batch [2490/4062], Loss: 0.0248\n",
            "Epoch [4/5], Batch [2500/4062], Loss: 0.0039\n",
            "Epoch [4/5], Batch [2510/4062], Loss: 0.0132\n",
            "Epoch [4/5], Batch [2520/4062], Loss: 0.0212\n",
            "Epoch [4/5], Batch [2530/4062], Loss: 0.0051\n",
            "Epoch [4/5], Batch [2540/4062], Loss: 0.0028\n",
            "Epoch [4/5], Batch [2550/4062], Loss: 0.0014\n",
            "Epoch [4/5], Batch [2560/4062], Loss: 0.0178\n",
            "Epoch [4/5], Batch [2570/4062], Loss: 0.0127\n",
            "Epoch [4/5], Batch [2580/4062], Loss: 0.0171\n",
            "Epoch [4/5], Batch [2590/4062], Loss: 0.0015\n",
            "Epoch [4/5], Batch [2600/4062], Loss: 0.0012\n",
            "Epoch [4/5], Batch [2610/4062], Loss: 0.0066\n",
            "Epoch [4/5], Batch [2620/4062], Loss: 0.0039\n",
            "Epoch [4/5], Batch [2630/4062], Loss: 0.0078\n",
            "Epoch [4/5], Batch [2640/4062], Loss: 0.0241\n",
            "Epoch [4/5], Batch [2650/4062], Loss: 0.0087\n",
            "Epoch [4/5], Batch [2660/4062], Loss: 0.0033\n",
            "Epoch [4/5], Batch [2670/4062], Loss: 0.0078\n",
            "Epoch [4/5], Batch [2680/4062], Loss: 0.0085\n",
            "Epoch [4/5], Batch [2690/4062], Loss: 0.0030\n",
            "Epoch [4/5], Batch [2700/4062], Loss: 0.0014\n",
            "Epoch [4/5], Batch [2710/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [2720/4062], Loss: 0.0108\n",
            "Epoch [4/5], Batch [2730/4062], Loss: 0.0055\n",
            "Epoch [4/5], Batch [2740/4062], Loss: 0.0057\n",
            "Epoch [4/5], Batch [2750/4062], Loss: 0.0029\n",
            "Epoch [4/5], Batch [2760/4062], Loss: 0.0071\n",
            "Epoch [4/5], Batch [2770/4062], Loss: 0.0174\n",
            "Epoch [4/5], Batch [2780/4062], Loss: 0.0049\n",
            "Epoch [4/5], Batch [2790/4062], Loss: 0.0044\n",
            "Epoch [4/5], Batch [2800/4062], Loss: 0.0007\n",
            "Epoch [4/5], Batch [2810/4062], Loss: 0.0084\n",
            "Epoch [4/5], Batch [2820/4062], Loss: 0.0152\n",
            "Epoch [4/5], Batch [2830/4062], Loss: 0.0089\n",
            "Epoch [4/5], Batch [2840/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [2850/4062], Loss: 0.0015\n",
            "Epoch [4/5], Batch [2860/4062], Loss: 0.0152\n",
            "Epoch [4/5], Batch [2870/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [2880/4062], Loss: 0.0063\n",
            "Epoch [4/5], Batch [2890/4062], Loss: 0.0050\n",
            "Epoch [4/5], Batch [2900/4062], Loss: 0.0067\n",
            "Epoch [4/5], Batch [2910/4062], Loss: 0.0038\n",
            "Epoch [4/5], Batch [2920/4062], Loss: 0.0032\n",
            "Epoch [4/5], Batch [2930/4062], Loss: 0.0041\n",
            "Epoch [4/5], Batch [2940/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [2950/4062], Loss: 0.0097\n",
            "Epoch [4/5], Batch [2960/4062], Loss: 0.0067\n",
            "Epoch [4/5], Batch [2970/4062], Loss: 0.0072\n",
            "Epoch [4/5], Batch [2980/4062], Loss: 0.0027\n",
            "Epoch [4/5], Batch [2990/4062], Loss: 0.0026\n",
            "Epoch [4/5], Batch [3000/4062], Loss: 0.0009\n",
            "Epoch [4/5], Batch [3010/4062], Loss: 0.0029\n",
            "Epoch [4/5], Batch [3020/4062], Loss: 0.0021\n",
            "Epoch [4/5], Batch [3030/4062], Loss: 0.0025\n",
            "Epoch [4/5], Batch [3040/4062], Loss: 0.0136\n",
            "Epoch [4/5], Batch [3050/4062], Loss: 0.0074\n",
            "Epoch [4/5], Batch [3060/4062], Loss: 0.0104\n",
            "Epoch [4/5], Batch [3070/4062], Loss: 0.0035\n",
            "Epoch [4/5], Batch [3080/4062], Loss: 0.0022\n",
            "Epoch [4/5], Batch [3090/4062], Loss: 0.0120\n",
            "Epoch [4/5], Batch [3100/4062], Loss: 0.0057\n",
            "Epoch [4/5], Batch [3110/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [3120/4062], Loss: 0.0097\n",
            "Epoch [4/5], Batch [3130/4062], Loss: 0.0150\n",
            "Epoch [4/5], Batch [3140/4062], Loss: 0.0032\n",
            "Epoch [4/5], Batch [3150/4062], Loss: 0.0076\n",
            "Epoch [4/5], Batch [3160/4062], Loss: 0.0016\n",
            "Epoch [4/5], Batch [3170/4062], Loss: 0.0090\n",
            "Epoch [4/5], Batch [3180/4062], Loss: 0.0247\n",
            "Epoch [4/5], Batch [3190/4062], Loss: 0.0273\n",
            "Epoch [4/5], Batch [3200/4062], Loss: 0.0048\n",
            "Epoch [4/5], Batch [3210/4062], Loss: 0.0248\n",
            "Epoch [4/5], Batch [3220/4062], Loss: 0.0130\n",
            "Epoch [4/5], Batch [3230/4062], Loss: 0.0098\n",
            "Epoch [4/5], Batch [3240/4062], Loss: 0.0117\n",
            "Epoch [4/5], Batch [3250/4062], Loss: 0.0239\n",
            "Epoch [4/5], Batch [3260/4062], Loss: 0.0042\n",
            "Epoch [4/5], Batch [3270/4062], Loss: 0.0181\n",
            "Epoch [4/5], Batch [3280/4062], Loss: 0.0096\n",
            "Epoch [4/5], Batch [3290/4062], Loss: 0.0186\n",
            "Epoch [4/5], Batch [3300/4062], Loss: 0.0024\n",
            "Epoch [4/5], Batch [3310/4062], Loss: 0.0058\n",
            "Epoch [4/5], Batch [3320/4062], Loss: 0.0032\n",
            "Epoch [4/5], Batch [3330/4062], Loss: 0.0063\n",
            "Epoch [4/5], Batch [3340/4062], Loss: 0.0018\n",
            "Epoch [4/5], Batch [3350/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [3360/4062], Loss: 0.0039\n",
            "Epoch [4/5], Batch [3370/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [3380/4062], Loss: 0.0246\n",
            "Epoch [4/5], Batch [3390/4062], Loss: 0.0091\n",
            "Epoch [4/5], Batch [3400/4062], Loss: 0.0054\n",
            "Epoch [4/5], Batch [3410/4062], Loss: 0.0083\n",
            "Epoch [4/5], Batch [3420/4062], Loss: 0.0080\n",
            "Epoch [4/5], Batch [3430/4062], Loss: 0.0037\n",
            "Epoch [4/5], Batch [3440/4062], Loss: 0.0067\n",
            "Epoch [4/5], Batch [3450/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [3460/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [3470/4062], Loss: 0.0061\n",
            "Epoch [4/5], Batch [3480/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [3490/4062], Loss: 0.0138\n",
            "Epoch [4/5], Batch [3500/4062], Loss: 0.0116\n",
            "Epoch [4/5], Batch [3510/4062], Loss: 0.0062\n",
            "Epoch [4/5], Batch [3520/4062], Loss: 0.0036\n",
            "Epoch [4/5], Batch [3530/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [3540/4062], Loss: 0.0058\n",
            "Epoch [4/5], Batch [3550/4062], Loss: 0.0091\n",
            "Epoch [4/5], Batch [3560/4062], Loss: 0.0066\n",
            "Epoch [4/5], Batch [3570/4062], Loss: 0.0010\n",
            "Epoch [4/5], Batch [3580/4062], Loss: 0.0029\n",
            "Epoch [4/5], Batch [3590/4062], Loss: 0.0100\n",
            "Epoch [4/5], Batch [3600/4062], Loss: 0.0044\n",
            "Epoch [4/5], Batch [3610/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [3620/4062], Loss: 0.0008\n",
            "Epoch [4/5], Batch [3630/4062], Loss: 0.0106\n",
            "Epoch [4/5], Batch [3640/4062], Loss: 0.0074\n",
            "Epoch [4/5], Batch [3650/4062], Loss: 0.0094\n",
            "Epoch [4/5], Batch [3660/4062], Loss: 0.0047\n",
            "Epoch [4/5], Batch [3670/4062], Loss: 0.0046\n",
            "Epoch [4/5], Batch [3680/4062], Loss: 0.0284\n",
            "Epoch [4/5], Batch [3690/4062], Loss: 0.0144\n",
            "Epoch [4/5], Batch [3700/4062], Loss: 0.0026\n",
            "Epoch [4/5], Batch [3710/4062], Loss: 0.0167\n",
            "Epoch [4/5], Batch [3720/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [3730/4062], Loss: 0.0048\n",
            "Epoch [4/5], Batch [3740/4062], Loss: 0.0091\n",
            "Epoch [4/5], Batch [3750/4062], Loss: 0.0061\n",
            "Epoch [4/5], Batch [3760/4062], Loss: 0.0048\n",
            "Epoch [4/5], Batch [3770/4062], Loss: 0.0102\n",
            "Epoch [4/5], Batch [3780/4062], Loss: 0.0127\n",
            "Epoch [4/5], Batch [3790/4062], Loss: 0.0121\n",
            "Epoch [4/5], Batch [3800/4062], Loss: 0.0049\n",
            "Epoch [4/5], Batch [3810/4062], Loss: 0.0086\n",
            "Epoch [4/5], Batch [3820/4062], Loss: 0.0030\n",
            "Epoch [4/5], Batch [3830/4062], Loss: 0.0018\n",
            "Epoch [4/5], Batch [3840/4062], Loss: 0.0107\n",
            "Epoch [4/5], Batch [3850/4062], Loss: 0.0075\n",
            "Epoch [4/5], Batch [3860/4062], Loss: 0.0074\n",
            "Epoch [4/5], Batch [3870/4062], Loss: 0.0110\n",
            "Epoch [4/5], Batch [3880/4062], Loss: 0.0056\n",
            "Epoch [4/5], Batch [3890/4062], Loss: 0.0115\n",
            "Epoch [4/5], Batch [3900/4062], Loss: 0.0022\n",
            "Epoch [4/5], Batch [3910/4062], Loss: 0.0084\n",
            "Epoch [4/5], Batch [3920/4062], Loss: 0.0045\n",
            "Epoch [4/5], Batch [3930/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [3940/4062], Loss: 0.0037\n",
            "Epoch [4/5], Batch [3950/4062], Loss: 0.0112\n",
            "Epoch [4/5], Batch [3960/4062], Loss: 0.0078\n",
            "Epoch [4/5], Batch [3970/4062], Loss: 0.0015\n",
            "Epoch [4/5], Batch [3980/4062], Loss: 0.0042\n",
            "Epoch [4/5], Batch [3990/4062], Loss: 0.0102\n",
            "Epoch [4/5], Batch [4000/4062], Loss: 0.0138\n",
            "Epoch [4/5], Batch [4010/4062], Loss: 0.0068\n",
            "Epoch [4/5], Batch [4020/4062], Loss: 0.0025\n",
            "Epoch [4/5], Batch [4030/4062], Loss: 0.0029\n",
            "Epoch [4/5], Batch [4040/4062], Loss: 0.0117\n",
            "Epoch [4/5], Batch [4050/4062], Loss: 0.0113\n",
            "Epoch [4/5], Batch [4060/4062], Loss: 0.0053\n",
            "Epoch 4/5, Loss: 0.00899155447431933\n",
            "Updated Learning Rate: [1.5752961e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.97      0.94      0.96       753\n",
            "   overdrive       1.00      0.99      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      3765\n",
            "      phaser       1.00      0.99      1.00      4518\n",
            "     flanger       0.99      0.99      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.96      0.99      0.98      6777\n",
            " hall_reverb       0.96      0.97      0.96      4518\n",
            "plate_reverb       0.96      0.98      0.97      3012\n",
            "     octaver       1.00      0.99      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     50451\n",
            "   macro avg       0.99      0.99      0.99     50451\n",
            "weighted avg       0.99      0.99      0.99     50451\n",
            " samples avg       0.99      0.99      0.99     50451\n",
            "\n",
            "\n",
            "Validation Loss: 0.0085, Accuracy: 0.9663, Precision: 0.9876, Recall: 0.9879, F1-score: 0.9877\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Batch [10/4062], Loss: 0.0011\n",
            "Epoch [5/5], Batch [20/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [30/4062], Loss: 0.0059\n",
            "Epoch [5/5], Batch [40/4062], Loss: 0.0027\n",
            "Epoch [5/5], Batch [50/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [60/4062], Loss: 0.0025\n",
            "Epoch [5/5], Batch [70/4062], Loss: 0.0193\n",
            "Epoch [5/5], Batch [80/4062], Loss: 0.0011\n",
            "Epoch [5/5], Batch [90/4062], Loss: 0.0014\n",
            "Epoch [5/5], Batch [100/4062], Loss: 0.0131\n",
            "Epoch [5/5], Batch [110/4062], Loss: 0.0062\n",
            "Epoch [5/5], Batch [120/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [130/4062], Loss: 0.0076\n",
            "Epoch [5/5], Batch [140/4062], Loss: 0.0110\n",
            "Epoch [5/5], Batch [150/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [160/4062], Loss: 0.0070\n",
            "Epoch [5/5], Batch [170/4062], Loss: 0.0164\n",
            "Epoch [5/5], Batch [180/4062], Loss: 0.0031\n",
            "Epoch [5/5], Batch [190/4062], Loss: 0.0071\n",
            "Epoch [5/5], Batch [200/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [210/4062], Loss: 0.0090\n",
            "Epoch [5/5], Batch [220/4062], Loss: 0.0099\n",
            "Epoch [5/5], Batch [230/4062], Loss: 0.0171\n",
            "Epoch [5/5], Batch [240/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [250/4062], Loss: 0.0193\n",
            "Epoch [5/5], Batch [260/4062], Loss: 0.0112\n",
            "Epoch [5/5], Batch [270/4062], Loss: 0.0047\n",
            "Epoch [5/5], Batch [280/4062], Loss: 0.0142\n",
            "Epoch [5/5], Batch [290/4062], Loss: 0.0392\n",
            "Epoch [5/5], Batch [300/4062], Loss: 0.0038\n",
            "Epoch [5/5], Batch [310/4062], Loss: 0.0392\n",
            "Epoch [5/5], Batch [320/4062], Loss: 0.0055\n",
            "Epoch [5/5], Batch [330/4062], Loss: 0.0159\n",
            "Epoch [5/5], Batch [340/4062], Loss: 0.0048\n",
            "Epoch [5/5], Batch [350/4062], Loss: 0.0087\n",
            "Epoch [5/5], Batch [360/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [370/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [380/4062], Loss: 0.0044\n",
            "Epoch [5/5], Batch [390/4062], Loss: 0.0092\n",
            "Epoch [5/5], Batch [400/4062], Loss: 0.0041\n",
            "Epoch [5/5], Batch [410/4062], Loss: 0.0089\n",
            "Epoch [5/5], Batch [420/4062], Loss: 0.0010\n",
            "Epoch [5/5], Batch [430/4062], Loss: 0.0105\n",
            "Epoch [5/5], Batch [440/4062], Loss: 0.0033\n",
            "Epoch [5/5], Batch [450/4062], Loss: 0.0040\n",
            "Epoch [5/5], Batch [460/4062], Loss: 0.0057\n",
            "Epoch [5/5], Batch [470/4062], Loss: 0.0075\n",
            "Epoch [5/5], Batch [480/4062], Loss: 0.0221\n",
            "Epoch [5/5], Batch [490/4062], Loss: 0.0091\n",
            "Epoch [5/5], Batch [500/4062], Loss: 0.0090\n",
            "Epoch [5/5], Batch [510/4062], Loss: 0.0177\n",
            "Epoch [5/5], Batch [520/4062], Loss: 0.0065\n",
            "Epoch [5/5], Batch [530/4062], Loss: 0.0049\n",
            "Epoch [5/5], Batch [540/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [550/4062], Loss: 0.0044\n",
            "Epoch [5/5], Batch [560/4062], Loss: 0.0130\n",
            "Epoch [5/5], Batch [570/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [580/4062], Loss: 0.0050\n",
            "Epoch [5/5], Batch [590/4062], Loss: 0.0214\n",
            "Epoch [5/5], Batch [600/4062], Loss: 0.0024\n",
            "Epoch [5/5], Batch [610/4062], Loss: 0.0032\n",
            "Epoch [5/5], Batch [620/4062], Loss: 0.0059\n",
            "Epoch [5/5], Batch [630/4062], Loss: 0.0040\n",
            "Epoch [5/5], Batch [640/4062], Loss: 0.0104\n",
            "Epoch [5/5], Batch [650/4062], Loss: 0.0199\n",
            "Epoch [5/5], Batch [660/4062], Loss: 0.0058\n",
            "Epoch [5/5], Batch [670/4062], Loss: 0.0177\n",
            "Epoch [5/5], Batch [680/4062], Loss: 0.0009\n",
            "Epoch [5/5], Batch [690/4062], Loss: 0.0118\n",
            "Epoch [5/5], Batch [700/4062], Loss: 0.0007\n",
            "Epoch [5/5], Batch [710/4062], Loss: 0.0034\n",
            "Epoch [5/5], Batch [720/4062], Loss: 0.0159\n",
            "Epoch [5/5], Batch [730/4062], Loss: 0.0104\n",
            "Epoch [5/5], Batch [740/4062], Loss: 0.0179\n",
            "Epoch [5/5], Batch [750/4062], Loss: 0.0090\n",
            "Epoch [5/5], Batch [760/4062], Loss: 0.0126\n",
            "Epoch [5/5], Batch [770/4062], Loss: 0.0117\n",
            "Epoch [5/5], Batch [780/4062], Loss: 0.0066\n",
            "Epoch [5/5], Batch [790/4062], Loss: 0.0085\n",
            "Epoch [5/5], Batch [800/4062], Loss: 0.0046\n",
            "Epoch [5/5], Batch [810/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [820/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [830/4062], Loss: 0.0100\n",
            "Epoch [5/5], Batch [840/4062], Loss: 0.0029\n",
            "Epoch [5/5], Batch [850/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [860/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [870/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [880/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [890/4062], Loss: 0.0043\n",
            "Epoch [5/5], Batch [900/4062], Loss: 0.0022\n",
            "Epoch [5/5], Batch [910/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [920/4062], Loss: 0.0048\n",
            "Epoch [5/5], Batch [930/4062], Loss: 0.0040\n",
            "Epoch [5/5], Batch [940/4062], Loss: 0.0229\n",
            "Epoch [5/5], Batch [950/4062], Loss: 0.0078\n",
            "Epoch [5/5], Batch [960/4062], Loss: 0.0130\n",
            "Epoch [5/5], Batch [970/4062], Loss: 0.0074\n",
            "Epoch [5/5], Batch [980/4062], Loss: 0.0271\n",
            "Epoch [5/5], Batch [990/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [1000/4062], Loss: 0.0024\n",
            "Epoch [5/5], Batch [1010/4062], Loss: 0.0048\n",
            "Epoch [5/5], Batch [1020/4062], Loss: 0.0128\n",
            "Epoch [5/5], Batch [1030/4062], Loss: 0.0141\n",
            "Epoch [5/5], Batch [1040/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [1050/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [1060/4062], Loss: 0.0017\n",
            "Epoch [5/5], Batch [1070/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [1080/4062], Loss: 0.0210\n",
            "Epoch [5/5], Batch [1090/4062], Loss: 0.0170\n",
            "Epoch [5/5], Batch [1100/4062], Loss: 0.0090\n",
            "Epoch [5/5], Batch [1110/4062], Loss: 0.0028\n",
            "Epoch [5/5], Batch [1120/4062], Loss: 0.0059\n",
            "Epoch [5/5], Batch [1130/4062], Loss: 0.0009\n",
            "Epoch [5/5], Batch [1140/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [1150/4062], Loss: 0.0058\n",
            "Epoch [5/5], Batch [1160/4062], Loss: 0.0227\n",
            "Epoch [5/5], Batch [1170/4062], Loss: 0.0059\n",
            "Epoch [5/5], Batch [1180/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [1190/4062], Loss: 0.0013\n",
            "Epoch [5/5], Batch [1200/4062], Loss: 0.0033\n",
            "Epoch [5/5], Batch [1210/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [1220/4062], Loss: 0.0114\n",
            "Epoch [5/5], Batch [1230/4062], Loss: 0.0116\n",
            "Epoch [5/5], Batch [1240/4062], Loss: 0.0022\n",
            "Epoch [5/5], Batch [1250/4062], Loss: 0.0072\n",
            "Epoch [5/5], Batch [1260/4062], Loss: 0.0022\n",
            "Epoch [5/5], Batch [1270/4062], Loss: 0.0069\n",
            "Epoch [5/5], Batch [1280/4062], Loss: 0.0049\n",
            "Epoch [5/5], Batch [1290/4062], Loss: 0.0014\n",
            "Epoch [5/5], Batch [1300/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [1310/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [1320/4062], Loss: 0.0065\n",
            "Epoch [5/5], Batch [1330/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [1340/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [1350/4062], Loss: 0.0043\n",
            "Epoch [5/5], Batch [1360/4062], Loss: 0.0010\n",
            "Epoch [5/5], Batch [1370/4062], Loss: 0.0055\n",
            "Epoch [5/5], Batch [1380/4062], Loss: 0.0041\n",
            "Epoch [5/5], Batch [1390/4062], Loss: 0.0055\n",
            "Epoch [5/5], Batch [1400/4062], Loss: 0.0099\n",
            "Epoch [5/5], Batch [1410/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [1420/4062], Loss: 0.0044\n",
            "Epoch [5/5], Batch [1430/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [1440/4062], Loss: 0.0052\n",
            "Epoch [5/5], Batch [1450/4062], Loss: 0.0020\n",
            "Epoch [5/5], Batch [1460/4062], Loss: 0.0198\n",
            "Epoch [5/5], Batch [1470/4062], Loss: 0.0124\n",
            "Epoch [5/5], Batch [1480/4062], Loss: 0.0111\n",
            "Epoch [5/5], Batch [1490/4062], Loss: 0.0067\n",
            "Epoch [5/5], Batch [1500/4062], Loss: 0.0149\n",
            "Epoch [5/5], Batch [1510/4062], Loss: 0.0081\n",
            "Epoch [5/5], Batch [1520/4062], Loss: 0.0183\n",
            "Epoch [5/5], Batch [1530/4062], Loss: 0.0088\n",
            "Epoch [5/5], Batch [1540/4062], Loss: 0.0100\n",
            "Epoch [5/5], Batch [1550/4062], Loss: 0.0279\n",
            "Epoch [5/5], Batch [1560/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [1570/4062], Loss: 0.0035\n",
            "Epoch [5/5], Batch [1580/4062], Loss: 0.0117\n",
            "Epoch [5/5], Batch [1590/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [1600/4062], Loss: 0.0141\n",
            "Epoch [5/5], Batch [1610/4062], Loss: 0.0074\n",
            "Epoch [5/5], Batch [1620/4062], Loss: 0.0082\n",
            "Epoch [5/5], Batch [1630/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [1640/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [1650/4062], Loss: 0.0145\n",
            "Epoch [5/5], Batch [1660/4062], Loss: 0.0082\n",
            "Epoch [5/5], Batch [1670/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [1680/4062], Loss: 0.0070\n",
            "Epoch [5/5], Batch [1690/4062], Loss: 0.0124\n",
            "Epoch [5/5], Batch [1700/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [1710/4062], Loss: 0.0206\n",
            "Epoch [5/5], Batch [1720/4062], Loss: 0.0089\n",
            "Epoch [5/5], Batch [1730/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [1740/4062], Loss: 0.0043\n",
            "Epoch [5/5], Batch [1750/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [1760/4062], Loss: 0.0030\n",
            "Epoch [5/5], Batch [1770/4062], Loss: 0.0260\n",
            "Epoch [5/5], Batch [1780/4062], Loss: 0.0028\n",
            "Epoch [5/5], Batch [1790/4062], Loss: 0.0130\n",
            "Epoch [5/5], Batch [1800/4062], Loss: 0.0044\n",
            "Epoch [5/5], Batch [1810/4062], Loss: 0.0070\n",
            "Epoch [5/5], Batch [1820/4062], Loss: 0.0105\n",
            "Epoch [5/5], Batch [1830/4062], Loss: 0.0046\n",
            "Epoch [5/5], Batch [1840/4062], Loss: 0.0105\n",
            "Epoch [5/5], Batch [1850/4062], Loss: 0.0040\n",
            "Epoch [5/5], Batch [1860/4062], Loss: 0.0065\n",
            "Epoch [5/5], Batch [1870/4062], Loss: 0.0128\n",
            "Epoch [5/5], Batch [1880/4062], Loss: 0.0172\n",
            "Epoch [5/5], Batch [1890/4062], Loss: 0.0020\n",
            "Epoch [5/5], Batch [1900/4062], Loss: 0.0035\n",
            "Epoch [5/5], Batch [1910/4062], Loss: 0.0095\n",
            "Epoch [5/5], Batch [1920/4062], Loss: 0.0011\n",
            "Epoch [5/5], Batch [1930/4062], Loss: 0.0061\n",
            "Epoch [5/5], Batch [1940/4062], Loss: 0.0029\n",
            "Epoch [5/5], Batch [1950/4062], Loss: 0.0043\n",
            "Epoch [5/5], Batch [1960/4062], Loss: 0.0142\n",
            "Epoch [5/5], Batch [1970/4062], Loss: 0.0046\n",
            "Epoch [5/5], Batch [1980/4062], Loss: 0.0055\n",
            "Epoch [5/5], Batch [1990/4062], Loss: 0.0194\n",
            "Epoch [5/5], Batch [2000/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [2010/4062], Loss: 0.0023\n",
            "Epoch [5/5], Batch [2020/4062], Loss: 0.0014\n",
            "Epoch [5/5], Batch [2030/4062], Loss: 0.0119\n",
            "Epoch [5/5], Batch [2040/4062], Loss: 0.0133\n",
            "Epoch [5/5], Batch [2050/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [2060/4062], Loss: 0.0050\n",
            "Epoch [5/5], Batch [2070/4062], Loss: 0.0087\n",
            "Epoch [5/5], Batch [2080/4062], Loss: 0.0172\n",
            "Epoch [5/5], Batch [2090/4062], Loss: 0.0010\n",
            "Epoch [5/5], Batch [2100/4062], Loss: 0.0100\n",
            "Epoch [5/5], Batch [2110/4062], Loss: 0.0050\n",
            "Epoch [5/5], Batch [2120/4062], Loss: 0.0079\n",
            "Epoch [5/5], Batch [2130/4062], Loss: 0.0126\n",
            "Epoch [5/5], Batch [2140/4062], Loss: 0.0064\n",
            "Epoch [5/5], Batch [2150/4062], Loss: 0.0035\n",
            "Epoch [5/5], Batch [2160/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [2170/4062], Loss: 0.0153\n",
            "Epoch [5/5], Batch [2180/4062], Loss: 0.0114\n",
            "Epoch [5/5], Batch [2190/4062], Loss: 0.0029\n",
            "Epoch [5/5], Batch [2200/4062], Loss: 0.0159\n",
            "Epoch [5/5], Batch [2210/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [2220/4062], Loss: 0.0135\n",
            "Epoch [5/5], Batch [2230/4062], Loss: 0.0062\n",
            "Epoch [5/5], Batch [2240/4062], Loss: 0.0064\n",
            "Epoch [5/5], Batch [2250/4062], Loss: 0.0061\n",
            "Epoch [5/5], Batch [2260/4062], Loss: 0.0016\n",
            "Epoch [5/5], Batch [2270/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [2280/4062], Loss: 0.0024\n",
            "Epoch [5/5], Batch [2290/4062], Loss: 0.0046\n",
            "Epoch [5/5], Batch [2300/4062], Loss: 0.0033\n",
            "Epoch [5/5], Batch [2310/4062], Loss: 0.0107\n",
            "Epoch [5/5], Batch [2320/4062], Loss: 0.0031\n",
            "Epoch [5/5], Batch [2330/4062], Loss: 0.0169\n",
            "Epoch [5/5], Batch [2340/4062], Loss: 0.0058\n",
            "Epoch [5/5], Batch [2350/4062], Loss: 0.0093\n",
            "Epoch [5/5], Batch [2360/4062], Loss: 0.0029\n",
            "Epoch [5/5], Batch [2370/4062], Loss: 0.0080\n",
            "Epoch [5/5], Batch [2380/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [2390/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [2400/4062], Loss: 0.0027\n",
            "Epoch [5/5], Batch [2410/4062], Loss: 0.0081\n",
            "Epoch [5/5], Batch [2420/4062], Loss: 0.0034\n",
            "Epoch [5/5], Batch [2430/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [2440/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [2450/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [2460/4062], Loss: 0.0173\n",
            "Epoch [5/5], Batch [2470/4062], Loss: 0.0012\n",
            "Epoch [5/5], Batch [2480/4062], Loss: 0.0009\n",
            "Epoch [5/5], Batch [2490/4062], Loss: 0.0012\n",
            "Epoch [5/5], Batch [2500/4062], Loss: 0.0091\n",
            "Epoch [5/5], Batch [2510/4062], Loss: 0.0031\n",
            "Epoch [5/5], Batch [2520/4062], Loss: 0.0133\n",
            "Epoch [5/5], Batch [2530/4062], Loss: 0.0057\n",
            "Epoch [5/5], Batch [2540/4062], Loss: 0.0070\n",
            "Epoch [5/5], Batch [2550/4062], Loss: 0.0201\n",
            "Epoch [5/5], Batch [2560/4062], Loss: 0.0132\n",
            "Epoch [5/5], Batch [2570/4062], Loss: 0.0164\n",
            "Epoch [5/5], Batch [2580/4062], Loss: 0.0064\n",
            "Epoch [5/5], Batch [2590/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [2600/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [2610/4062], Loss: 0.0123\n",
            "Epoch [5/5], Batch [2620/4062], Loss: 0.0203\n",
            "Epoch [5/5], Batch [2630/4062], Loss: 0.0095\n",
            "Epoch [5/5], Batch [2640/4062], Loss: 0.0086\n",
            "Epoch [5/5], Batch [2650/4062], Loss: 0.0190\n",
            "Epoch [5/5], Batch [2660/4062], Loss: 0.0038\n",
            "Epoch [5/5], Batch [2670/4062], Loss: 0.0019\n",
            "Epoch [5/5], Batch [2680/4062], Loss: 0.0316\n",
            "Epoch [5/5], Batch [2690/4062], Loss: 0.0031\n",
            "Epoch [5/5], Batch [2700/4062], Loss: 0.0007\n",
            "Epoch [5/5], Batch [2710/4062], Loss: 0.0073\n",
            "Epoch [5/5], Batch [2720/4062], Loss: 0.0014\n",
            "Epoch [5/5], Batch [2730/4062], Loss: 0.0011\n",
            "Epoch [5/5], Batch [2740/4062], Loss: 0.0028\n",
            "Epoch [5/5], Batch [2750/4062], Loss: 0.0060\n",
            "Epoch [5/5], Batch [2760/4062], Loss: 0.0147\n",
            "Epoch [5/5], Batch [2770/4062], Loss: 0.0031\n",
            "Epoch [5/5], Batch [2780/4062], Loss: 0.0064\n",
            "Epoch [5/5], Batch [2790/4062], Loss: 0.0091\n",
            "Epoch [5/5], Batch [2800/4062], Loss: 0.0068\n",
            "Epoch [5/5], Batch [2810/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [2820/4062], Loss: 0.0011\n",
            "Epoch [5/5], Batch [2830/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [2840/4062], Loss: 0.0262\n",
            "Epoch [5/5], Batch [2850/4062], Loss: 0.0138\n",
            "Epoch [5/5], Batch [2860/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [2870/4062], Loss: 0.0055\n",
            "Epoch [5/5], Batch [2880/4062], Loss: 0.0132\n",
            "Epoch [5/5], Batch [2890/4062], Loss: 0.0061\n",
            "Epoch [5/5], Batch [2900/4062], Loss: 0.0080\n",
            "Epoch [5/5], Batch [2910/4062], Loss: 0.0016\n",
            "Epoch [5/5], Batch [2920/4062], Loss: 0.0058\n",
            "Epoch [5/5], Batch [2930/4062], Loss: 0.0012\n",
            "Epoch [5/5], Batch [2940/4062], Loss: 0.0143\n",
            "Epoch [5/5], Batch [2950/4062], Loss: 0.0008\n",
            "Epoch [5/5], Batch [2960/4062], Loss: 0.0111\n",
            "Epoch [5/5], Batch [2970/4062], Loss: 0.0027\n",
            "Epoch [5/5], Batch [2980/4062], Loss: 0.0027\n",
            "Epoch [5/5], Batch [2990/4062], Loss: 0.0048\n",
            "Epoch [5/5], Batch [3000/4062], Loss: 0.0072\n",
            "Epoch [5/5], Batch [3010/4062], Loss: 0.0100\n",
            "Epoch [5/5], Batch [3020/4062], Loss: 0.0137\n",
            "Epoch [5/5], Batch [3030/4062], Loss: 0.0009\n",
            "Epoch [5/5], Batch [3040/4062], Loss: 0.0129\n",
            "Epoch [5/5], Batch [3050/4062], Loss: 0.0043\n",
            "Epoch [5/5], Batch [3060/4062], Loss: 0.0014\n",
            "Epoch [5/5], Batch [3070/4062], Loss: 0.0016\n",
            "Epoch [5/5], Batch [3080/4062], Loss: 0.0022\n",
            "Epoch [5/5], Batch [3090/4062], Loss: 0.0213\n",
            "Epoch [5/5], Batch [3100/4062], Loss: 0.0074\n",
            "Epoch [5/5], Batch [3110/4062], Loss: 0.0105\n",
            "Epoch [5/5], Batch [3120/4062], Loss: 0.0068\n",
            "Epoch [5/5], Batch [3130/4062], Loss: 0.0032\n",
            "Epoch [5/5], Batch [3140/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [3150/4062], Loss: 0.0082\n",
            "Epoch [5/5], Batch [3160/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [3170/4062], Loss: 0.0031\n",
            "Epoch [5/5], Batch [3180/4062], Loss: 0.0014\n",
            "Epoch [5/5], Batch [3190/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [3200/4062], Loss: 0.0042\n",
            "Epoch [5/5], Batch [3210/4062], Loss: 0.0075\n",
            "Epoch [5/5], Batch [3220/4062], Loss: 0.0057\n",
            "Epoch [5/5], Batch [3230/4062], Loss: 0.0028\n",
            "Epoch [5/5], Batch [3240/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [3250/4062], Loss: 0.0029\n",
            "Epoch [5/5], Batch [3260/4062], Loss: 0.0190\n",
            "Epoch [5/5], Batch [3270/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [3280/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [3290/4062], Loss: 0.0042\n",
            "Epoch [5/5], Batch [3300/4062], Loss: 0.0149\n",
            "Epoch [5/5], Batch [3310/4062], Loss: 0.0018\n",
            "Epoch [5/5], Batch [3320/4062], Loss: 0.0153\n",
            "Epoch [5/5], Batch [3330/4062], Loss: 0.0057\n",
            "Epoch [5/5], Batch [3340/4062], Loss: 0.0089\n",
            "Epoch [5/5], Batch [3350/4062], Loss: 0.0012\n",
            "Epoch [5/5], Batch [3360/4062], Loss: 0.0016\n",
            "Epoch [5/5], Batch [3370/4062], Loss: 0.0028\n",
            "Epoch [5/5], Batch [3380/4062], Loss: 0.0153\n",
            "Epoch [5/5], Batch [3390/4062], Loss: 0.0047\n",
            "Epoch [5/5], Batch [3400/4062], Loss: 0.0045\n",
            "Epoch [5/5], Batch [3410/4062], Loss: 0.0317\n",
            "Epoch [5/5], Batch [3420/4062], Loss: 0.0032\n",
            "Epoch [5/5], Batch [3430/4062], Loss: 0.0140\n",
            "Epoch [5/5], Batch [3440/4062], Loss: 0.0039\n",
            "Epoch [5/5], Batch [3450/4062], Loss: 0.0122\n",
            "Epoch [5/5], Batch [3460/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [3470/4062], Loss: 0.0168\n",
            "Epoch [5/5], Batch [3480/4062], Loss: 0.0193\n",
            "Epoch [5/5], Batch [3490/4062], Loss: 0.0064\n",
            "Epoch [5/5], Batch [3500/4062], Loss: 0.0056\n",
            "Epoch [5/5], Batch [3510/4062], Loss: 0.0021\n",
            "Epoch [5/5], Batch [3520/4062], Loss: 0.0203\n",
            "Epoch [5/5], Batch [3530/4062], Loss: 0.0094\n",
            "Epoch [5/5], Batch [3540/4062], Loss: 0.0017\n",
            "Epoch [5/5], Batch [3550/4062], Loss: 0.0047\n",
            "Epoch [5/5], Batch [3560/4062], Loss: 0.0038\n",
            "Epoch [5/5], Batch [3570/4062], Loss: 0.0061\n",
            "Epoch [5/5], Batch [3580/4062], Loss: 0.0017\n",
            "Epoch [5/5], Batch [3590/4062], Loss: 0.0143\n",
            "Epoch [5/5], Batch [3600/4062], Loss: 0.0038\n",
            "Epoch [5/5], Batch [3610/4062], Loss: 0.0122\n",
            "Epoch [5/5], Batch [3620/4062], Loss: 0.0030\n",
            "Epoch [5/5], Batch [3630/4062], Loss: 0.0033\n",
            "Epoch [5/5], Batch [3640/4062], Loss: 0.0118\n",
            "Epoch [5/5], Batch [3650/4062], Loss: 0.0026\n",
            "Epoch [5/5], Batch [3660/4062], Loss: 0.0044\n",
            "Epoch [5/5], Batch [3670/4062], Loss: 0.0062\n",
            "Epoch [5/5], Batch [3680/4062], Loss: 0.0058\n",
            "Epoch [5/5], Batch [3690/4062], Loss: 0.0073\n",
            "Epoch [5/5], Batch [3700/4062], Loss: 0.0055\n",
            "Epoch [5/5], Batch [3710/4062], Loss: 0.0151\n",
            "Epoch [5/5], Batch [3720/4062], Loss: 0.0050\n",
            "Epoch [5/5], Batch [3730/4062], Loss: 0.0082\n",
            "Epoch [5/5], Batch [3740/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [3750/4062], Loss: 0.0082\n",
            "Epoch [5/5], Batch [3760/4062], Loss: 0.0037\n",
            "Epoch [5/5], Batch [3770/4062], Loss: 0.0048\n",
            "Epoch [5/5], Batch [3780/4062], Loss: 0.0042\n",
            "Epoch [5/5], Batch [3790/4062], Loss: 0.0042\n",
            "Epoch [5/5], Batch [3800/4062], Loss: 0.0076\n",
            "Epoch [5/5], Batch [3810/4062], Loss: 0.0286\n",
            "Epoch [5/5], Batch [3820/4062], Loss: 0.0058\n",
            "Epoch [5/5], Batch [3830/4062], Loss: 0.0202\n",
            "Epoch [5/5], Batch [3840/4062], Loss: 0.0076\n",
            "Epoch [5/5], Batch [3850/4062], Loss: 0.0022\n",
            "Epoch [5/5], Batch [3860/4062], Loss: 0.0078\n",
            "Epoch [5/5], Batch [3870/4062], Loss: 0.0053\n",
            "Epoch [5/5], Batch [3880/4062], Loss: 0.0093\n",
            "Epoch [5/5], Batch [3890/4062], Loss: 0.0176\n",
            "Epoch [5/5], Batch [3900/4062], Loss: 0.0028\n",
            "Epoch [5/5], Batch [3910/4062], Loss: 0.0130\n",
            "Epoch [5/5], Batch [3920/4062], Loss: 0.0119\n",
            "Epoch [5/5], Batch [3930/4062], Loss: 0.0054\n",
            "Epoch [5/5], Batch [3940/4062], Loss: 0.0184\n",
            "Epoch [5/5], Batch [3950/4062], Loss: 0.0068\n",
            "Epoch [5/5], Batch [3960/4062], Loss: 0.0048\n",
            "Epoch [5/5], Batch [3970/4062], Loss: 0.0139\n",
            "Epoch [5/5], Batch [3980/4062], Loss: 0.0107\n",
            "Epoch [5/5], Batch [3990/4062], Loss: 0.0036\n",
            "Epoch [5/5], Batch [4000/4062], Loss: 0.0118\n",
            "Epoch [5/5], Batch [4010/4062], Loss: 0.0075\n",
            "Epoch [5/5], Batch [4020/4062], Loss: 0.0152\n",
            "Epoch [5/5], Batch [4030/4062], Loss: 0.0159\n",
            "Epoch [5/5], Batch [4040/4062], Loss: 0.0080\n",
            "Epoch [5/5], Batch [4050/4062], Loss: 0.0062\n",
            "Epoch [5/5], Batch [4060/4062], Loss: 0.0018\n",
            "Epoch 5/5, Loss: 0.007335332999778605\n",
            "Updated Learning Rate: [9.92436543e-06]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.99      0.93      0.96       753\n",
            "   overdrive       1.00      0.99      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      3765\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      0.99      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5271\n",
            "       delay       0.99      0.97      0.98      6777\n",
            " hall_reverb       0.95      0.97      0.96      4518\n",
            "plate_reverb       0.95      0.99      0.97      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     50451\n",
            "   macro avg       0.99      0.99      0.99     50451\n",
            "weighted avg       0.99      0.99      0.99     50451\n",
            " samples avg       0.99      0.99      0.99     50451\n",
            "\n",
            "\n",
            "Validation Loss: 0.0084, Accuracy: 0.9672, Precision: 0.9895, Recall: 0.9863, F1-score: 0.9878\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=6, pin_memory=True)\n",
        "\n",
        "num_classes = len(train_dataset.label_map)\n",
        "\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.63)  # 0.0001 → 0.00001 over 5 epochs\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "print_freq = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    print(f\"Updated Learning Rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrograms, labels in val_loader:\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            outputs = model(spectrograms)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
        "\n",
        "            # Store for metric computation\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Convert lists to numpy arrays for metric calculations\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Print classification report\n",
        "    class_names = train_dataset.label_map\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(f\"\\nValidation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGiHpYYikrqv",
        "outputId": "8e9757a9-d7f0-41ef-91d6-d50df34671b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-02c0234577ea>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n",
            "\n",
            "Test Loss: 0.0080, Accuracy: 0.9700, Precision: 0.9881, Recall: 0.9856, F1-score: 0.9868\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.97      0.94      0.95       757\n",
            "   overdrive       1.00      0.99      0.99      3028\n",
            "  distortion       1.00      0.99      1.00      4544\n",
            "        fuzz       1.00      0.99      1.00      5300\n",
            "     tremolo       1.00      1.00      1.00      3028\n",
            "      phaser       1.00      1.00      1.00      4542\n",
            "     flanger       1.00      0.99      0.99      3028\n",
            "      chorus       1.00      0.99      1.00      5300\n",
            "       delay       1.00      0.98      0.99      6814\n",
            " hall_reverb       0.94      0.98      0.96      3788\n",
            "plate_reverb       0.97      0.99      0.98      3028\n",
            "     octaver       0.98      0.99      0.99      2271\n",
            " auto_filter       1.00      0.99      1.00      3785\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     49213\n",
            "   macro avg       0.99      0.99      0.99     49213\n",
            "weighted avg       0.99      0.99      0.99     49213\n",
            " samples avg       0.99      0.99      0.99     49213\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "# h5_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_test.h5\"\n",
        "# csv_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_test.csv\"\n",
        "\n",
        "h5_test_path = \"/content/final_datasets/final_test.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_test.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4M37nJkp9Vq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c428852-b927-4a70-c747-e2166a0aaa23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-91f58b229832>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.1077, Accuracy: 0.7143, Precision: 0.8979, Recall: 0.8377, F1-score: 0.8459\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.97      0.34      0.50       858\n",
            "   overdrive       0.81      0.67      0.74      3432\n",
            "  distortion       0.99      0.96      0.98      5148\n",
            "        fuzz       1.00      0.91      0.95      6006\n",
            "     tremolo       0.86      1.00      0.92      4290\n",
            "      phaser       1.00      0.91      0.96      5148\n",
            "     flanger       0.98      0.70      0.82      3432\n",
            "      chorus       0.94      0.92      0.93      6006\n",
            "       delay       0.97      0.81      0.88      7722\n",
            " hall_reverb       0.87      0.95      0.91      5148\n",
            "plate_reverb       0.95      0.80      0.87      3432\n",
            "     octaver       0.51      0.98      0.67      2574\n",
            " auto_filter       0.82      0.94      0.88      4290\n",
            "\n",
            "   micro avg       0.89      0.88      0.88     57486\n",
            "   macro avg       0.90      0.84      0.85     57486\n",
            "weighted avg       0.92      0.88      0.89     57486\n",
            " samples avg       0.86      0.85      0.84     57486\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "# h5_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.h5\"\n",
        "# csv_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.csv\"\n",
        "\n",
        "h5_test_path = \"/content/final_datasets/final_real.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_real.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt3.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}