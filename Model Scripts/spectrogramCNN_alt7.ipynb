{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbdionne/toneclone/blob/main/spectrogramCNN_alt7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mIYygHe1JGIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4904b7e-c942-4330-96c6-98587d719453"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/final_datasets\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.h5\" \"/content/final_datasets/final_real.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.csv\" \"/content/final_datasets/final_real.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/test_extra_TRM_DLY.h5\" \"/content/final_datasets/test_extra_TRM_DLY.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/test_extra_TRM_DLY.csv\" \"/content/final_datasets/test_extra_TRM_DLY.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/validate_extra_TRM_DLY.h5\" \"/content/final_datasets/validate_extra_TRM_DLY.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/validate_extra_TRM_DLY.csv\" \"/content/final_datasets/validate_extra_TRM_DLY.csv\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/train_extra_TRM_DLY.h5\" \"/content/final_datasets/train_extra_TRM_DLY.h5\"\n",
        "!cp \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/train_extra_TRM_DLY.csv\" \"/content/final_datasets/train_extra_TRM_DLY.csv\""
      ],
      "metadata": {
        "id": "7QQiSvhn9atV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hvDlTB85Sraj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import torchaudio.transforms as T\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"Downcasting object dtype arrays on .fillna\")\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for spectrogram data with data augmentation.\n",
        "    Includes:\n",
        "    - Random Gaussian noise\n",
        "    - Pitch shifting using torch.roll() with zero-padding (prevents wrapping)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, csv_file, augment=True, noise_level=0.03, pitch_shift_range=(-0.5, 0.5)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_file (str): Path to the HDF5 file containing spectrograms.\n",
        "            csv_file (str): Path to CSV file with labels.\n",
        "            augment (bool): Whether to apply data augmentation.\n",
        "            noise_level (float): Standard deviation of Gaussian noise to add.\n",
        "            pitch_shift_range (tuple): Min/max semitones for pitch shifting.\n",
        "        \"\"\"\n",
        "        self.hdf5_file_path = hdf5_file\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "\n",
        "        # Manually define only important columns\n",
        "        self.label_map = [\n",
        "            'overdrive', 'distortion', 'fuzz', 'tremolo', 'phaser',\n",
        "            'flanger', 'chorus', 'delay', 'hall_reverb', 'plate_reverb',\n",
        "            'octaver', 'auto_filter'\n",
        "        ]\n",
        "\n",
        "        # Drop all non-label columns\n",
        "        self.labels = self.labels[['key'] + self.label_map]\n",
        "\n",
        "        self.hdf5_file = None  # Open HDF5 file once per worker\n",
        "\n",
        "        self.augment = augment\n",
        "        self.noise_level = noise_level\n",
        "        self.pitch_shift_range = pitch_shift_range\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Open HDF5 file per worker to avoid threading issues\n",
        "        if self.hdf5_file is None:\n",
        "            self.hdf5_file = h5py.File(self.hdf5_file_path, \"r\", swmr=True)\n",
        "\n",
        "        # Retrieve spectrogram\n",
        "        key = self.labels.iloc[idx]['key']\n",
        "        spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Retrieve labels\n",
        "        label_values = self.labels.iloc[idx][1:].fillna(0).astype(float).values\n",
        "        label = torch.tensor(label_values, dtype=torch.float32)\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            spectrogram = self.add_noise(spectrogram)\n",
        "            spectrogram = self.pitch_shift(spectrogram)\n",
        "\n",
        "        return spectrogram, label\n",
        "\n",
        "    def add_noise(self, spectrogram):\n",
        "        \"\"\"Adds Gaussian noise where noise level is randomly chosen between 0 and self.noise_level.\"\"\"\n",
        "        noise_level = random.uniform(0, self.noise_level)  # Random noise per sample\n",
        "        noise = torch.randn_like(spectrogram) * noise_level  # Scale noise\n",
        "        return spectrogram + noise\n",
        "\n",
        "    def pitch_shift(self, spectrogram):\n",
        "        \"\"\"Shifts spectrogram frequency bins using torch.roll() with zero padding.\"\"\"\n",
        "        semitone_shift = random.uniform(*self.pitch_shift_range)  # Random shift between min/max\n",
        "        shift_bins = int(semitone_shift / 12 * spectrogram.shape[-2])  # Convert semitone shift to frequency bins\n",
        "\n",
        "        # Apply frequency bin shift using torch.roll() with zero-padding\n",
        "        shifted = torch.roll(spectrogram, shifts=shift_bins, dims=-2)  # Shift along frequency axis\n",
        "\n",
        "        if shift_bins > 0:  # Shift up (higher pitch)\n",
        "            shifted[..., :shift_bins, :] = 0  # Zero-pad low frequencies\n",
        "        elif shift_bins < 0:  # Shift down (lower pitch)\n",
        "            shifted[..., shift_bins:, :] = 0  # Zero-pad high frequencies\n",
        "\n",
        "        return shifted\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.hdf5_file is not None:\n",
        "            self.hdf5_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pVGFYMDhDnI9"
      },
      "outputs": [],
      "source": [
        "class spectrogramCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(spectrogramCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x) # Dropout\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p3BlietYUQpv"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset from HD5F and csv file\n",
        "\n",
        "h5_train_path = '/content/final_datasets/train_extra_TRM_DLY.h5'\n",
        "csv_train_path = '/content/final_datasets/train_extra_TRM_DLY.csv'\n",
        "\n",
        "h5_val_path = '/content/final_datasets/validate_extra_TRM_DLY.h5'\n",
        "csv_val_path = '/content/final_datasets/validate_extra_TRM_DLY.csv'\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\"\n",
        "\n",
        "train_dataset = SpectrogramDataset(h5_train_path, csv_train_path)\n",
        "val_dataset = SpectrogramDataset(h5_val_path, csv_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUEbBB7wX05C",
        "outputId": "fe2e2469-d3a9-4511-b652-b3a3cdfcfe3a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Batch [50/4502], Loss: 0.3647\n",
            "Epoch [1/15], Batch [100/4502], Loss: 0.2850\n",
            "Epoch [1/15], Batch [150/4502], Loss: 0.2740\n",
            "Epoch [1/15], Batch [200/4502], Loss: 0.2295\n",
            "Epoch [1/15], Batch [250/4502], Loss: 0.1917\n",
            "Epoch [1/15], Batch [300/4502], Loss: 0.1696\n",
            "Epoch [1/15], Batch [350/4502], Loss: 0.2025\n",
            "Epoch [1/15], Batch [400/4502], Loss: 0.1460\n",
            "Epoch [1/15], Batch [450/4502], Loss: 0.1250\n",
            "Epoch [1/15], Batch [500/4502], Loss: 0.1440\n",
            "Epoch [1/15], Batch [550/4502], Loss: 0.1223\n",
            "Epoch [1/15], Batch [600/4502], Loss: 0.1519\n",
            "Epoch [1/15], Batch [650/4502], Loss: 0.1155\n",
            "Epoch [1/15], Batch [700/4502], Loss: 0.1433\n",
            "Epoch [1/15], Batch [750/4502], Loss: 0.0894\n",
            "Epoch [1/15], Batch [800/4502], Loss: 0.1006\n",
            "Epoch [1/15], Batch [850/4502], Loss: 0.1094\n",
            "Epoch [1/15], Batch [900/4502], Loss: 0.0981\n",
            "Epoch [1/15], Batch [950/4502], Loss: 0.1130\n",
            "Epoch [1/15], Batch [1000/4502], Loss: 0.0726\n",
            "Epoch [1/15], Batch [1050/4502], Loss: 0.0951\n",
            "Epoch [1/15], Batch [1100/4502], Loss: 0.0603\n",
            "Epoch [1/15], Batch [1150/4502], Loss: 0.0923\n",
            "Epoch [1/15], Batch [1200/4502], Loss: 0.0840\n",
            "Epoch [1/15], Batch [1250/4502], Loss: 0.0596\n",
            "Epoch [1/15], Batch [1300/4502], Loss: 0.1020\n",
            "Epoch [1/15], Batch [1350/4502], Loss: 0.1025\n",
            "Epoch [1/15], Batch [1400/4502], Loss: 0.0585\n",
            "Epoch [1/15], Batch [1450/4502], Loss: 0.0748\n",
            "Epoch [1/15], Batch [1500/4502], Loss: 0.0776\n",
            "Epoch [1/15], Batch [1550/4502], Loss: 0.0548\n",
            "Epoch [1/15], Batch [1600/4502], Loss: 0.0512\n",
            "Epoch [1/15], Batch [1650/4502], Loss: 0.0532\n",
            "Epoch [1/15], Batch [1700/4502], Loss: 0.0443\n",
            "Epoch [1/15], Batch [1750/4502], Loss: 0.0469\n",
            "Epoch [1/15], Batch [1800/4502], Loss: 0.0797\n",
            "Epoch [1/15], Batch [1850/4502], Loss: 0.0522\n",
            "Epoch [1/15], Batch [1900/4502], Loss: 0.0732\n",
            "Epoch [1/15], Batch [1950/4502], Loss: 0.1340\n",
            "Epoch [1/15], Batch [2000/4502], Loss: 0.0619\n",
            "Epoch [1/15], Batch [2050/4502], Loss: 0.0430\n",
            "Epoch [1/15], Batch [2100/4502], Loss: 0.0469\n",
            "Epoch [1/15], Batch [2150/4502], Loss: 0.0226\n",
            "Epoch [1/15], Batch [2200/4502], Loss: 0.0256\n",
            "Epoch [1/15], Batch [2250/4502], Loss: 0.0619\n",
            "Epoch [1/15], Batch [2300/4502], Loss: 0.0392\n",
            "Epoch [1/15], Batch [2350/4502], Loss: 0.0489\n",
            "Epoch [1/15], Batch [2400/4502], Loss: 0.0544\n",
            "Epoch [1/15], Batch [2450/4502], Loss: 0.0540\n",
            "Epoch [1/15], Batch [2500/4502], Loss: 0.0667\n",
            "Epoch [1/15], Batch [2550/4502], Loss: 0.0462\n",
            "Epoch [1/15], Batch [2600/4502], Loss: 0.0417\n",
            "Epoch [1/15], Batch [2650/4502], Loss: 0.0446\n",
            "Epoch [1/15], Batch [2700/4502], Loss: 0.0279\n",
            "Epoch [1/15], Batch [2750/4502], Loss: 0.0290\n",
            "Epoch [1/15], Batch [2800/4502], Loss: 0.0210\n",
            "Epoch [1/15], Batch [2850/4502], Loss: 0.0274\n",
            "Epoch [1/15], Batch [2900/4502], Loss: 0.0534\n",
            "Epoch [1/15], Batch [2950/4502], Loss: 0.0389\n",
            "Epoch [1/15], Batch [3000/4502], Loss: 0.0311\n",
            "Epoch [1/15], Batch [3050/4502], Loss: 0.0445\n",
            "Epoch [1/15], Batch [3100/4502], Loss: 0.0457\n",
            "Epoch [1/15], Batch [3150/4502], Loss: 0.0298\n",
            "Epoch [1/15], Batch [3200/4502], Loss: 0.0524\n",
            "Epoch [1/15], Batch [3250/4502], Loss: 0.0171\n",
            "Epoch [1/15], Batch [3300/4502], Loss: 0.0231\n",
            "Epoch [1/15], Batch [3350/4502], Loss: 0.0222\n",
            "Epoch [1/15], Batch [3400/4502], Loss: 0.0293\n",
            "Epoch [1/15], Batch [3450/4502], Loss: 0.0398\n",
            "Epoch [1/15], Batch [3500/4502], Loss: 0.0174\n",
            "Epoch [1/15], Batch [3550/4502], Loss: 0.0307\n",
            "Epoch [1/15], Batch [3600/4502], Loss: 0.0446\n",
            "Epoch [1/15], Batch [3650/4502], Loss: 0.0196\n",
            "Epoch [1/15], Batch [3700/4502], Loss: 0.0543\n",
            "Epoch [1/15], Batch [3750/4502], Loss: 0.0173\n",
            "Epoch [1/15], Batch [3800/4502], Loss: 0.0253\n",
            "Epoch [1/15], Batch [3850/4502], Loss: 0.0597\n",
            "Epoch [1/15], Batch [3900/4502], Loss: 0.0297\n",
            "Epoch [1/15], Batch [3950/4502], Loss: 0.0171\n",
            "Epoch [1/15], Batch [4000/4502], Loss: 0.0174\n",
            "Epoch [1/15], Batch [4050/4502], Loss: 0.0219\n",
            "Epoch [1/15], Batch [4100/4502], Loss: 0.0151\n",
            "Epoch [1/15], Batch [4150/4502], Loss: 0.0224\n",
            "Epoch [1/15], Batch [4200/4502], Loss: 0.0276\n",
            "Epoch [1/15], Batch [4250/4502], Loss: 0.0543\n",
            "Epoch [1/15], Batch [4300/4502], Loss: 0.0293\n",
            "Epoch [1/15], Batch [4350/4502], Loss: 0.0105\n",
            "Epoch [1/15], Batch [4400/4502], Loss: 0.0310\n",
            "Epoch [1/15], Batch [4450/4502], Loss: 0.0359\n",
            "Epoch [1/15], Batch [4500/4502], Loss: 0.0100\n",
            "Epoch 1/15, Loss: 0.07492271518185387\n",
            "Updated Learning Rate: [8.577e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.86      0.92      3012\n",
            "  distortion       1.00      0.98      0.99      4518\n",
            "        fuzz       1.00      0.99      1.00      5271\n",
            "     tremolo       0.98      1.00      0.99      5671\n",
            "      phaser       1.00      0.99      0.99      4518\n",
            "     flanger       1.00      0.51      0.68      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.77      0.99      0.86      7530\n",
            " hall_reverb       0.74      0.98      0.85      4518\n",
            "plate_reverb       1.00      0.10      0.18      3012\n",
            "     octaver       0.96      0.99      0.97      2259\n",
            " auto_filter       0.97      1.00      0.98      3765\n",
            "\n",
            "   micro avg       0.92      0.91      0.91     52757\n",
            "   macro avg       0.95      0.87      0.87     52757\n",
            "weighted avg       0.94      0.91      0.89     52757\n",
            " samples avg       0.88      0.88      0.87     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0674, Accuracy: 0.7797, Precision: 0.9507, Recall: 0.8656, F1-score: 0.8681\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Batch [50/4502], Loss: 0.0258\n",
            "Epoch [2/15], Batch [100/4502], Loss: 0.0150\n",
            "Epoch [2/15], Batch [150/4502], Loss: 0.0261\n",
            "Epoch [2/15], Batch [200/4502], Loss: 0.0185\n",
            "Epoch [2/15], Batch [250/4502], Loss: 0.0517\n",
            "Epoch [2/15], Batch [300/4502], Loss: 0.0313\n",
            "Epoch [2/15], Batch [350/4502], Loss: 0.0118\n",
            "Epoch [2/15], Batch [400/4502], Loss: 0.0234\n",
            "Epoch [2/15], Batch [450/4502], Loss: 0.0243\n",
            "Epoch [2/15], Batch [500/4502], Loss: 0.0263\n",
            "Epoch [2/15], Batch [550/4502], Loss: 0.0380\n",
            "Epoch [2/15], Batch [600/4502], Loss: 0.0244\n",
            "Epoch [2/15], Batch [650/4502], Loss: 0.0363\n",
            "Epoch [2/15], Batch [700/4502], Loss: 0.0211\n",
            "Epoch [2/15], Batch [750/4502], Loss: 0.0250\n",
            "Epoch [2/15], Batch [800/4502], Loss: 0.0141\n",
            "Epoch [2/15], Batch [850/4502], Loss: 0.0080\n",
            "Epoch [2/15], Batch [900/4502], Loss: 0.0415\n",
            "Epoch [2/15], Batch [950/4502], Loss: 0.0143\n",
            "Epoch [2/15], Batch [1000/4502], Loss: 0.0115\n",
            "Epoch [2/15], Batch [1050/4502], Loss: 0.0173\n",
            "Epoch [2/15], Batch [1100/4502], Loss: 0.0280\n",
            "Epoch [2/15], Batch [1150/4502], Loss: 0.0110\n",
            "Epoch [2/15], Batch [1200/4502], Loss: 0.0131\n",
            "Epoch [2/15], Batch [1250/4502], Loss: 0.0216\n",
            "Epoch [2/15], Batch [1300/4502], Loss: 0.0334\n",
            "Epoch [2/15], Batch [1350/4502], Loss: 0.0279\n",
            "Epoch [2/15], Batch [1400/4502], Loss: 0.0096\n",
            "Epoch [2/15], Batch [1450/4502], Loss: 0.0099\n",
            "Epoch [2/15], Batch [1500/4502], Loss: 0.0100\n",
            "Epoch [2/15], Batch [1550/4502], Loss: 0.0097\n",
            "Epoch [2/15], Batch [1600/4502], Loss: 0.0195\n",
            "Epoch [2/15], Batch [1650/4502], Loss: 0.0244\n",
            "Epoch [2/15], Batch [1700/4502], Loss: 0.0164\n",
            "Epoch [2/15], Batch [1750/4502], Loss: 0.0240\n",
            "Epoch [2/15], Batch [1800/4502], Loss: 0.0205\n",
            "Epoch [2/15], Batch [1850/4502], Loss: 0.0124\n",
            "Epoch [2/15], Batch [1900/4502], Loss: 0.0152\n",
            "Epoch [2/15], Batch [1950/4502], Loss: 0.0195\n",
            "Epoch [2/15], Batch [2000/4502], Loss: 0.0132\n",
            "Epoch [2/15], Batch [2050/4502], Loss: 0.0104\n",
            "Epoch [2/15], Batch [2100/4502], Loss: 0.0140\n",
            "Epoch [2/15], Batch [2150/4502], Loss: 0.0336\n",
            "Epoch [2/15], Batch [2200/4502], Loss: 0.0162\n",
            "Epoch [2/15], Batch [2250/4502], Loss: 0.0064\n",
            "Epoch [2/15], Batch [2300/4502], Loss: 0.0126\n",
            "Epoch [2/15], Batch [2350/4502], Loss: 0.0197\n",
            "Epoch [2/15], Batch [2400/4502], Loss: 0.0246\n",
            "Epoch [2/15], Batch [2450/4502], Loss: 0.0210\n",
            "Epoch [2/15], Batch [2500/4502], Loss: 0.0118\n",
            "Epoch [2/15], Batch [2550/4502], Loss: 0.0124\n",
            "Epoch [2/15], Batch [2600/4502], Loss: 0.0165\n",
            "Epoch [2/15], Batch [2650/4502], Loss: 0.0191\n",
            "Epoch [2/15], Batch [2700/4502], Loss: 0.0344\n",
            "Epoch [2/15], Batch [2750/4502], Loss: 0.0344\n",
            "Epoch [2/15], Batch [2800/4502], Loss: 0.0233\n",
            "Epoch [2/15], Batch [2850/4502], Loss: 0.0304\n",
            "Epoch [2/15], Batch [2900/4502], Loss: 0.0039\n",
            "Epoch [2/15], Batch [2950/4502], Loss: 0.0079\n",
            "Epoch [2/15], Batch [3000/4502], Loss: 0.0193\n",
            "Epoch [2/15], Batch [3050/4502], Loss: 0.0164\n",
            "Epoch [2/15], Batch [3100/4502], Loss: 0.0074\n",
            "Epoch [2/15], Batch [3150/4502], Loss: 0.0124\n",
            "Epoch [2/15], Batch [3200/4502], Loss: 0.0063\n",
            "Epoch [2/15], Batch [3250/4502], Loss: 0.0371\n",
            "Epoch [2/15], Batch [3300/4502], Loss: 0.0500\n",
            "Epoch [2/15], Batch [3350/4502], Loss: 0.0228\n",
            "Epoch [2/15], Batch [3400/4502], Loss: 0.0134\n",
            "Epoch [2/15], Batch [3450/4502], Loss: 0.0183\n",
            "Epoch [2/15], Batch [3500/4502], Loss: 0.0242\n",
            "Epoch [2/15], Batch [3550/4502], Loss: 0.0105\n",
            "Epoch [2/15], Batch [3600/4502], Loss: 0.0171\n",
            "Epoch [2/15], Batch [3650/4502], Loss: 0.0411\n",
            "Epoch [2/15], Batch [3700/4502], Loss: 0.0124\n",
            "Epoch [2/15], Batch [3750/4502], Loss: 0.0049\n",
            "Epoch [2/15], Batch [3800/4502], Loss: 0.0189\n",
            "Epoch [2/15], Batch [3850/4502], Loss: 0.0051\n",
            "Epoch [2/15], Batch [3900/4502], Loss: 0.0170\n",
            "Epoch [2/15], Batch [3950/4502], Loss: 0.0151\n",
            "Epoch [2/15], Batch [4000/4502], Loss: 0.0202\n",
            "Epoch [2/15], Batch [4050/4502], Loss: 0.0061\n",
            "Epoch [2/15], Batch [4100/4502], Loss: 0.0174\n",
            "Epoch [2/15], Batch [4150/4502], Loss: 0.0070\n",
            "Epoch [2/15], Batch [4200/4502], Loss: 0.0092\n",
            "Epoch [2/15], Batch [4250/4502], Loss: 0.0067\n",
            "Epoch [2/15], Batch [4300/4502], Loss: 0.0281\n",
            "Epoch [2/15], Batch [4350/4502], Loss: 0.0088\n",
            "Epoch [2/15], Batch [4400/4502], Loss: 0.0088\n",
            "Epoch [2/15], Batch [4450/4502], Loss: 0.0276\n",
            "Epoch [2/15], Batch [4500/4502], Loss: 0.0182\n",
            "Epoch 2/15, Loss: 0.020295779205917044\n",
            "Updated Learning Rate: [7.3564929e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.99      0.99      0.99      3012\n",
            "  distortion       1.00      0.99      1.00      4518\n",
            "        fuzz       0.99      1.00      0.99      5271\n",
            "     tremolo       1.00      0.98      0.99      5671\n",
            "      phaser       0.97      1.00      0.98      4518\n",
            "     flanger       0.99      0.98      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.96      0.98      0.97      7530\n",
            " hall_reverb       0.88      0.88      0.88      4518\n",
            "plate_reverb       0.56      0.99      0.72      3012\n",
            "     octaver       1.00      0.92      0.96      2259\n",
            " auto_filter       1.00      0.95      0.98      3765\n",
            "\n",
            "   micro avg       0.94      0.97      0.96     52757\n",
            "   macro avg       0.94      0.97      0.95     52757\n",
            "weighted avg       0.95      0.97      0.96     52757\n",
            " samples avg       0.92      0.95      0.93     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0377, Accuracy: 0.8709, Precision: 0.9446, Recall: 0.9723, F1-score: 0.9532\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/15], Batch [50/4502], Loss: 0.0146\n",
            "Epoch [3/15], Batch [100/4502], Loss: 0.0162\n",
            "Epoch [3/15], Batch [150/4502], Loss: 0.0249\n",
            "Epoch [3/15], Batch [200/4502], Loss: 0.0146\n",
            "Epoch [3/15], Batch [250/4502], Loss: 0.0188\n",
            "Epoch [3/15], Batch [300/4502], Loss: 0.0129\n",
            "Epoch [3/15], Batch [350/4502], Loss: 0.0308\n",
            "Epoch [3/15], Batch [400/4502], Loss: 0.0231\n",
            "Epoch [3/15], Batch [450/4502], Loss: 0.0168\n",
            "Epoch [3/15], Batch [500/4502], Loss: 0.0066\n",
            "Epoch [3/15], Batch [550/4502], Loss: 0.0075\n",
            "Epoch [3/15], Batch [600/4502], Loss: 0.0194\n",
            "Epoch [3/15], Batch [650/4502], Loss: 0.0410\n",
            "Epoch [3/15], Batch [700/4502], Loss: 0.0048\n",
            "Epoch [3/15], Batch [750/4502], Loss: 0.0114\n",
            "Epoch [3/15], Batch [800/4502], Loss: 0.0167\n",
            "Epoch [3/15], Batch [850/4502], Loss: 0.0096\n",
            "Epoch [3/15], Batch [900/4502], Loss: 0.0051\n",
            "Epoch [3/15], Batch [950/4502], Loss: 0.0039\n",
            "Epoch [3/15], Batch [1000/4502], Loss: 0.0279\n",
            "Epoch [3/15], Batch [1050/4502], Loss: 0.0098\n",
            "Epoch [3/15], Batch [1100/4502], Loss: 0.0213\n",
            "Epoch [3/15], Batch [1150/4502], Loss: 0.0086\n",
            "Epoch [3/15], Batch [1200/4502], Loss: 0.0180\n",
            "Epoch [3/15], Batch [1250/4502], Loss: 0.0121\n",
            "Epoch [3/15], Batch [1300/4502], Loss: 0.0053\n",
            "Epoch [3/15], Batch [1350/4502], Loss: 0.0089\n",
            "Epoch [3/15], Batch [1400/4502], Loss: 0.0390\n",
            "Epoch [3/15], Batch [1450/4502], Loss: 0.0072\n",
            "Epoch [3/15], Batch [1500/4502], Loss: 0.0055\n",
            "Epoch [3/15], Batch [1550/4502], Loss: 0.0301\n",
            "Epoch [3/15], Batch [1600/4502], Loss: 0.0073\n",
            "Epoch [3/15], Batch [1650/4502], Loss: 0.0169\n",
            "Epoch [3/15], Batch [1700/4502], Loss: 0.0236\n",
            "Epoch [3/15], Batch [1750/4502], Loss: 0.0075\n",
            "Epoch [3/15], Batch [1800/4502], Loss: 0.0094\n",
            "Epoch [3/15], Batch [1850/4502], Loss: 0.0111\n",
            "Epoch [3/15], Batch [1900/4502], Loss: 0.0324\n",
            "Epoch [3/15], Batch [1950/4502], Loss: 0.0178\n",
            "Epoch [3/15], Batch [2000/4502], Loss: 0.0032\n",
            "Epoch [3/15], Batch [2050/4502], Loss: 0.0408\n",
            "Epoch [3/15], Batch [2100/4502], Loss: 0.0085\n",
            "Epoch [3/15], Batch [2150/4502], Loss: 0.0045\n",
            "Epoch [3/15], Batch [2200/4502], Loss: 0.0174\n",
            "Epoch [3/15], Batch [2250/4502], Loss: 0.0059\n",
            "Epoch [3/15], Batch [2300/4502], Loss: 0.0058\n",
            "Epoch [3/15], Batch [2350/4502], Loss: 0.0060\n",
            "Epoch [3/15], Batch [2400/4502], Loss: 0.0124\n",
            "Epoch [3/15], Batch [2450/4502], Loss: 0.0139\n",
            "Epoch [3/15], Batch [2500/4502], Loss: 0.0072\n",
            "Epoch [3/15], Batch [2550/4502], Loss: 0.0152\n",
            "Epoch [3/15], Batch [2600/4502], Loss: 0.0147\n",
            "Epoch [3/15], Batch [2650/4502], Loss: 0.0037\n",
            "Epoch [3/15], Batch [2700/4502], Loss: 0.0139\n",
            "Epoch [3/15], Batch [2750/4502], Loss: 0.0117\n",
            "Epoch [3/15], Batch [2800/4502], Loss: 0.0041\n",
            "Epoch [3/15], Batch [2850/4502], Loss: 0.0136\n",
            "Epoch [3/15], Batch [2900/4502], Loss: 0.0171\n",
            "Epoch [3/15], Batch [2950/4502], Loss: 0.0031\n",
            "Epoch [3/15], Batch [3000/4502], Loss: 0.0156\n",
            "Epoch [3/15], Batch [3050/4502], Loss: 0.0028\n",
            "Epoch [3/15], Batch [3100/4502], Loss: 0.0196\n",
            "Epoch [3/15], Batch [3150/4502], Loss: 0.0140\n",
            "Epoch [3/15], Batch [3200/4502], Loss: 0.0053\n",
            "Epoch [3/15], Batch [3250/4502], Loss: 0.0080\n",
            "Epoch [3/15], Batch [3300/4502], Loss: 0.0113\n",
            "Epoch [3/15], Batch [3350/4502], Loss: 0.0108\n",
            "Epoch [3/15], Batch [3400/4502], Loss: 0.0056\n",
            "Epoch [3/15], Batch [3450/4502], Loss: 0.0141\n",
            "Epoch [3/15], Batch [3500/4502], Loss: 0.0071\n",
            "Epoch [3/15], Batch [3550/4502], Loss: 0.0324\n",
            "Epoch [3/15], Batch [3600/4502], Loss: 0.0038\n",
            "Epoch [3/15], Batch [3650/4502], Loss: 0.0050\n",
            "Epoch [3/15], Batch [3700/4502], Loss: 0.0049\n",
            "Epoch [3/15], Batch [3750/4502], Loss: 0.0089\n",
            "Epoch [3/15], Batch [3800/4502], Loss: 0.0074\n",
            "Epoch [3/15], Batch [3850/4502], Loss: 0.0100\n",
            "Epoch [3/15], Batch [3900/4502], Loss: 0.0173\n",
            "Epoch [3/15], Batch [3950/4502], Loss: 0.0310\n",
            "Epoch [3/15], Batch [4000/4502], Loss: 0.0036\n",
            "Epoch [3/15], Batch [4050/4502], Loss: 0.0054\n",
            "Epoch [3/15], Batch [4100/4502], Loss: 0.0018\n",
            "Epoch [3/15], Batch [4150/4502], Loss: 0.0098\n",
            "Epoch [3/15], Batch [4200/4502], Loss: 0.0036\n",
            "Epoch [3/15], Batch [4250/4502], Loss: 0.0152\n",
            "Epoch [3/15], Batch [4300/4502], Loss: 0.0135\n",
            "Epoch [3/15], Batch [4350/4502], Loss: 0.0068\n",
            "Epoch [3/15], Batch [4400/4502], Loss: 0.0055\n",
            "Epoch [3/15], Batch [4450/4502], Loss: 0.0017\n",
            "Epoch [3/15], Batch [4500/4502], Loss: 0.0216\n",
            "Epoch 3/15, Loss: 0.013446120887066754\n",
            "Updated Learning Rate: [6.30966396033e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.99      1.00      0.99      3012\n",
            "  distortion       0.99      0.99      0.99      4518\n",
            "        fuzz       1.00      0.99      0.99      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      0.99      0.99      4518\n",
            "     flanger       1.00      0.99      0.99      3012\n",
            "      chorus       0.96      1.00      0.98      5671\n",
            "       delay       0.98      0.92      0.95      7530\n",
            " hall_reverb       0.89      0.89      0.89      4518\n",
            "plate_reverb       0.82      0.99      0.90      3012\n",
            "     octaver       0.99      0.99      0.99      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.97      0.97      0.97     52757\n",
            "   macro avg       0.97      0.98      0.97     52757\n",
            "weighted avg       0.97      0.97      0.97     52757\n",
            " samples avg       0.94      0.95      0.94     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0224, Accuracy: 0.9209, Precision: 0.9687, Recall: 0.9780, F1-score: 0.9725\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/15], Batch [50/4502], Loss: 0.0121\n",
            "Epoch [4/15], Batch [100/4502], Loss: 0.0099\n",
            "Epoch [4/15], Batch [150/4502], Loss: 0.0100\n",
            "Epoch [4/15], Batch [200/4502], Loss: 0.0188\n",
            "Epoch [4/15], Batch [250/4502], Loss: 0.0040\n",
            "Epoch [4/15], Batch [300/4502], Loss: 0.0086\n",
            "Epoch [4/15], Batch [350/4502], Loss: 0.0020\n",
            "Epoch [4/15], Batch [400/4502], Loss: 0.0202\n",
            "Epoch [4/15], Batch [450/4502], Loss: 0.0048\n",
            "Epoch [4/15], Batch [500/4502], Loss: 0.0086\n",
            "Epoch [4/15], Batch [550/4502], Loss: 0.0061\n",
            "Epoch [4/15], Batch [600/4502], Loss: 0.0235\n",
            "Epoch [4/15], Batch [650/4502], Loss: 0.0167\n",
            "Epoch [4/15], Batch [700/4502], Loss: 0.0093\n",
            "Epoch [4/15], Batch [750/4502], Loss: 0.0029\n",
            "Epoch [4/15], Batch [800/4502], Loss: 0.0037\n",
            "Epoch [4/15], Batch [850/4502], Loss: 0.0049\n",
            "Epoch [4/15], Batch [900/4502], Loss: 0.0036\n",
            "Epoch [4/15], Batch [950/4502], Loss: 0.0141\n",
            "Epoch [4/15], Batch [1000/4502], Loss: 0.0272\n",
            "Epoch [4/15], Batch [1050/4502], Loss: 0.0118\n",
            "Epoch [4/15], Batch [1100/4502], Loss: 0.0048\n",
            "Epoch [4/15], Batch [1150/4502], Loss: 0.0220\n",
            "Epoch [4/15], Batch [1200/4502], Loss: 0.0067\n",
            "Epoch [4/15], Batch [1250/4502], Loss: 0.0102\n",
            "Epoch [4/15], Batch [1300/4502], Loss: 0.0034\n",
            "Epoch [4/15], Batch [1350/4502], Loss: 0.0055\n",
            "Epoch [4/15], Batch [1400/4502], Loss: 0.0032\n",
            "Epoch [4/15], Batch [1450/4502], Loss: 0.0016\n",
            "Epoch [4/15], Batch [1500/4502], Loss: 0.0329\n",
            "Epoch [4/15], Batch [1550/4502], Loss: 0.0096\n",
            "Epoch [4/15], Batch [1600/4502], Loss: 0.0221\n",
            "Epoch [4/15], Batch [1650/4502], Loss: 0.0168\n",
            "Epoch [4/15], Batch [1700/4502], Loss: 0.0112\n",
            "Epoch [4/15], Batch [1750/4502], Loss: 0.0147\n",
            "Epoch [4/15], Batch [1800/4502], Loss: 0.0053\n",
            "Epoch [4/15], Batch [1850/4502], Loss: 0.0193\n",
            "Epoch [4/15], Batch [1900/4502], Loss: 0.0030\n",
            "Epoch [4/15], Batch [1950/4502], Loss: 0.0035\n",
            "Epoch [4/15], Batch [2000/4502], Loss: 0.0102\n",
            "Epoch [4/15], Batch [2050/4502], Loss: 0.0120\n",
            "Epoch [4/15], Batch [2100/4502], Loss: 0.0229\n",
            "Epoch [4/15], Batch [2150/4502], Loss: 0.0048\n",
            "Epoch [4/15], Batch [2200/4502], Loss: 0.0106\n",
            "Epoch [4/15], Batch [2250/4502], Loss: 0.0082\n",
            "Epoch [4/15], Batch [2300/4502], Loss: 0.0065\n",
            "Epoch [4/15], Batch [2350/4502], Loss: 0.0032\n",
            "Epoch [4/15], Batch [2400/4502], Loss: 0.0083\n",
            "Epoch [4/15], Batch [2450/4502], Loss: 0.0193\n",
            "Epoch [4/15], Batch [2500/4502], Loss: 0.0087\n",
            "Epoch [4/15], Batch [2550/4502], Loss: 0.0127\n",
            "Epoch [4/15], Batch [2600/4502], Loss: 0.0278\n",
            "Epoch [4/15], Batch [2650/4502], Loss: 0.0021\n",
            "Epoch [4/15], Batch [2700/4502], Loss: 0.0006\n",
            "Epoch [4/15], Batch [2750/4502], Loss: 0.0036\n",
            "Epoch [4/15], Batch [2800/4502], Loss: 0.0009\n",
            "Epoch [4/15], Batch [2850/4502], Loss: 0.0025\n",
            "Epoch [4/15], Batch [2900/4502], Loss: 0.0032\n",
            "Epoch [4/15], Batch [2950/4502], Loss: 0.0157\n",
            "Epoch [4/15], Batch [3000/4502], Loss: 0.0083\n",
            "Epoch [4/15], Batch [3050/4502], Loss: 0.0045\n",
            "Epoch [4/15], Batch [3100/4502], Loss: 0.0131\n",
            "Epoch [4/15], Batch [3150/4502], Loss: 0.0254\n",
            "Epoch [4/15], Batch [3200/4502], Loss: 0.0133\n",
            "Epoch [4/15], Batch [3250/4502], Loss: 0.0131\n",
            "Epoch [4/15], Batch [3300/4502], Loss: 0.0200\n",
            "Epoch [4/15], Batch [3350/4502], Loss: 0.0063\n",
            "Epoch [4/15], Batch [3400/4502], Loss: 0.0110\n",
            "Epoch [4/15], Batch [3450/4502], Loss: 0.0186\n",
            "Epoch [4/15], Batch [3500/4502], Loss: 0.0030\n",
            "Epoch [4/15], Batch [3550/4502], Loss: 0.0029\n",
            "Epoch [4/15], Batch [3600/4502], Loss: 0.0247\n",
            "Epoch [4/15], Batch [3650/4502], Loss: 0.0041\n",
            "Epoch [4/15], Batch [3700/4502], Loss: 0.0046\n",
            "Epoch [4/15], Batch [3750/4502], Loss: 0.0070\n",
            "Epoch [4/15], Batch [3800/4502], Loss: 0.0061\n",
            "Epoch [4/15], Batch [3850/4502], Loss: 0.0031\n",
            "Epoch [4/15], Batch [3900/4502], Loss: 0.0115\n",
            "Epoch [4/15], Batch [3950/4502], Loss: 0.0095\n",
            "Epoch [4/15], Batch [4000/4502], Loss: 0.0030\n",
            "Epoch [4/15], Batch [4050/4502], Loss: 0.0063\n",
            "Epoch [4/15], Batch [4100/4502], Loss: 0.0016\n",
            "Epoch [4/15], Batch [4150/4502], Loss: 0.0217\n",
            "Epoch [4/15], Batch [4200/4502], Loss: 0.0114\n",
            "Epoch [4/15], Batch [4250/4502], Loss: 0.0341\n",
            "Epoch [4/15], Batch [4300/4502], Loss: 0.0118\n",
            "Epoch [4/15], Batch [4350/4502], Loss: 0.0022\n",
            "Epoch [4/15], Batch [4400/4502], Loss: 0.0021\n",
            "Epoch [4/15], Batch [4450/4502], Loss: 0.0083\n",
            "Epoch [4/15], Batch [4500/4502], Loss: 0.0022\n",
            "Epoch 4/15, Loss: 0.010223782482385918\n",
            "Updated Learning Rate: [5.4117987787750416e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.98      1.00      0.99      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      0.99      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      0.99      1.00      4518\n",
            "     flanger       0.99      0.99      0.99      3012\n",
            "      chorus       0.98      1.00      0.99      5671\n",
            "       delay       0.98      0.99      0.98      7530\n",
            " hall_reverb       0.93      0.95      0.94      4518\n",
            "plate_reverb       0.88      0.99      0.93      3012\n",
            "     octaver       0.96      1.00      0.98      2259\n",
            " auto_filter       0.99      0.99      0.99      3765\n",
            "\n",
            "   micro avg       0.98      0.99      0.98     52757\n",
            "   macro avg       0.97      0.99      0.98     52757\n",
            "weighted avg       0.98      0.99      0.98     52757\n",
            " samples avg       0.95      0.97      0.96     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0131, Accuracy: 0.9505, Precision: 0.9736, Recall: 0.9906, F1-score: 0.9817\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/15], Batch [50/4502], Loss: 0.0085\n",
            "Epoch [5/15], Batch [100/4502], Loss: 0.0131\n",
            "Epoch [5/15], Batch [150/4502], Loss: 0.0081\n",
            "Epoch [5/15], Batch [200/4502], Loss: 0.0064\n",
            "Epoch [5/15], Batch [250/4502], Loss: 0.0025\n",
            "Epoch [5/15], Batch [300/4502], Loss: 0.0080\n",
            "Epoch [5/15], Batch [350/4502], Loss: 0.0063\n",
            "Epoch [5/15], Batch [400/4502], Loss: 0.0023\n",
            "Epoch [5/15], Batch [450/4502], Loss: 0.0226\n",
            "Epoch [5/15], Batch [500/4502], Loss: 0.0028\n",
            "Epoch [5/15], Batch [550/4502], Loss: 0.0097\n",
            "Epoch [5/15], Batch [600/4502], Loss: 0.0141\n",
            "Epoch [5/15], Batch [650/4502], Loss: 0.0069\n",
            "Epoch [5/15], Batch [700/4502], Loss: 0.0392\n",
            "Epoch [5/15], Batch [750/4502], Loss: 0.0101\n",
            "Epoch [5/15], Batch [800/4502], Loss: 0.0075\n",
            "Epoch [5/15], Batch [850/4502], Loss: 0.0026\n",
            "Epoch [5/15], Batch [900/4502], Loss: 0.0020\n",
            "Epoch [5/15], Batch [950/4502], Loss: 0.0024\n",
            "Epoch [5/15], Batch [1000/4502], Loss: 0.0093\n",
            "Epoch [5/15], Batch [1050/4502], Loss: 0.0033\n",
            "Epoch [5/15], Batch [1100/4502], Loss: 0.0051\n",
            "Epoch [5/15], Batch [1150/4502], Loss: 0.0032\n",
            "Epoch [5/15], Batch [1200/4502], Loss: 0.0059\n",
            "Epoch [5/15], Batch [1250/4502], Loss: 0.0091\n",
            "Epoch [5/15], Batch [1300/4502], Loss: 0.0008\n",
            "Epoch [5/15], Batch [1350/4502], Loss: 0.0045\n",
            "Epoch [5/15], Batch [1400/4502], Loss: 0.0033\n",
            "Epoch [5/15], Batch [1450/4502], Loss: 0.0049\n",
            "Epoch [5/15], Batch [1500/4502], Loss: 0.0249\n",
            "Epoch [5/15], Batch [1550/4502], Loss: 0.0023\n",
            "Epoch [5/15], Batch [1600/4502], Loss: 0.0077\n",
            "Epoch [5/15], Batch [1650/4502], Loss: 0.0081\n",
            "Epoch [5/15], Batch [1700/4502], Loss: 0.0073\n",
            "Epoch [5/15], Batch [1750/4502], Loss: 0.0003\n",
            "Epoch [5/15], Batch [1800/4502], Loss: 0.0183\n",
            "Epoch [5/15], Batch [1850/4502], Loss: 0.0134\n",
            "Epoch [5/15], Batch [1900/4502], Loss: 0.0078\n",
            "Epoch [5/15], Batch [1950/4502], Loss: 0.0060\n",
            "Epoch [5/15], Batch [2000/4502], Loss: 0.0039\n",
            "Epoch [5/15], Batch [2050/4502], Loss: 0.0027\n",
            "Epoch [5/15], Batch [2100/4502], Loss: 0.0240\n",
            "Epoch [5/15], Batch [2150/4502], Loss: 0.0166\n",
            "Epoch [5/15], Batch [2200/4502], Loss: 0.0136\n",
            "Epoch [5/15], Batch [2250/4502], Loss: 0.0068\n",
            "Epoch [5/15], Batch [2300/4502], Loss: 0.0027\n",
            "Epoch [5/15], Batch [2350/4502], Loss: 0.0042\n",
            "Epoch [5/15], Batch [2400/4502], Loss: 0.0250\n",
            "Epoch [5/15], Batch [2450/4502], Loss: 0.0026\n",
            "Epoch [5/15], Batch [2500/4502], Loss: 0.0038\n",
            "Epoch [5/15], Batch [2550/4502], Loss: 0.0188\n",
            "Epoch [5/15], Batch [2600/4502], Loss: 0.0089\n",
            "Epoch [5/15], Batch [2650/4502], Loss: 0.0138\n",
            "Epoch [5/15], Batch [2700/4502], Loss: 0.0242\n",
            "Epoch [5/15], Batch [2750/4502], Loss: 0.0173\n",
            "Epoch [5/15], Batch [2800/4502], Loss: 0.0026\n",
            "Epoch [5/15], Batch [2850/4502], Loss: 0.0039\n",
            "Epoch [5/15], Batch [2900/4502], Loss: 0.0059\n",
            "Epoch [5/15], Batch [2950/4502], Loss: 0.0168\n",
            "Epoch [5/15], Batch [3000/4502], Loss: 0.0081\n",
            "Epoch [5/15], Batch [3050/4502], Loss: 0.0262\n",
            "Epoch [5/15], Batch [3100/4502], Loss: 0.0135\n",
            "Epoch [5/15], Batch [3150/4502], Loss: 0.0155\n",
            "Epoch [5/15], Batch [3200/4502], Loss: 0.0163\n",
            "Epoch [5/15], Batch [3250/4502], Loss: 0.0082\n",
            "Epoch [5/15], Batch [3300/4502], Loss: 0.0046\n",
            "Epoch [5/15], Batch [3350/4502], Loss: 0.0127\n",
            "Epoch [5/15], Batch [3400/4502], Loss: 0.0114\n",
            "Epoch [5/15], Batch [3450/4502], Loss: 0.0143\n",
            "Epoch [5/15], Batch [3500/4502], Loss: 0.0055\n",
            "Epoch [5/15], Batch [3550/4502], Loss: 0.0047\n",
            "Epoch [5/15], Batch [3600/4502], Loss: 0.0055\n",
            "Epoch [5/15], Batch [3650/4502], Loss: 0.0009\n",
            "Epoch [5/15], Batch [3700/4502], Loss: 0.0020\n",
            "Epoch [5/15], Batch [3750/4502], Loss: 0.0014\n",
            "Epoch [5/15], Batch [3800/4502], Loss: 0.0103\n",
            "Epoch [5/15], Batch [3850/4502], Loss: 0.0010\n",
            "Epoch [5/15], Batch [3900/4502], Loss: 0.0226\n",
            "Epoch [5/15], Batch [3950/4502], Loss: 0.0015\n",
            "Epoch [5/15], Batch [4000/4502], Loss: 0.0055\n",
            "Epoch [5/15], Batch [4050/4502], Loss: 0.0027\n",
            "Epoch [5/15], Batch [4100/4502], Loss: 0.0198\n",
            "Epoch [5/15], Batch [4150/4502], Loss: 0.0161\n",
            "Epoch [5/15], Batch [4200/4502], Loss: 0.0268\n",
            "Epoch [5/15], Batch [4250/4502], Loss: 0.0061\n",
            "Epoch [5/15], Batch [4300/4502], Loss: 0.0017\n",
            "Epoch [5/15], Batch [4350/4502], Loss: 0.0095\n",
            "Epoch [5/15], Batch [4400/4502], Loss: 0.0065\n",
            "Epoch [5/15], Batch [4450/4502], Loss: 0.0006\n",
            "Epoch [5/15], Batch [4500/4502], Loss: 0.0003\n",
            "Epoch 5/15, Loss: 0.007992865368463603\n",
            "Updated Learning Rate: [4.641699812555353e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.98      0.99      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.97      0.98      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.97      0.99      0.98      7530\n",
            " hall_reverb       0.95      0.98      0.96      4518\n",
            "plate_reverb       0.91      0.98      0.94      3012\n",
            "     octaver       1.00      0.99      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     52757\n",
            "   macro avg       0.99      0.99      0.99     52757\n",
            "weighted avg       0.99      0.99      0.99     52757\n",
            " samples avg       0.96      0.97      0.96     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0094, Accuracy: 0.9637, Precision: 0.9855, Recall: 0.9893, F1-score: 0.9872\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/15], Batch [50/4502], Loss: 0.0148\n",
            "Epoch [6/15], Batch [100/4502], Loss: 0.0025\n",
            "Epoch [6/15], Batch [150/4502], Loss: 0.0004\n",
            "Epoch [6/15], Batch [200/4502], Loss: 0.0064\n",
            "Epoch [6/15], Batch [250/4502], Loss: 0.0019\n",
            "Epoch [6/15], Batch [300/4502], Loss: 0.0013\n",
            "Epoch [6/15], Batch [350/4502], Loss: 0.0057\n",
            "Epoch [6/15], Batch [400/4502], Loss: 0.0116\n",
            "Epoch [6/15], Batch [450/4502], Loss: 0.0034\n",
            "Epoch [6/15], Batch [500/4502], Loss: 0.0011\n",
            "Epoch [6/15], Batch [550/4502], Loss: 0.0008\n",
            "Epoch [6/15], Batch [600/4502], Loss: 0.0018\n",
            "Epoch [6/15], Batch [650/4502], Loss: 0.0035\n",
            "Epoch [6/15], Batch [700/4502], Loss: 0.0015\n",
            "Epoch [6/15], Batch [750/4502], Loss: 0.0142\n",
            "Epoch [6/15], Batch [800/4502], Loss: 0.0248\n",
            "Epoch [6/15], Batch [850/4502], Loss: 0.0142\n",
            "Epoch [6/15], Batch [900/4502], Loss: 0.0010\n",
            "Epoch [6/15], Batch [950/4502], Loss: 0.0056\n",
            "Epoch [6/15], Batch [1000/4502], Loss: 0.0023\n",
            "Epoch [6/15], Batch [1050/4502], Loss: 0.0196\n",
            "Epoch [6/15], Batch [1100/4502], Loss: 0.0043\n",
            "Epoch [6/15], Batch [1150/4502], Loss: 0.0058\n",
            "Epoch [6/15], Batch [1200/4502], Loss: 0.0030\n",
            "Epoch [6/15], Batch [1250/4502], Loss: 0.0115\n",
            "Epoch [6/15], Batch [1300/4502], Loss: 0.0006\n",
            "Epoch [6/15], Batch [1350/4502], Loss: 0.0009\n",
            "Epoch [6/15], Batch [1400/4502], Loss: 0.0023\n",
            "Epoch [6/15], Batch [1450/4502], Loss: 0.0045\n",
            "Epoch [6/15], Batch [1500/4502], Loss: 0.0103\n",
            "Epoch [6/15], Batch [1550/4502], Loss: 0.0146\n",
            "Epoch [6/15], Batch [1600/4502], Loss: 0.0027\n",
            "Epoch [6/15], Batch [1650/4502], Loss: 0.0090\n",
            "Epoch [6/15], Batch [1700/4502], Loss: 0.0273\n",
            "Epoch [6/15], Batch [1750/4502], Loss: 0.0175\n",
            "Epoch [6/15], Batch [1800/4502], Loss: 0.0085\n",
            "Epoch [6/15], Batch [1850/4502], Loss: 0.0015\n",
            "Epoch [6/15], Batch [1900/4502], Loss: 0.0013\n",
            "Epoch [6/15], Batch [1950/4502], Loss: 0.0060\n",
            "Epoch [6/15], Batch [2000/4502], Loss: 0.0315\n",
            "Epoch [6/15], Batch [2050/4502], Loss: 0.0032\n",
            "Epoch [6/15], Batch [2100/4502], Loss: 0.0009\n",
            "Epoch [6/15], Batch [2150/4502], Loss: 0.0030\n",
            "Epoch [6/15], Batch [2200/4502], Loss: 0.0035\n",
            "Epoch [6/15], Batch [2250/4502], Loss: 0.0031\n",
            "Epoch [6/15], Batch [2300/4502], Loss: 0.0005\n",
            "Epoch [6/15], Batch [2350/4502], Loss: 0.0050\n",
            "Epoch [6/15], Batch [2400/4502], Loss: 0.0136\n",
            "Epoch [6/15], Batch [2450/4502], Loss: 0.0038\n",
            "Epoch [6/15], Batch [2500/4502], Loss: 0.0031\n",
            "Epoch [6/15], Batch [2550/4502], Loss: 0.0181\n",
            "Epoch [6/15], Batch [2600/4502], Loss: 0.0037\n",
            "Epoch [6/15], Batch [2650/4502], Loss: 0.0080\n",
            "Epoch [6/15], Batch [2700/4502], Loss: 0.0116\n",
            "Epoch [6/15], Batch [2750/4502], Loss: 0.0101\n",
            "Epoch [6/15], Batch [2800/4502], Loss: 0.0013\n",
            "Epoch [6/15], Batch [2850/4502], Loss: 0.0040\n",
            "Epoch [6/15], Batch [2900/4502], Loss: 0.0013\n",
            "Epoch [6/15], Batch [2950/4502], Loss: 0.0051\n",
            "Epoch [6/15], Batch [3000/4502], Loss: 0.0054\n",
            "Epoch [6/15], Batch [3050/4502], Loss: 0.0111\n",
            "Epoch [6/15], Batch [3100/4502], Loss: 0.0162\n",
            "Epoch [6/15], Batch [3150/4502], Loss: 0.0041\n",
            "Epoch [6/15], Batch [3200/4502], Loss: 0.0010\n",
            "Epoch [6/15], Batch [3250/4502], Loss: 0.0090\n",
            "Epoch [6/15], Batch [3300/4502], Loss: 0.0021\n",
            "Epoch [6/15], Batch [3350/4502], Loss: 0.0027\n",
            "Epoch [6/15], Batch [3400/4502], Loss: 0.0025\n",
            "Epoch [6/15], Batch [3450/4502], Loss: 0.0002\n",
            "Epoch [6/15], Batch [3500/4502], Loss: 0.0083\n",
            "Epoch [6/15], Batch [3550/4502], Loss: 0.0021\n",
            "Epoch [6/15], Batch [3600/4502], Loss: 0.0100\n",
            "Epoch [6/15], Batch [3650/4502], Loss: 0.0068\n",
            "Epoch [6/15], Batch [3700/4502], Loss: 0.0036\n",
            "Epoch [6/15], Batch [3750/4502], Loss: 0.0008\n",
            "Epoch [6/15], Batch [3800/4502], Loss: 0.0241\n",
            "Epoch [6/15], Batch [3850/4502], Loss: 0.0119\n",
            "Epoch [6/15], Batch [3900/4502], Loss: 0.0052\n",
            "Epoch [6/15], Batch [3950/4502], Loss: 0.0038\n",
            "Epoch [6/15], Batch [4000/4502], Loss: 0.0165\n",
            "Epoch [6/15], Batch [4050/4502], Loss: 0.0058\n",
            "Epoch [6/15], Batch [4100/4502], Loss: 0.0043\n",
            "Epoch [6/15], Batch [4150/4502], Loss: 0.0003\n",
            "Epoch [6/15], Batch [4200/4502], Loss: 0.0011\n",
            "Epoch [6/15], Batch [4250/4502], Loss: 0.0132\n",
            "Epoch [6/15], Batch [4300/4502], Loss: 0.0065\n",
            "Epoch [6/15], Batch [4350/4502], Loss: 0.0106\n",
            "Epoch [6/15], Batch [4400/4502], Loss: 0.0012\n",
            "Epoch [6/15], Batch [4450/4502], Loss: 0.0095\n",
            "Epoch [6/15], Batch [4500/4502], Loss: 0.0003\n",
            "Epoch 6/15, Loss: 0.006598163439651372\n",
            "Updated Learning Rate: [3.9811859292287264e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       0.96      1.00      0.98      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.98      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.98      0.97      0.98      7530\n",
            " hall_reverb       0.92      0.98      0.95      4518\n",
            "plate_reverb       1.00      0.86      0.93      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.98      0.98     52757\n",
            "   macro avg       0.99      0.98      0.98     52757\n",
            "weighted avg       0.99      0.98      0.98     52757\n",
            " samples avg       0.96      0.96      0.96     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0126, Accuracy: 0.9536, Precision: 0.9875, Recall: 0.9809, F1-score: 0.9837\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/15], Batch [50/4502], Loss: 0.0052\n",
            "Epoch [7/15], Batch [100/4502], Loss: 0.0039\n",
            "Epoch [7/15], Batch [150/4502], Loss: 0.0012\n",
            "Epoch [7/15], Batch [200/4502], Loss: 0.0085\n",
            "Epoch [7/15], Batch [250/4502], Loss: 0.0151\n",
            "Epoch [7/15], Batch [300/4502], Loss: 0.0035\n",
            "Epoch [7/15], Batch [350/4502], Loss: 0.0069\n",
            "Epoch [7/15], Batch [400/4502], Loss: 0.0024\n",
            "Epoch [7/15], Batch [450/4502], Loss: 0.0017\n",
            "Epoch [7/15], Batch [500/4502], Loss: 0.0095\n",
            "Epoch [7/15], Batch [550/4502], Loss: 0.0001\n",
            "Epoch [7/15], Batch [600/4502], Loss: 0.0178\n",
            "Epoch [7/15], Batch [650/4502], Loss: 0.0003\n",
            "Epoch [7/15], Batch [700/4502], Loss: 0.0040\n",
            "Epoch [7/15], Batch [750/4502], Loss: 0.0003\n",
            "Epoch [7/15], Batch [800/4502], Loss: 0.0023\n",
            "Epoch [7/15], Batch [850/4502], Loss: 0.0023\n",
            "Epoch [7/15], Batch [900/4502], Loss: 0.0043\n",
            "Epoch [7/15], Batch [950/4502], Loss: 0.0055\n",
            "Epoch [7/15], Batch [1000/4502], Loss: 0.0094\n",
            "Epoch [7/15], Batch [1050/4502], Loss: 0.0021\n",
            "Epoch [7/15], Batch [1100/4502], Loss: 0.0133\n",
            "Epoch [7/15], Batch [1150/4502], Loss: 0.0025\n",
            "Epoch [7/15], Batch [1200/4502], Loss: 0.0054\n",
            "Epoch [7/15], Batch [1250/4502], Loss: 0.0028\n",
            "Epoch [7/15], Batch [1300/4502], Loss: 0.0044\n",
            "Epoch [7/15], Batch [1350/4502], Loss: 0.0010\n",
            "Epoch [7/15], Batch [1400/4502], Loss: 0.0090\n",
            "Epoch [7/15], Batch [1450/4502], Loss: 0.0024\n",
            "Epoch [7/15], Batch [1500/4502], Loss: 0.0219\n",
            "Epoch [7/15], Batch [1550/4502], Loss: 0.0016\n",
            "Epoch [7/15], Batch [1600/4502], Loss: 0.0174\n",
            "Epoch [7/15], Batch [1650/4502], Loss: 0.0042\n",
            "Epoch [7/15], Batch [1700/4502], Loss: 0.0021\n",
            "Epoch [7/15], Batch [1750/4502], Loss: 0.0034\n",
            "Epoch [7/15], Batch [1800/4502], Loss: 0.0008\n",
            "Epoch [7/15], Batch [1850/4502], Loss: 0.0102\n",
            "Epoch [7/15], Batch [1900/4502], Loss: 0.0069\n",
            "Epoch [7/15], Batch [1950/4502], Loss: 0.0095\n",
            "Epoch [7/15], Batch [2000/4502], Loss: 0.0134\n",
            "Epoch [7/15], Batch [2050/4502], Loss: 0.0054\n",
            "Epoch [7/15], Batch [2100/4502], Loss: 0.0231\n",
            "Epoch [7/15], Batch [2150/4502], Loss: 0.0093\n",
            "Epoch [7/15], Batch [2200/4502], Loss: 0.0009\n",
            "Epoch [7/15], Batch [2250/4502], Loss: 0.0017\n",
            "Epoch [7/15], Batch [2300/4502], Loss: 0.0092\n",
            "Epoch [7/15], Batch [2350/4502], Loss: 0.0008\n",
            "Epoch [7/15], Batch [2400/4502], Loss: 0.0070\n",
            "Epoch [7/15], Batch [2450/4502], Loss: 0.0043\n",
            "Epoch [7/15], Batch [2500/4502], Loss: 0.0156\n",
            "Epoch [7/15], Batch [2550/4502], Loss: 0.0139\n",
            "Epoch [7/15], Batch [2600/4502], Loss: 0.0058\n",
            "Epoch [7/15], Batch [2650/4502], Loss: 0.0103\n",
            "Epoch [7/15], Batch [2700/4502], Loss: 0.0025\n",
            "Epoch [7/15], Batch [2750/4502], Loss: 0.0119\n",
            "Epoch [7/15], Batch [2800/4502], Loss: 0.0019\n",
            "Epoch [7/15], Batch [2850/4502], Loss: 0.0048\n",
            "Epoch [7/15], Batch [2900/4502], Loss: 0.0180\n",
            "Epoch [7/15], Batch [2950/4502], Loss: 0.0068\n",
            "Epoch [7/15], Batch [3000/4502], Loss: 0.0035\n",
            "Epoch [7/15], Batch [3050/4502], Loss: 0.0070\n",
            "Epoch [7/15], Batch [3100/4502], Loss: 0.0110\n",
            "Epoch [7/15], Batch [3150/4502], Loss: 0.0004\n",
            "Epoch [7/15], Batch [3200/4502], Loss: 0.0104\n",
            "Epoch [7/15], Batch [3250/4502], Loss: 0.0012\n",
            "Epoch [7/15], Batch [3300/4502], Loss: 0.0046\n",
            "Epoch [7/15], Batch [3350/4502], Loss: 0.0021\n",
            "Epoch [7/15], Batch [3400/4502], Loss: 0.0164\n",
            "Epoch [7/15], Batch [3450/4502], Loss: 0.0005\n",
            "Epoch [7/15], Batch [3500/4502], Loss: 0.0253\n",
            "Epoch [7/15], Batch [3550/4502], Loss: 0.0160\n",
            "Epoch [7/15], Batch [3600/4502], Loss: 0.0007\n",
            "Epoch [7/15], Batch [3650/4502], Loss: 0.0019\n",
            "Epoch [7/15], Batch [3700/4502], Loss: 0.0309\n",
            "Epoch [7/15], Batch [3750/4502], Loss: 0.0042\n",
            "Epoch [7/15], Batch [3800/4502], Loss: 0.0201\n",
            "Epoch [7/15], Batch [3850/4502], Loss: 0.0005\n",
            "Epoch [7/15], Batch [3900/4502], Loss: 0.0007\n",
            "Epoch [7/15], Batch [3950/4502], Loss: 0.0081\n",
            "Epoch [7/15], Batch [4000/4502], Loss: 0.0017\n",
            "Epoch [7/15], Batch [4050/4502], Loss: 0.0048\n",
            "Epoch [7/15], Batch [4100/4502], Loss: 0.0087\n",
            "Epoch [7/15], Batch [4150/4502], Loss: 0.0174\n",
            "Epoch [7/15], Batch [4200/4502], Loss: 0.0126\n",
            "Epoch [7/15], Batch [4250/4502], Loss: 0.0048\n",
            "Epoch [7/15], Batch [4300/4502], Loss: 0.0004\n",
            "Epoch [7/15], Batch [4350/4502], Loss: 0.0048\n",
            "Epoch [7/15], Batch [4400/4502], Loss: 0.0010\n",
            "Epoch [7/15], Batch [4450/4502], Loss: 0.0014\n",
            "Epoch [7/15], Batch [4500/4502], Loss: 0.0043\n",
            "Epoch 7/15, Loss: 0.005992488061797705\n",
            "Updated Learning Rate: [3.414663171499479e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.99      1.00      1.00      3012\n",
            "  distortion       0.99      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       0.99      1.00      1.00      4518\n",
            "     flanger       0.99      0.99      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.98      0.99      0.98      7530\n",
            " hall_reverb       0.98      0.97      0.98      4518\n",
            "plate_reverb       0.99      0.96      0.98      3012\n",
            "     octaver       0.99      0.99      0.99      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     52757\n",
            "   macro avg       0.99      0.99      0.99     52757\n",
            "weighted avg       0.99      0.99      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0065, Accuracy: 0.9746, Precision: 0.9920, Recall: 0.9914, F1-score: 0.9917\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/15], Batch [50/4502], Loss: 0.0007\n",
            "Epoch [8/15], Batch [100/4502], Loss: 0.0044\n",
            "Epoch [8/15], Batch [150/4502], Loss: 0.0022\n",
            "Epoch [8/15], Batch [200/4502], Loss: 0.0131\n",
            "Epoch [8/15], Batch [250/4502], Loss: 0.0093\n",
            "Epoch [8/15], Batch [300/4502], Loss: 0.0024\n",
            "Epoch [8/15], Batch [350/4502], Loss: 0.0103\n",
            "Epoch [8/15], Batch [400/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [450/4502], Loss: 0.0040\n",
            "Epoch [8/15], Batch [500/4502], Loss: 0.0049\n",
            "Epoch [8/15], Batch [550/4502], Loss: 0.0002\n",
            "Epoch [8/15], Batch [600/4502], Loss: 0.0045\n",
            "Epoch [8/15], Batch [650/4502], Loss: 0.0094\n",
            "Epoch [8/15], Batch [700/4502], Loss: 0.0004\n",
            "Epoch [8/15], Batch [750/4502], Loss: 0.0017\n",
            "Epoch [8/15], Batch [800/4502], Loss: 0.0002\n",
            "Epoch [8/15], Batch [850/4502], Loss: 0.0119\n",
            "Epoch [8/15], Batch [900/4502], Loss: 0.0060\n",
            "Epoch [8/15], Batch [950/4502], Loss: 0.0056\n",
            "Epoch [8/15], Batch [1000/4502], Loss: 0.0050\n",
            "Epoch [8/15], Batch [1050/4502], Loss: 0.0114\n",
            "Epoch [8/15], Batch [1100/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [1150/4502], Loss: 0.0017\n",
            "Epoch [8/15], Batch [1200/4502], Loss: 0.0176\n",
            "Epoch [8/15], Batch [1250/4502], Loss: 0.0004\n",
            "Epoch [8/15], Batch [1300/4502], Loss: 0.0021\n",
            "Epoch [8/15], Batch [1350/4502], Loss: 0.0032\n",
            "Epoch [8/15], Batch [1400/4502], Loss: 0.0022\n",
            "Epoch [8/15], Batch [1450/4502], Loss: 0.0007\n",
            "Epoch [8/15], Batch [1500/4502], Loss: 0.0021\n",
            "Epoch [8/15], Batch [1550/4502], Loss: 0.0053\n",
            "Epoch [8/15], Batch [1600/4502], Loss: 0.0041\n",
            "Epoch [8/15], Batch [1650/4502], Loss: 0.0069\n",
            "Epoch [8/15], Batch [1700/4502], Loss: 0.0001\n",
            "Epoch [8/15], Batch [1750/4502], Loss: 0.0025\n",
            "Epoch [8/15], Batch [1800/4502], Loss: 0.0180\n",
            "Epoch [8/15], Batch [1850/4502], Loss: 0.0015\n",
            "Epoch [8/15], Batch [1900/4502], Loss: 0.0058\n",
            "Epoch [8/15], Batch [1950/4502], Loss: 0.0032\n",
            "Epoch [8/15], Batch [2000/4502], Loss: 0.0024\n",
            "Epoch [8/15], Batch [2050/4502], Loss: 0.0014\n",
            "Epoch [8/15], Batch [2100/4502], Loss: 0.0064\n",
            "Epoch [8/15], Batch [2150/4502], Loss: 0.0012\n",
            "Epoch [8/15], Batch [2200/4502], Loss: 0.0023\n",
            "Epoch [8/15], Batch [2250/4502], Loss: 0.0029\n",
            "Epoch [8/15], Batch [2300/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [2350/4502], Loss: 0.0013\n",
            "Epoch [8/15], Batch [2400/4502], Loss: 0.0009\n",
            "Epoch [8/15], Batch [2450/4502], Loss: 0.0010\n",
            "Epoch [8/15], Batch [2500/4502], Loss: 0.0008\n",
            "Epoch [8/15], Batch [2550/4502], Loss: 0.0014\n",
            "Epoch [8/15], Batch [2600/4502], Loss: 0.0021\n",
            "Epoch [8/15], Batch [2650/4502], Loss: 0.0027\n",
            "Epoch [8/15], Batch [2700/4502], Loss: 0.0033\n",
            "Epoch [8/15], Batch [2750/4502], Loss: 0.0005\n",
            "Epoch [8/15], Batch [2800/4502], Loss: 0.0120\n",
            "Epoch [8/15], Batch [2850/4502], Loss: 0.0128\n",
            "Epoch [8/15], Batch [2900/4502], Loss: 0.0008\n",
            "Epoch [8/15], Batch [2950/4502], Loss: 0.0161\n",
            "Epoch [8/15], Batch [3000/4502], Loss: 0.0123\n",
            "Epoch [8/15], Batch [3050/4502], Loss: 0.0053\n",
            "Epoch [8/15], Batch [3100/4502], Loss: 0.0005\n",
            "Epoch [8/15], Batch [3150/4502], Loss: 0.0004\n",
            "Epoch [8/15], Batch [3200/4502], Loss: 0.0021\n",
            "Epoch [8/15], Batch [3250/4502], Loss: 0.0004\n",
            "Epoch [8/15], Batch [3300/4502], Loss: 0.0026\n",
            "Epoch [8/15], Batch [3350/4502], Loss: 0.0137\n",
            "Epoch [8/15], Batch [3400/4502], Loss: 0.0022\n",
            "Epoch [8/15], Batch [3450/4502], Loss: 0.0024\n",
            "Epoch [8/15], Batch [3500/4502], Loss: 0.0026\n",
            "Epoch [8/15], Batch [3550/4502], Loss: 0.0150\n",
            "Epoch [8/15], Batch [3600/4502], Loss: 0.0186\n",
            "Epoch [8/15], Batch [3650/4502], Loss: 0.0010\n",
            "Epoch [8/15], Batch [3700/4502], Loss: 0.0018\n",
            "Epoch [8/15], Batch [3750/4502], Loss: 0.0069\n",
            "Epoch [8/15], Batch [3800/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [3850/4502], Loss: 0.0020\n",
            "Epoch [8/15], Batch [3900/4502], Loss: 0.0008\n",
            "Epoch [8/15], Batch [3950/4502], Loss: 0.0031\n",
            "Epoch [8/15], Batch [4000/4502], Loss: 0.0068\n",
            "Epoch [8/15], Batch [4050/4502], Loss: 0.0067\n",
            "Epoch [8/15], Batch [4100/4502], Loss: 0.0064\n",
            "Epoch [8/15], Batch [4150/4502], Loss: 0.0037\n",
            "Epoch [8/15], Batch [4200/4502], Loss: 0.0202\n",
            "Epoch [8/15], Batch [4250/4502], Loss: 0.0002\n",
            "Epoch [8/15], Batch [4300/4502], Loss: 0.0087\n",
            "Epoch [8/15], Batch [4350/4502], Loss: 0.0009\n",
            "Epoch [8/15], Batch [4400/4502], Loss: 0.0014\n",
            "Epoch [8/15], Batch [4450/4502], Loss: 0.0043\n",
            "Epoch [8/15], Batch [4500/4502], Loss: 0.0022\n",
            "Epoch 8/15, Loss: 0.0050887862558942195\n",
            "Updated Learning Rate: [2.9287566021951033e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.99      1.00      3012\n",
            "      chorus       0.97      1.00      0.98      5671\n",
            "       delay       0.98      0.99      0.99      7530\n",
            " hall_reverb       0.97      0.98      0.97      4518\n",
            "plate_reverb       0.97      0.98      0.98      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     52757\n",
            "   macro avg       0.99      0.99      0.99     52757\n",
            "weighted avg       0.99      0.99      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0068, Accuracy: 0.9728, Precision: 0.9899, Recall: 0.9941, F1-score: 0.9920\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/15], Batch [50/4502], Loss: 0.0013\n",
            "Epoch [9/15], Batch [100/4502], Loss: 0.0053\n",
            "Epoch [9/15], Batch [150/4502], Loss: 0.0027\n",
            "Epoch [9/15], Batch [200/4502], Loss: 0.0172\n",
            "Epoch [9/15], Batch [250/4502], Loss: 0.0188\n",
            "Epoch [9/15], Batch [300/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [350/4502], Loss: 0.0011\n",
            "Epoch [9/15], Batch [400/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [450/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [500/4502], Loss: 0.0097\n",
            "Epoch [9/15], Batch [550/4502], Loss: 0.0049\n",
            "Epoch [9/15], Batch [600/4502], Loss: 0.0037\n",
            "Epoch [9/15], Batch [650/4502], Loss: 0.0017\n",
            "Epoch [9/15], Batch [700/4502], Loss: 0.0050\n",
            "Epoch [9/15], Batch [750/4502], Loss: 0.0024\n",
            "Epoch [9/15], Batch [800/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [850/4502], Loss: 0.0071\n",
            "Epoch [9/15], Batch [900/4502], Loss: 0.0125\n",
            "Epoch [9/15], Batch [950/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [1000/4502], Loss: 0.0006\n",
            "Epoch [9/15], Batch [1050/4502], Loss: 0.0014\n",
            "Epoch [9/15], Batch [1100/4502], Loss: 0.0021\n",
            "Epoch [9/15], Batch [1150/4502], Loss: 0.0070\n",
            "Epoch [9/15], Batch [1200/4502], Loss: 0.0066\n",
            "Epoch [9/15], Batch [1250/4502], Loss: 0.0023\n",
            "Epoch [9/15], Batch [1300/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [1350/4502], Loss: 0.0023\n",
            "Epoch [9/15], Batch [1400/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [1450/4502], Loss: 0.0027\n",
            "Epoch [9/15], Batch [1500/4502], Loss: 0.0011\n",
            "Epoch [9/15], Batch [1550/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [1600/4502], Loss: 0.0120\n",
            "Epoch [9/15], Batch [1650/4502], Loss: 0.0037\n",
            "Epoch [9/15], Batch [1700/4502], Loss: 0.0068\n",
            "Epoch [9/15], Batch [1750/4502], Loss: 0.0045\n",
            "Epoch [9/15], Batch [1800/4502], Loss: 0.0071\n",
            "Epoch [9/15], Batch [1850/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [1900/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [1950/4502], Loss: 0.0038\n",
            "Epoch [9/15], Batch [2000/4502], Loss: 0.0036\n",
            "Epoch [9/15], Batch [2050/4502], Loss: 0.0025\n",
            "Epoch [9/15], Batch [2100/4502], Loss: 0.0174\n",
            "Epoch [9/15], Batch [2150/4502], Loss: 0.0137\n",
            "Epoch [9/15], Batch [2200/4502], Loss: 0.0069\n",
            "Epoch [9/15], Batch [2250/4502], Loss: 0.0015\n",
            "Epoch [9/15], Batch [2300/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [2350/4502], Loss: 0.0011\n",
            "Epoch [9/15], Batch [2400/4502], Loss: 0.0022\n",
            "Epoch [9/15], Batch [2450/4502], Loss: 0.0009\n",
            "Epoch [9/15], Batch [2500/4502], Loss: 0.0064\n",
            "Epoch [9/15], Batch [2550/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [2600/4502], Loss: 0.0035\n",
            "Epoch [9/15], Batch [2650/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [2700/4502], Loss: 0.0060\n",
            "Epoch [9/15], Batch [2750/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [2800/4502], Loss: 0.0198\n",
            "Epoch [9/15], Batch [2850/4502], Loss: 0.0013\n",
            "Epoch [9/15], Batch [2900/4502], Loss: 0.0029\n",
            "Epoch [9/15], Batch [2950/4502], Loss: 0.0020\n",
            "Epoch [9/15], Batch [3000/4502], Loss: 0.0010\n",
            "Epoch [9/15], Batch [3050/4502], Loss: 0.0007\n",
            "Epoch [9/15], Batch [3100/4502], Loss: 0.0177\n",
            "Epoch [9/15], Batch [3150/4502], Loss: 0.0008\n",
            "Epoch [9/15], Batch [3200/4502], Loss: 0.0011\n",
            "Epoch [9/15], Batch [3250/4502], Loss: 0.0029\n",
            "Epoch [9/15], Batch [3300/4502], Loss: 0.0014\n",
            "Epoch [9/15], Batch [3350/4502], Loss: 0.0172\n",
            "Epoch [9/15], Batch [3400/4502], Loss: 0.0024\n",
            "Epoch [9/15], Batch [3450/4502], Loss: 0.0023\n",
            "Epoch [9/15], Batch [3500/4502], Loss: 0.0005\n",
            "Epoch [9/15], Batch [3550/4502], Loss: 0.0004\n",
            "Epoch [9/15], Batch [3600/4502], Loss: 0.0006\n",
            "Epoch [9/15], Batch [3650/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [3700/4502], Loss: 0.0001\n",
            "Epoch [9/15], Batch [3750/4502], Loss: 0.0017\n",
            "Epoch [9/15], Batch [3800/4502], Loss: 0.0012\n",
            "Epoch [9/15], Batch [3850/4502], Loss: 0.0055\n",
            "Epoch [9/15], Batch [3900/4502], Loss: 0.0035\n",
            "Epoch [9/15], Batch [3950/4502], Loss: 0.0053\n",
            "Epoch [9/15], Batch [4000/4502], Loss: 0.0026\n",
            "Epoch [9/15], Batch [4050/4502], Loss: 0.0009\n",
            "Epoch [9/15], Batch [4100/4502], Loss: 0.0016\n",
            "Epoch [9/15], Batch [4150/4502], Loss: 0.0015\n",
            "Epoch [9/15], Batch [4200/4502], Loss: 0.0003\n",
            "Epoch [9/15], Batch [4250/4502], Loss: 0.0010\n",
            "Epoch [9/15], Batch [4300/4502], Loss: 0.0009\n",
            "Epoch [9/15], Batch [4350/4502], Loss: 0.0121\n",
            "Epoch [9/15], Batch [4400/4502], Loss: 0.0032\n",
            "Epoch [9/15], Batch [4450/4502], Loss: 0.0024\n",
            "Epoch [9/15], Batch [4500/4502], Loss: 0.0050\n",
            "Epoch 9/15, Loss: 0.004670429355018395\n",
            "Updated Learning Rate: [2.5119945377027402e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.99      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.99      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.99      0.95      0.97      7530\n",
            " hall_reverb       0.92      0.98      0.95      4518\n",
            "plate_reverb       0.84      1.00      0.91      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.98      0.99      0.98     52757\n",
            "   macro avg       0.98      0.99      0.98     52757\n",
            "weighted avg       0.98      0.99      0.99     52757\n",
            " samples avg       0.96      0.96      0.96     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0148, Accuracy: 0.9555, Precision: 0.9793, Recall: 0.9909, F1-score: 0.9844\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/15], Batch [50/4502], Loss: 0.0090\n",
            "Epoch [10/15], Batch [100/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [150/4502], Loss: 0.0094\n",
            "Epoch [10/15], Batch [200/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [250/4502], Loss: 0.0040\n",
            "Epoch [10/15], Batch [300/4502], Loss: 0.0015\n",
            "Epoch [10/15], Batch [350/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [400/4502], Loss: 0.0019\n",
            "Epoch [10/15], Batch [450/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [500/4502], Loss: 0.0053\n",
            "Epoch [10/15], Batch [550/4502], Loss: 0.0018\n",
            "Epoch [10/15], Batch [600/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [650/4502], Loss: 0.0015\n",
            "Epoch [10/15], Batch [700/4502], Loss: 0.0017\n",
            "Epoch [10/15], Batch [750/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [800/4502], Loss: 0.0038\n",
            "Epoch [10/15], Batch [850/4502], Loss: 0.0013\n",
            "Epoch [10/15], Batch [900/4502], Loss: 0.0012\n",
            "Epoch [10/15], Batch [950/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [1000/4502], Loss: 0.0194\n",
            "Epoch [10/15], Batch [1050/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [1100/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [1150/4502], Loss: 0.0177\n",
            "Epoch [10/15], Batch [1200/4502], Loss: 0.0016\n",
            "Epoch [10/15], Batch [1250/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [1300/4502], Loss: 0.0287\n",
            "Epoch [10/15], Batch [1350/4502], Loss: 0.0013\n",
            "Epoch [10/15], Batch [1400/4502], Loss: 0.0012\n",
            "Epoch [10/15], Batch [1450/4502], Loss: 0.0017\n",
            "Epoch [10/15], Batch [1500/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [1550/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [1600/4502], Loss: 0.0043\n",
            "Epoch [10/15], Batch [1650/4502], Loss: 0.0013\n",
            "Epoch [10/15], Batch [1700/4502], Loss: 0.0120\n",
            "Epoch [10/15], Batch [1750/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [1800/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [1850/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [1900/4502], Loss: 0.0235\n",
            "Epoch [10/15], Batch [1950/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [2000/4502], Loss: 0.0069\n",
            "Epoch [10/15], Batch [2050/4502], Loss: 0.0002\n",
            "Epoch [10/15], Batch [2100/4502], Loss: 0.0053\n",
            "Epoch [10/15], Batch [2150/4502], Loss: 0.0054\n",
            "Epoch [10/15], Batch [2200/4502], Loss: 0.0028\n",
            "Epoch [10/15], Batch [2250/4502], Loss: 0.0183\n",
            "Epoch [10/15], Batch [2300/4502], Loss: 0.0013\n",
            "Epoch [10/15], Batch [2350/4502], Loss: 0.0007\n",
            "Epoch [10/15], Batch [2400/4502], Loss: 0.0078\n",
            "Epoch [10/15], Batch [2450/4502], Loss: 0.0012\n",
            "Epoch [10/15], Batch [2500/4502], Loss: 0.0075\n",
            "Epoch [10/15], Batch [2550/4502], Loss: 0.0121\n",
            "Epoch [10/15], Batch [2600/4502], Loss: 0.0086\n",
            "Epoch [10/15], Batch [2650/4502], Loss: 0.0039\n",
            "Epoch [10/15], Batch [2700/4502], Loss: 0.0011\n",
            "Epoch [10/15], Batch [2750/4502], Loss: 0.0005\n",
            "Epoch [10/15], Batch [2800/4502], Loss: 0.0058\n",
            "Epoch [10/15], Batch [2850/4502], Loss: 0.0012\n",
            "Epoch [10/15], Batch [2900/4502], Loss: 0.0149\n",
            "Epoch [10/15], Batch [2950/4502], Loss: 0.0003\n",
            "Epoch [10/15], Batch [3000/4502], Loss: 0.0009\n",
            "Epoch [10/15], Batch [3050/4502], Loss: 0.0061\n",
            "Epoch [10/15], Batch [3100/4502], Loss: 0.0149\n",
            "Epoch [10/15], Batch [3150/4502], Loss: 0.0015\n",
            "Epoch [10/15], Batch [3200/4502], Loss: 0.0017\n",
            "Epoch [10/15], Batch [3250/4502], Loss: 0.0081\n",
            "Epoch [10/15], Batch [3300/4502], Loss: 0.0008\n",
            "Epoch [10/15], Batch [3350/4502], Loss: 0.0001\n",
            "Epoch [10/15], Batch [3400/4502], Loss: 0.0087\n",
            "Epoch [10/15], Batch [3450/4502], Loss: 0.0017\n",
            "Epoch [10/15], Batch [3500/4502], Loss: 0.0002\n",
            "Epoch [10/15], Batch [3550/4502], Loss: 0.0011\n",
            "Epoch [10/15], Batch [3600/4502], Loss: 0.0005\n",
            "Epoch [10/15], Batch [3650/4502], Loss: 0.0028\n",
            "Epoch [10/15], Batch [3700/4502], Loss: 0.0019\n",
            "Epoch [10/15], Batch [3750/4502], Loss: 0.0130\n",
            "Epoch [10/15], Batch [3800/4502], Loss: 0.0011\n",
            "Epoch [10/15], Batch [3850/4502], Loss: 0.0068\n",
            "Epoch [10/15], Batch [3900/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [3950/4502], Loss: 0.0013\n",
            "Epoch [10/15], Batch [4000/4502], Loss: 0.0009\n",
            "Epoch [10/15], Batch [4050/4502], Loss: 0.0041\n",
            "Epoch [10/15], Batch [4100/4502], Loss: 0.0010\n",
            "Epoch [10/15], Batch [4150/4502], Loss: 0.0021\n",
            "Epoch [10/15], Batch [4200/4502], Loss: 0.0062\n",
            "Epoch [10/15], Batch [4250/4502], Loss: 0.0221\n",
            "Epoch [10/15], Batch [4300/4502], Loss: 0.0006\n",
            "Epoch [10/15], Batch [4350/4502], Loss: 0.0018\n",
            "Epoch [10/15], Batch [4400/4502], Loss: 0.0011\n",
            "Epoch [10/15], Batch [4450/4502], Loss: 0.0004\n",
            "Epoch [10/15], Batch [4500/4502], Loss: 0.0011\n",
            "Epoch 10/15, Loss: 0.004134667450467218\n",
            "Updated Learning Rate: [2.1545377149876404e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      0.99      0.99      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.99      0.98      0.99      7530\n",
            " hall_reverb       0.97      0.98      0.98      4518\n",
            "plate_reverb       0.99      0.98      0.99      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       1.00      0.99      0.99     52757\n",
            "   macro avg       1.00      0.99      0.99     52757\n",
            "weighted avg       1.00      0.99      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0045, Accuracy: 0.9822, Precision: 0.9957, Recall: 0.9935, F1-score: 0.9946\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/15], Batch [50/4502], Loss: 0.0131\n",
            "Epoch [11/15], Batch [100/4502], Loss: 0.0030\n",
            "Epoch [11/15], Batch [150/4502], Loss: 0.0008\n",
            "Epoch [11/15], Batch [200/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [250/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [300/4502], Loss: 0.0114\n",
            "Epoch [11/15], Batch [350/4502], Loss: 0.0046\n",
            "Epoch [11/15], Batch [400/4502], Loss: 0.0095\n",
            "Epoch [11/15], Batch [450/4502], Loss: 0.0010\n",
            "Epoch [11/15], Batch [500/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [550/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [600/4502], Loss: 0.0135\n",
            "Epoch [11/15], Batch [650/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [700/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [750/4502], Loss: 0.0020\n",
            "Epoch [11/15], Batch [800/4502], Loss: 0.0054\n",
            "Epoch [11/15], Batch [850/4502], Loss: 0.0050\n",
            "Epoch [11/15], Batch [900/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [950/4502], Loss: 0.0036\n",
            "Epoch [11/15], Batch [1000/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [1050/4502], Loss: 0.0007\n",
            "Epoch [11/15], Batch [1100/4502], Loss: 0.0018\n",
            "Epoch [11/15], Batch [1150/4502], Loss: 0.0126\n",
            "Epoch [11/15], Batch [1200/4502], Loss: 0.0014\n",
            "Epoch [11/15], Batch [1250/4502], Loss: 0.0032\n",
            "Epoch [11/15], Batch [1300/4502], Loss: 0.0008\n",
            "Epoch [11/15], Batch [1350/4502], Loss: 0.0110\n",
            "Epoch [11/15], Batch [1400/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [1450/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [1500/4502], Loss: 0.0049\n",
            "Epoch [11/15], Batch [1550/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [1600/4502], Loss: 0.0016\n",
            "Epoch [11/15], Batch [1650/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [1700/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [1750/4502], Loss: 0.0007\n",
            "Epoch [11/15], Batch [1800/4502], Loss: 0.0115\n",
            "Epoch [11/15], Batch [1850/4502], Loss: 0.0016\n",
            "Epoch [11/15], Batch [1900/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [1950/4502], Loss: 0.0077\n",
            "Epoch [11/15], Batch [2000/4502], Loss: 0.0007\n",
            "Epoch [11/15], Batch [2050/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [2100/4502], Loss: 0.0026\n",
            "Epoch [11/15], Batch [2150/4502], Loss: 0.0026\n",
            "Epoch [11/15], Batch [2200/4502], Loss: 0.0027\n",
            "Epoch [11/15], Batch [2250/4502], Loss: 0.0012\n",
            "Epoch [11/15], Batch [2300/4502], Loss: 0.0017\n",
            "Epoch [11/15], Batch [2350/4502], Loss: 0.0014\n",
            "Epoch [11/15], Batch [2400/4502], Loss: 0.0192\n",
            "Epoch [11/15], Batch [2450/4502], Loss: 0.0069\n",
            "Epoch [11/15], Batch [2500/4502], Loss: 0.0008\n",
            "Epoch [11/15], Batch [2550/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [2600/4502], Loss: 0.0017\n",
            "Epoch [11/15], Batch [2650/4502], Loss: 0.0038\n",
            "Epoch [11/15], Batch [2700/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [2750/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [2800/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [2850/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [2900/4502], Loss: 0.0055\n",
            "Epoch [11/15], Batch [2950/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [3000/4502], Loss: 0.0010\n",
            "Epoch [11/15], Batch [3050/4502], Loss: 0.0007\n",
            "Epoch [11/15], Batch [3100/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [3150/4502], Loss: 0.0034\n",
            "Epoch [11/15], Batch [3200/4502], Loss: 0.0025\n",
            "Epoch [11/15], Batch [3250/4502], Loss: 0.0001\n",
            "Epoch [11/15], Batch [3300/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [3350/4502], Loss: 0.0060\n",
            "Epoch [11/15], Batch [3400/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [3450/4502], Loss: 0.0121\n",
            "Epoch [11/15], Batch [3500/4502], Loss: 0.0100\n",
            "Epoch [11/15], Batch [3550/4502], Loss: 0.0006\n",
            "Epoch [11/15], Batch [3600/4502], Loss: 0.0045\n",
            "Epoch [11/15], Batch [3650/4502], Loss: 0.0038\n",
            "Epoch [11/15], Batch [3700/4502], Loss: 0.0004\n",
            "Epoch [11/15], Batch [3750/4502], Loss: 0.0040\n",
            "Epoch [11/15], Batch [3800/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [3850/4502], Loss: 0.0117\n",
            "Epoch [11/15], Batch [3900/4502], Loss: 0.0010\n",
            "Epoch [11/15], Batch [3950/4502], Loss: 0.0005\n",
            "Epoch [11/15], Batch [4000/4502], Loss: 0.0037\n",
            "Epoch [11/15], Batch [4050/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [4100/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [4150/4502], Loss: 0.0003\n",
            "Epoch [11/15], Batch [4200/4502], Loss: 0.0002\n",
            "Epoch [11/15], Batch [4250/4502], Loss: 0.0011\n",
            "Epoch [11/15], Batch [4300/4502], Loss: 0.0014\n",
            "Epoch [11/15], Batch [4350/4502], Loss: 0.0056\n",
            "Epoch [11/15], Batch [4400/4502], Loss: 0.0030\n",
            "Epoch [11/15], Batch [4450/4502], Loss: 0.0027\n",
            "Epoch [11/15], Batch [4500/4502], Loss: 0.0022\n",
            "Epoch 11/15, Loss: 0.0036942410184231497\n",
            "Updated Learning Rate: [1.8479469981448992e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       0.99      1.00      1.00      4518\n",
            "     flanger       1.00      0.99      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       1.00      0.98      0.99      7530\n",
            " hall_reverb       0.96      0.99      0.97      4518\n",
            "plate_reverb       0.98      1.00      0.99      3012\n",
            "     octaver       0.98      1.00      0.99      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     52757\n",
            "   macro avg       0.99      0.99      0.99     52757\n",
            "weighted avg       0.99      0.99      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0056, Accuracy: 0.9792, Precision: 0.9921, Recall: 0.9946, F1-score: 0.9933\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/15], Batch [50/4502], Loss: 0.0049\n",
            "Epoch [12/15], Batch [100/4502], Loss: 0.0051\n",
            "Epoch [12/15], Batch [150/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [200/4502], Loss: 0.0008\n",
            "Epoch [12/15], Batch [250/4502], Loss: 0.0006\n",
            "Epoch [12/15], Batch [300/4502], Loss: 0.0047\n",
            "Epoch [12/15], Batch [350/4502], Loss: 0.0027\n",
            "Epoch [12/15], Batch [400/4502], Loss: 0.0146\n",
            "Epoch [12/15], Batch [450/4502], Loss: 0.0011\n",
            "Epoch [12/15], Batch [500/4502], Loss: 0.0006\n",
            "Epoch [12/15], Batch [550/4502], Loss: 0.0012\n",
            "Epoch [12/15], Batch [600/4502], Loss: 0.0017\n",
            "Epoch [12/15], Batch [650/4502], Loss: 0.0110\n",
            "Epoch [12/15], Batch [700/4502], Loss: 0.0006\n",
            "Epoch [12/15], Batch [750/4502], Loss: 0.0015\n",
            "Epoch [12/15], Batch [800/4502], Loss: 0.0045\n",
            "Epoch [12/15], Batch [850/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [900/4502], Loss: 0.0006\n",
            "Epoch [12/15], Batch [950/4502], Loss: 0.0039\n",
            "Epoch [12/15], Batch [1000/4502], Loss: 0.0029\n",
            "Epoch [12/15], Batch [1050/4502], Loss: 0.0014\n",
            "Epoch [12/15], Batch [1100/4502], Loss: 0.0063\n",
            "Epoch [12/15], Batch [1150/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [1200/4502], Loss: 0.0017\n",
            "Epoch [12/15], Batch [1250/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [1300/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [1350/4502], Loss: 0.0019\n",
            "Epoch [12/15], Batch [1400/4502], Loss: 0.0026\n",
            "Epoch [12/15], Batch [1450/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [1500/4502], Loss: 0.0043\n",
            "Epoch [12/15], Batch [1550/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [1600/4502], Loss: 0.0008\n",
            "Epoch [12/15], Batch [1650/4502], Loss: 0.0014\n",
            "Epoch [12/15], Batch [1700/4502], Loss: 0.0032\n",
            "Epoch [12/15], Batch [1750/4502], Loss: 0.0206\n",
            "Epoch [12/15], Batch [1800/4502], Loss: 0.0220\n",
            "Epoch [12/15], Batch [1850/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [1900/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [1950/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [2000/4502], Loss: 0.0175\n",
            "Epoch [12/15], Batch [2050/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [2100/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [2150/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [2200/4502], Loss: 0.0002\n",
            "Epoch [12/15], Batch [2250/4502], Loss: 0.0010\n",
            "Epoch [12/15], Batch [2300/4502], Loss: 0.0108\n",
            "Epoch [12/15], Batch [2350/4502], Loss: 0.0023\n",
            "Epoch [12/15], Batch [2400/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [2450/4502], Loss: 0.0025\n",
            "Epoch [12/15], Batch [2500/4502], Loss: 0.0107\n",
            "Epoch [12/15], Batch [2550/4502], Loss: 0.0030\n",
            "Epoch [12/15], Batch [2600/4502], Loss: 0.0046\n",
            "Epoch [12/15], Batch [2650/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [2700/4502], Loss: 0.0046\n",
            "Epoch [12/15], Batch [2750/4502], Loss: 0.0154\n",
            "Epoch [12/15], Batch [2800/4502], Loss: 0.0020\n",
            "Epoch [12/15], Batch [2850/4502], Loss: 0.0013\n",
            "Epoch [12/15], Batch [2900/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [2950/4502], Loss: 0.0119\n",
            "Epoch [12/15], Batch [3000/4502], Loss: 0.0047\n",
            "Epoch [12/15], Batch [3050/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [3100/4502], Loss: 0.0023\n",
            "Epoch [12/15], Batch [3150/4502], Loss: 0.0046\n",
            "Epoch [12/15], Batch [3200/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [3250/4502], Loss: 0.0004\n",
            "Epoch [12/15], Batch [3300/4502], Loss: 0.0003\n",
            "Epoch [12/15], Batch [3350/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [3400/4502], Loss: 0.0056\n",
            "Epoch [12/15], Batch [3450/4502], Loss: 0.0036\n",
            "Epoch [12/15], Batch [3500/4502], Loss: 0.0231\n",
            "Epoch [12/15], Batch [3550/4502], Loss: 0.0000\n",
            "Epoch [12/15], Batch [3600/4502], Loss: 0.0104\n",
            "Epoch [12/15], Batch [3650/4502], Loss: 0.0046\n",
            "Epoch [12/15], Batch [3700/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [3750/4502], Loss: 0.0018\n",
            "Epoch [12/15], Batch [3800/4502], Loss: 0.0019\n",
            "Epoch [12/15], Batch [3850/4502], Loss: 0.0017\n",
            "Epoch [12/15], Batch [3900/4502], Loss: 0.0007\n",
            "Epoch [12/15], Batch [3950/4502], Loss: 0.0020\n",
            "Epoch [12/15], Batch [4000/4502], Loss: 0.0001\n",
            "Epoch [12/15], Batch [4050/4502], Loss: 0.0009\n",
            "Epoch [12/15], Batch [4100/4502], Loss: 0.0043\n",
            "Epoch [12/15], Batch [4150/4502], Loss: 0.0008\n",
            "Epoch [12/15], Batch [4200/4502], Loss: 0.0037\n",
            "Epoch [12/15], Batch [4250/4502], Loss: 0.0016\n",
            "Epoch [12/15], Batch [4300/4502], Loss: 0.0009\n",
            "Epoch [12/15], Batch [4350/4502], Loss: 0.0005\n",
            "Epoch [12/15], Batch [4400/4502], Loss: 0.0159\n",
            "Epoch [12/15], Batch [4450/4502], Loss: 0.0040\n",
            "Epoch [12/15], Batch [4500/4502], Loss: 0.0016\n",
            "Epoch 12/15, Loss: 0.004105381384204262\n",
            "Updated Learning Rate: [1.58498414030888e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.99      0.99      0.99      7530\n",
            " hall_reverb       0.97      0.99      0.98      4518\n",
            "plate_reverb       0.99      0.99      0.99      3012\n",
            "     octaver       0.96      1.00      0.98      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      1.00      0.99     52757\n",
            "   macro avg       0.99      1.00      0.99     52757\n",
            "weighted avg       0.99      1.00      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0053, Accuracy: 0.9804, Precision: 0.9912, Recall: 0.9956, F1-score: 0.9934\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/15], Batch [50/4502], Loss: 0.0155\n",
            "Epoch [13/15], Batch [100/4502], Loss: 0.0020\n",
            "Epoch [13/15], Batch [150/4502], Loss: 0.0052\n",
            "Epoch [13/15], Batch [200/4502], Loss: 0.0036\n",
            "Epoch [13/15], Batch [250/4502], Loss: 0.0031\n",
            "Epoch [13/15], Batch [300/4502], Loss: 0.0055\n",
            "Epoch [13/15], Batch [350/4502], Loss: 0.0055\n",
            "Epoch [13/15], Batch [400/4502], Loss: 0.0015\n",
            "Epoch [13/15], Batch [450/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [500/4502], Loss: 0.0035\n",
            "Epoch [13/15], Batch [550/4502], Loss: 0.0012\n",
            "Epoch [13/15], Batch [600/4502], Loss: 0.0048\n",
            "Epoch [13/15], Batch [650/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [700/4502], Loss: 0.0073\n",
            "Epoch [13/15], Batch [750/4502], Loss: 0.0121\n",
            "Epoch [13/15], Batch [800/4502], Loss: 0.0095\n",
            "Epoch [13/15], Batch [850/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [900/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [950/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1000/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [1050/4502], Loss: 0.0081\n",
            "Epoch [13/15], Batch [1100/4502], Loss: 0.0017\n",
            "Epoch [13/15], Batch [1150/4502], Loss: 0.0009\n",
            "Epoch [13/15], Batch [1200/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1250/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1300/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [1350/4502], Loss: 0.0013\n",
            "Epoch [13/15], Batch [1400/4502], Loss: 0.0018\n",
            "Epoch [13/15], Batch [1450/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [1500/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [1550/4502], Loss: 0.0081\n",
            "Epoch [13/15], Batch [1600/4502], Loss: 0.0035\n",
            "Epoch [13/15], Batch [1650/4502], Loss: 0.0030\n",
            "Epoch [13/15], Batch [1700/4502], Loss: 0.0137\n",
            "Epoch [13/15], Batch [1750/4502], Loss: 0.0016\n",
            "Epoch [13/15], Batch [1800/4502], Loss: 0.0021\n",
            "Epoch [13/15], Batch [1850/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [1900/4502], Loss: 0.0101\n",
            "Epoch [13/15], Batch [1950/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [2000/4502], Loss: 0.0143\n",
            "Epoch [13/15], Batch [2050/4502], Loss: 0.0043\n",
            "Epoch [13/15], Batch [2100/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [2150/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [2200/4502], Loss: 0.0010\n",
            "Epoch [13/15], Batch [2250/4502], Loss: 0.0012\n",
            "Epoch [13/15], Batch [2300/4502], Loss: 0.0008\n",
            "Epoch [13/15], Batch [2350/4502], Loss: 0.0006\n",
            "Epoch [13/15], Batch [2400/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [2450/4502], Loss: 0.0068\n",
            "Epoch [13/15], Batch [2500/4502], Loss: 0.0168\n",
            "Epoch [13/15], Batch [2550/4502], Loss: 0.0049\n",
            "Epoch [13/15], Batch [2600/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [2650/4502], Loss: 0.0036\n",
            "Epoch [13/15], Batch [2700/4502], Loss: 0.0015\n",
            "Epoch [13/15], Batch [2750/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [2800/4502], Loss: 0.0046\n",
            "Epoch [13/15], Batch [2850/4502], Loss: 0.0036\n",
            "Epoch [13/15], Batch [2900/4502], Loss: 0.0015\n",
            "Epoch [13/15], Batch [2950/4502], Loss: 0.0011\n",
            "Epoch [13/15], Batch [3000/4502], Loss: 0.0118\n",
            "Epoch [13/15], Batch [3050/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [3100/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [3150/4502], Loss: 0.0008\n",
            "Epoch [13/15], Batch [3200/4502], Loss: 0.0014\n",
            "Epoch [13/15], Batch [3250/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [3300/4502], Loss: 0.0166\n",
            "Epoch [13/15], Batch [3350/4502], Loss: 0.0025\n",
            "Epoch [13/15], Batch [3400/4502], Loss: 0.0027\n",
            "Epoch [13/15], Batch [3450/4502], Loss: 0.0008\n",
            "Epoch [13/15], Batch [3500/4502], Loss: 0.0049\n",
            "Epoch [13/15], Batch [3550/4502], Loss: 0.0046\n",
            "Epoch [13/15], Batch [3600/4502], Loss: 0.0015\n",
            "Epoch [13/15], Batch [3650/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [3700/4502], Loss: 0.0007\n",
            "Epoch [13/15], Batch [3750/4502], Loss: 0.0003\n",
            "Epoch [13/15], Batch [3800/4502], Loss: 0.0001\n",
            "Epoch [13/15], Batch [3850/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [3900/4502], Loss: 0.0021\n",
            "Epoch [13/15], Batch [3950/4502], Loss: 0.0032\n",
            "Epoch [13/15], Batch [4000/4502], Loss: 0.0005\n",
            "Epoch [13/15], Batch [4050/4502], Loss: 0.0017\n",
            "Epoch [13/15], Batch [4100/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [4150/4502], Loss: 0.0076\n",
            "Epoch [13/15], Batch [4200/4502], Loss: 0.0002\n",
            "Epoch [13/15], Batch [4250/4502], Loss: 0.0090\n",
            "Epoch [13/15], Batch [4300/4502], Loss: 0.0017\n",
            "Epoch [13/15], Batch [4350/4502], Loss: 0.0144\n",
            "Epoch [13/15], Batch [4400/4502], Loss: 0.0158\n",
            "Epoch [13/15], Batch [4450/4502], Loss: 0.0009\n",
            "Epoch [13/15], Batch [4500/4502], Loss: 0.0000\n",
            "Epoch 13/15, Loss: 0.0032467591325991813\n",
            "Updated Learning Rate: [1.3594408971429265e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.98      0.99      0.99      7530\n",
            " hall_reverb       0.97      0.98      0.98      4518\n",
            "plate_reverb       0.96      0.99      0.98      3012\n",
            "     octaver       0.99      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      1.00      0.99     52757\n",
            "   macro avg       0.99      1.00      0.99     52757\n",
            "weighted avg       0.99      1.00      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0051, Accuracy: 0.9801, Precision: 0.9915, Recall: 0.9959, F1-score: 0.9937\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/15], Batch [50/4502], Loss: 0.0039\n",
            "Epoch [14/15], Batch [100/4502], Loss: 0.0029\n",
            "Epoch [14/15], Batch [150/4502], Loss: 0.0038\n",
            "Epoch [14/15], Batch [200/4502], Loss: 0.0154\n",
            "Epoch [14/15], Batch [250/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [300/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [350/4502], Loss: 0.0010\n",
            "Epoch [14/15], Batch [400/4502], Loss: 0.0154\n",
            "Epoch [14/15], Batch [450/4502], Loss: 0.0014\n",
            "Epoch [14/15], Batch [500/4502], Loss: 0.0101\n",
            "Epoch [14/15], Batch [550/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [600/4502], Loss: 0.0028\n",
            "Epoch [14/15], Batch [650/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [700/4502], Loss: 0.0108\n",
            "Epoch [14/15], Batch [750/4502], Loss: 0.0015\n",
            "Epoch [14/15], Batch [800/4502], Loss: 0.0221\n",
            "Epoch [14/15], Batch [850/4502], Loss: 0.0126\n",
            "Epoch [14/15], Batch [900/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [950/4502], Loss: 0.0008\n",
            "Epoch [14/15], Batch [1000/4502], Loss: 0.0043\n",
            "Epoch [14/15], Batch [1050/4502], Loss: 0.0033\n",
            "Epoch [14/15], Batch [1100/4502], Loss: 0.0009\n",
            "Epoch [14/15], Batch [1150/4502], Loss: 0.0121\n",
            "Epoch [14/15], Batch [1200/4502], Loss: 0.0016\n",
            "Epoch [14/15], Batch [1250/4502], Loss: 0.0212\n",
            "Epoch [14/15], Batch [1300/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1350/4502], Loss: 0.0043\n",
            "Epoch [14/15], Batch [1400/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [1450/4502], Loss: 0.0026\n",
            "Epoch [14/15], Batch [1500/4502], Loss: 0.0008\n",
            "Epoch [14/15], Batch [1550/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [1600/4502], Loss: 0.0032\n",
            "Epoch [14/15], Batch [1650/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [1700/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [1750/4502], Loss: 0.0068\n",
            "Epoch [14/15], Batch [1800/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [1850/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [1900/4502], Loss: 0.0007\n",
            "Epoch [14/15], Batch [1950/4502], Loss: 0.0035\n",
            "Epoch [14/15], Batch [2000/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [2050/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [2100/4502], Loss: 0.0020\n",
            "Epoch [14/15], Batch [2150/4502], Loss: 0.0047\n",
            "Epoch [14/15], Batch [2200/4502], Loss: 0.0000\n",
            "Epoch [14/15], Batch [2250/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [2300/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [2350/4502], Loss: 0.0037\n",
            "Epoch [14/15], Batch [2400/4502], Loss: 0.0006\n",
            "Epoch [14/15], Batch [2450/4502], Loss: 0.0080\n",
            "Epoch [14/15], Batch [2500/4502], Loss: 0.0096\n",
            "Epoch [14/15], Batch [2550/4502], Loss: 0.0023\n",
            "Epoch [14/15], Batch [2600/4502], Loss: 0.0010\n",
            "Epoch [14/15], Batch [2650/4502], Loss: 0.0027\n",
            "Epoch [14/15], Batch [2700/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2750/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [2800/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [2850/4502], Loss: 0.0055\n",
            "Epoch [14/15], Batch [2900/4502], Loss: 0.0030\n",
            "Epoch [14/15], Batch [2950/4502], Loss: 0.0166\n",
            "Epoch [14/15], Batch [3000/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [3050/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [3100/4502], Loss: 0.0025\n",
            "Epoch [14/15], Batch [3150/4502], Loss: 0.0007\n",
            "Epoch [14/15], Batch [3200/4502], Loss: 0.0004\n",
            "Epoch [14/15], Batch [3250/4502], Loss: 0.0008\n",
            "Epoch [14/15], Batch [3300/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [3350/4502], Loss: 0.0051\n",
            "Epoch [14/15], Batch [3400/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [3450/4502], Loss: 0.0001\n",
            "Epoch [14/15], Batch [3500/4502], Loss: 0.0011\n",
            "Epoch [14/15], Batch [3550/4502], Loss: 0.0024\n",
            "Epoch [14/15], Batch [3600/4502], Loss: 0.0008\n",
            "Epoch [14/15], Batch [3650/4502], Loss: 0.0008\n",
            "Epoch [14/15], Batch [3700/4502], Loss: 0.0027\n",
            "Epoch [14/15], Batch [3750/4502], Loss: 0.0006\n",
            "Epoch [14/15], Batch [3800/4502], Loss: 0.0020\n",
            "Epoch [14/15], Batch [3850/4502], Loss: 0.0237\n",
            "Epoch [14/15], Batch [3900/4502], Loss: 0.0066\n",
            "Epoch [14/15], Batch [3950/4502], Loss: 0.0013\n",
            "Epoch [14/15], Batch [4000/4502], Loss: 0.0043\n",
            "Epoch [14/15], Batch [4050/4502], Loss: 0.0037\n",
            "Epoch [14/15], Batch [4100/4502], Loss: 0.0008\n",
            "Epoch [14/15], Batch [4150/4502], Loss: 0.0006\n",
            "Epoch [14/15], Batch [4200/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [4250/4502], Loss: 0.0002\n",
            "Epoch [14/15], Batch [4300/4502], Loss: 0.0000\n",
            "Epoch [14/15], Batch [4350/4502], Loss: 0.0007\n",
            "Epoch [14/15], Batch [4400/4502], Loss: 0.0005\n",
            "Epoch [14/15], Batch [4450/4502], Loss: 0.0003\n",
            "Epoch [14/15], Batch [4500/4502], Loss: 0.0168\n",
            "Epoch 14/15, Loss: 0.0031217374454060873\n",
            "Updated Learning Rate: [1.165992457479488e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       0.99      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.99      0.98      0.99      7530\n",
            " hall_reverb       0.98      0.98      0.98      4518\n",
            "plate_reverb       1.00      0.98      0.99      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       1.00      0.99      0.99     52757\n",
            "   macro avg       1.00      0.99      0.99     52757\n",
            "weighted avg       1.00      0.99      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0043, Accuracy: 0.9836, Precision: 0.9961, Recall: 0.9936, F1-score: 0.9948\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/15], Batch [50/4502], Loss: 0.0028\n",
            "Epoch [15/15], Batch [100/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [150/4502], Loss: 0.0010\n",
            "Epoch [15/15], Batch [200/4502], Loss: 0.0067\n",
            "Epoch [15/15], Batch [250/4502], Loss: 0.0029\n",
            "Epoch [15/15], Batch [300/4502], Loss: 0.0050\n",
            "Epoch [15/15], Batch [350/4502], Loss: 0.0007\n",
            "Epoch [15/15], Batch [400/4502], Loss: 0.0008\n",
            "Epoch [15/15], Batch [450/4502], Loss: 0.0015\n",
            "Epoch [15/15], Batch [500/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [550/4502], Loss: 0.0042\n",
            "Epoch [15/15], Batch [600/4502], Loss: 0.0101\n",
            "Epoch [15/15], Batch [650/4502], Loss: 0.0036\n",
            "Epoch [15/15], Batch [700/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [750/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [800/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [850/4502], Loss: 0.0295\n",
            "Epoch [15/15], Batch [900/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [950/4502], Loss: 0.0301\n",
            "Epoch [15/15], Batch [1000/4502], Loss: 0.0024\n",
            "Epoch [15/15], Batch [1050/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [1100/4502], Loss: 0.0216\n",
            "Epoch [15/15], Batch [1150/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [1200/4502], Loss: 0.0015\n",
            "Epoch [15/15], Batch [1250/4502], Loss: 0.0013\n",
            "Epoch [15/15], Batch [1300/4502], Loss: 0.0156\n",
            "Epoch [15/15], Batch [1350/4502], Loss: 0.0020\n",
            "Epoch [15/15], Batch [1400/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1450/4502], Loss: 0.0017\n",
            "Epoch [15/15], Batch [1500/4502], Loss: 0.0014\n",
            "Epoch [15/15], Batch [1550/4502], Loss: 0.0007\n",
            "Epoch [15/15], Batch [1600/4502], Loss: 0.0096\n",
            "Epoch [15/15], Batch [1650/4502], Loss: 0.0010\n",
            "Epoch [15/15], Batch [1700/4502], Loss: 0.0147\n",
            "Epoch [15/15], Batch [1750/4502], Loss: 0.0008\n",
            "Epoch [15/15], Batch [1800/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [1850/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [1900/4502], Loss: 0.0011\n",
            "Epoch [15/15], Batch [1950/4502], Loss: 0.0054\n",
            "Epoch [15/15], Batch [2000/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2050/4502], Loss: 0.0015\n",
            "Epoch [15/15], Batch [2100/4502], Loss: 0.0124\n",
            "Epoch [15/15], Batch [2150/4502], Loss: 0.0116\n",
            "Epoch [15/15], Batch [2200/4502], Loss: 0.0023\n",
            "Epoch [15/15], Batch [2250/4502], Loss: 0.0178\n",
            "Epoch [15/15], Batch [2300/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [2350/4502], Loss: 0.0020\n",
            "Epoch [15/15], Batch [2400/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2450/4502], Loss: 0.0017\n",
            "Epoch [15/15], Batch [2500/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2550/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [2600/4502], Loss: 0.0056\n",
            "Epoch [15/15], Batch [2650/4502], Loss: 0.0012\n",
            "Epoch [15/15], Batch [2700/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [2750/4502], Loss: 0.0044\n",
            "Epoch [15/15], Batch [2800/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2850/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [2900/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [2950/4502], Loss: 0.0020\n",
            "Epoch [15/15], Batch [3000/4502], Loss: 0.0009\n",
            "Epoch [15/15], Batch [3050/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [3100/4502], Loss: 0.0006\n",
            "Epoch [15/15], Batch [3150/4502], Loss: 0.0040\n",
            "Epoch [15/15], Batch [3200/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [3250/4502], Loss: 0.0022\n",
            "Epoch [15/15], Batch [3300/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [3350/4502], Loss: 0.0015\n",
            "Epoch [15/15], Batch [3400/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [3450/4502], Loss: 0.0003\n",
            "Epoch [15/15], Batch [3500/4502], Loss: 0.0050\n",
            "Epoch [15/15], Batch [3550/4502], Loss: 0.0020\n",
            "Epoch [15/15], Batch [3600/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [3650/4502], Loss: 0.0018\n",
            "Epoch [15/15], Batch [3700/4502], Loss: 0.0002\n",
            "Epoch [15/15], Batch [3750/4502], Loss: 0.0000\n",
            "Epoch [15/15], Batch [3800/4502], Loss: 0.0013\n",
            "Epoch [15/15], Batch [3850/4502], Loss: 0.0005\n",
            "Epoch [15/15], Batch [3900/4502], Loss: 0.0162\n",
            "Epoch [15/15], Batch [3950/4502], Loss: 0.0017\n",
            "Epoch [15/15], Batch [4000/4502], Loss: 0.0080\n",
            "Epoch [15/15], Batch [4050/4502], Loss: 0.0001\n",
            "Epoch [15/15], Batch [4100/4502], Loss: 0.0028\n",
            "Epoch [15/15], Batch [4150/4502], Loss: 0.0031\n",
            "Epoch [15/15], Batch [4200/4502], Loss: 0.0010\n",
            "Epoch [15/15], Batch [4250/4502], Loss: 0.0023\n",
            "Epoch [15/15], Batch [4300/4502], Loss: 0.0101\n",
            "Epoch [15/15], Batch [4350/4502], Loss: 0.0004\n",
            "Epoch [15/15], Batch [4400/4502], Loss: 0.0037\n",
            "Epoch [15/15], Batch [4450/4502], Loss: 0.0030\n",
            "Epoch [15/15], Batch [4500/4502], Loss: 0.0001\n",
            "Epoch 15/15, Loss: 0.002970080424513402\n",
            "Updated Learning Rate: [1.0000717307801568e-05]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      1.00      1.00      3012\n",
            "  distortion       1.00      1.00      1.00      4518\n",
            "        fuzz       1.00      1.00      1.00      5271\n",
            "     tremolo       1.00      1.00      1.00      5671\n",
            "      phaser       1.00      1.00      1.00      4518\n",
            "     flanger       1.00      1.00      1.00      3012\n",
            "      chorus       1.00      1.00      1.00      5671\n",
            "       delay       0.97      1.00      0.98      7530\n",
            " hall_reverb       0.97      0.99      0.98      4518\n",
            "plate_reverb       0.98      0.99      0.99      3012\n",
            "     octaver       1.00      1.00      1.00      2259\n",
            " auto_filter       1.00      1.00      1.00      3765\n",
            "\n",
            "   micro avg       0.99      1.00      0.99     52757\n",
            "   macro avg       0.99      1.00      0.99     52757\n",
            "weighted avg       0.99      1.00      0.99     52757\n",
            " samples avg       0.97      0.97      0.97     52757\n",
            "\n",
            "\n",
            "Validation Loss: 0.0050, Accuracy: 0.9806, Precision: 0.9923, Recall: 0.9968, F1-score: 0.9945\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=6, pin_memory=True)\n",
        "\n",
        "num_classes = len(train_dataset.label_map)\n",
        "\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8577)  # 0.0001 → 0.00001 over 15 epochs\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "print_freq = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    print(f\"Updated Learning Rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrograms, labels in val_loader:\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            outputs = model(spectrograms)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
        "\n",
        "            # Store for metric computation\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Convert lists to numpy arrays for metric calculations\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Print classification report\n",
        "    class_names = train_dataset.label_map\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(f\"\\nValidation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGiHpYYikrqv",
        "outputId": "2c127eca-499c-453c-bbb2-5e8225cfd3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c3455c24d392>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n",
            "\n",
            "Test Loss: 0.0050, Accuracy: 0.9835, Precision: 0.9936, Recall: 0.9932, F1-score: 0.9934\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       1.00      0.99      1.00      3028\n",
            "  distortion       1.00      0.99      1.00      4544\n",
            "        fuzz       1.00      1.00      1.00      5300\n",
            "     tremolo       1.00      1.00      1.00      4542\n",
            "      phaser       1.00      1.00      1.00      4542\n",
            "     flanger       1.00      0.99      0.99      3028\n",
            "      chorus       1.00      1.00      1.00      5300\n",
            "       delay       0.99      1.00      0.99      8328\n",
            " hall_reverb       0.97      0.98      0.98      3788\n",
            "plate_reverb       0.98      0.99      0.99      3028\n",
            "     octaver       0.99      0.99      0.99      2271\n",
            " auto_filter       1.00      0.99      1.00      3785\n",
            "\n",
            "   micro avg       0.99      0.99      0.99     51484\n",
            "   macro avg       0.99      0.99      0.99     51484\n",
            "weighted avg       0.99      0.99      0.99     51484\n",
            " samples avg       0.97      0.97      0.97     51484\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "h5_test_path = \"/content/final_datasets/test_extra_TRM_DLY.h5\"\n",
        "csv_test_path = \"/content/final_datasets/test_extra_TRM_DLY.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4M37nJkp9Vq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885601f4-bc87-41f3-f88e-ec9f7ed7ff88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-5-a87bdd7aeb2c>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n",
            "\n",
            "Test Loss: 0.0951, Accuracy: 0.7859, Precision: 0.9025, Recall: 0.9216, F1-score: 0.9048\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   overdrive       0.84      0.79      0.81      3432\n",
            "  distortion       0.99      0.97      0.98      5148\n",
            "        fuzz       0.99      0.96      0.97      6006\n",
            "     tremolo       0.87      0.99      0.92      4290\n",
            "      phaser       0.99      0.96      0.98      5148\n",
            "     flanger       0.99      0.73      0.84      3432\n",
            "      chorus       0.95      0.95      0.95      6006\n",
            "       delay       0.93      0.93      0.93      7722\n",
            " hall_reverb       0.89      0.98      0.93      5148\n",
            "plate_reverb       0.96      0.87      0.91      3432\n",
            "     octaver       0.57      0.99      0.73      2574\n",
            " auto_filter       0.87      0.94      0.90      4290\n",
            "\n",
            "   micro avg       0.91      0.93      0.92     56628\n",
            "   macro avg       0.90      0.92      0.90     56628\n",
            "weighted avg       0.92      0.93      0.92     56628\n",
            " samples avg       0.87      0.89      0.87     56628\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "# h5_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.h5\"\n",
        "# csv_test_path = \"/content/drive/MyDrive/Capstone 210/Data/Final Datasets/final_real.csv\"\n",
        "\n",
        "h5_test_path = \"/content/final_datasets/final_real.h5\"\n",
        "csv_test_path = \"/content/final_datasets/final_real.csv\"\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/final_multi_effects_alt7.mod\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "v-liaT6r8uXU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-QKEy3qc2hc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}