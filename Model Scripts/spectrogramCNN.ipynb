{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmBJnvR9NbKJ",
        "outputId": "ce7e26b6-647f-41d0-ecf0-09865d9de177"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hvDlTB85Sraj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for spectrogram data.\n",
        "\n",
        "    When the Spectrogram dataset is used to create a dataloader object, the\n",
        "    dataloader consists of batches of spectrograms and their corresponding labels.\n",
        "    Here is info on the shape of the spectrogram and label objects in each batch:\n",
        "\n",
        "    Spectrogram Tensor Dimensions in Batch - (32, 1, 128, 626)\n",
        "        Batch Size: 32\n",
        "        Channels: 1 - Think of it as a grayscale image, rather than RGB\n",
        "        Mel Bands (Height): 128 - 128 Mel filter banks (typical for Mel spectrograms)\n",
        "        Time Steps (Width): 626 - Number of frames\n",
        "\n",
        "    Label Tensor Dimensions in Batch - (32, 13)\n",
        "        Batch Size: 32\n",
        "        Number of Labels: 13 - Multi-hot encoded vector of the 13 effects (includes clean). This\n",
        "            would increase if we added additional effects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hdf5_file, csv_file):\n",
        "        self.hdf5_file_path = hdf5_file\n",
        "        self.labels = pd.read_csv(csv_file)\n",
        "        self.label_map = self.labels.columns[1:].tolist() # Get effect label names\n",
        "        self.hdf5_file = None   # File will be opened for each worker\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Open HDF5 file once per worker\n",
        "        if self.hdf5_file is None:\n",
        "            self.hdf5_file = h5py.File(self.hdf5_file_path, \"r\", swmr=True) # SWMR ensures multi-thread safe\n",
        "\n",
        "        key = self.labels.iloc[idx]['key']\n",
        "        spectrogram = torch.tensor(self.hdf5_file[key][()], dtype=torch.float32).unsqueeze(0)\n",
        "        label_values = self.labels.iloc[idx][1:].infer_objects(copy=False).fillna(0).astype(float).values  # Convert all label columns to float\n",
        "        label = torch.tensor(label_values, dtype=torch.float32)  # Convert to tensor\n",
        "\n",
        "\n",
        "        return spectrogram, label\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.hdf5_file is not None:\n",
        "            self.hdf5_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pVGFYMDhDnI9"
      },
      "outputs": [],
      "source": [
        "class spectrogramCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(spectrogramCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten_size = (512 * (128 // 32) * (626 // 32))\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool(torch.relu(self.bn5(self.conv5(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x) # removed sigmoid, redundant if using BCEWithLogitcsLoss\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p3BlietYUQpv"
      },
      "outputs": [],
      "source": [
        "# Initialize dataset from HD5F and csv file\n",
        "\n",
        "h5_train_path = '/content/drive/MyDrive/Capstone 210/Final Dataset/Train/final_train_single_effects.h5'\n",
        "csv_train_path = '/content/drive/MyDrive/Capstone 210/Final Dataset/Train/final_train_single_effects.csv'\n",
        "\n",
        "h5_val_path = '/content/drive/MyDrive/Capstone 210/Final Dataset/Validate/final_validate_single_effects.h5'\n",
        "csv_val_path = '/content/drive/MyDrive/Capstone 210/Final Dataset/Validate/final_validate_single_effects.csv'\n",
        "\n",
        "model_save_path = \"/content/drive/MyDrive/Capstone 210/Models/last_model.mod\"\n",
        "model_load_path = \"/content/drive/MyDrive/Capstone 210/Models/last_model.mod\"\n",
        "\n",
        "train_dataset = SpectrogramDataset(h5_train_path, csv_train_path)\n",
        "val_dataset = SpectrogramDataset(h5_val_path, csv_val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUEbBB7wX05C",
        "outputId": "0a9f214f-9621-49fc-ee63-98de1638c9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [10/1428], Loss: 0.2647\n",
            "Epoch [1/5], Batch [20/1428], Loss: 0.2466\n",
            "Epoch [1/5], Batch [30/1428], Loss: 0.2468\n",
            "Epoch [1/5], Batch [40/1428], Loss: 0.2309\n",
            "Epoch [1/5], Batch [50/1428], Loss: 0.2296\n",
            "Epoch [1/5], Batch [60/1428], Loss: 0.1676\n",
            "Epoch [1/5], Batch [70/1428], Loss: 0.1847\n",
            "Epoch [1/5], Batch [80/1428], Loss: 0.1715\n",
            "Epoch [1/5], Batch [90/1428], Loss: 0.1662\n",
            "Epoch [1/5], Batch [100/1428], Loss: 0.1465\n",
            "Epoch [1/5], Batch [110/1428], Loss: 0.1611\n",
            "Epoch [1/5], Batch [120/1428], Loss: 0.1695\n",
            "Epoch [1/5], Batch [130/1428], Loss: 0.1488\n",
            "Epoch [1/5], Batch [140/1428], Loss: 0.1382\n",
            "Epoch [1/5], Batch [150/1428], Loss: 0.0884\n",
            "Epoch [1/5], Batch [160/1428], Loss: 0.1042\n",
            "Epoch [1/5], Batch [170/1428], Loss: 0.1228\n",
            "Epoch [1/5], Batch [180/1428], Loss: 0.1310\n",
            "Epoch [1/5], Batch [190/1428], Loss: 0.0854\n",
            "Epoch [1/5], Batch [200/1428], Loss: 0.1188\n",
            "Epoch [1/5], Batch [210/1428], Loss: 0.1168\n",
            "Epoch [1/5], Batch [220/1428], Loss: 0.0914\n",
            "Epoch [1/5], Batch [230/1428], Loss: 0.0804\n",
            "Epoch [1/5], Batch [240/1428], Loss: 0.0830\n",
            "Epoch [1/5], Batch [250/1428], Loss: 0.0954\n",
            "Epoch [1/5], Batch [260/1428], Loss: 0.0565\n",
            "Epoch [1/5], Batch [270/1428], Loss: 0.0673\n",
            "Epoch [1/5], Batch [280/1428], Loss: 0.0675\n",
            "Epoch [1/5], Batch [290/1428], Loss: 0.0528\n",
            "Epoch [1/5], Batch [300/1428], Loss: 0.0667\n",
            "Epoch [1/5], Batch [310/1428], Loss: 0.0606\n",
            "Epoch [1/5], Batch [320/1428], Loss: 0.0547\n",
            "Epoch [1/5], Batch [330/1428], Loss: 0.0401\n",
            "Epoch [1/5], Batch [340/1428], Loss: 0.0685\n",
            "Epoch [1/5], Batch [350/1428], Loss: 0.0765\n",
            "Epoch [1/5], Batch [360/1428], Loss: 0.0341\n",
            "Epoch [1/5], Batch [370/1428], Loss: 0.0626\n",
            "Epoch [1/5], Batch [380/1428], Loss: 0.0390\n",
            "Epoch [1/5], Batch [390/1428], Loss: 0.0418\n",
            "Epoch [1/5], Batch [400/1428], Loss: 0.0630\n",
            "Epoch [1/5], Batch [410/1428], Loss: 0.0562\n",
            "Epoch [1/5], Batch [420/1428], Loss: 0.0904\n",
            "Epoch [1/5], Batch [430/1428], Loss: 0.0282\n",
            "Epoch [1/5], Batch [440/1428], Loss: 0.0313\n",
            "Epoch [1/5], Batch [450/1428], Loss: 0.0556\n",
            "Epoch [1/5], Batch [460/1428], Loss: 0.0475\n",
            "Epoch [1/5], Batch [470/1428], Loss: 0.0316\n",
            "Epoch [1/5], Batch [480/1428], Loss: 0.0608\n",
            "Epoch [1/5], Batch [490/1428], Loss: 0.0266\n",
            "Epoch [1/5], Batch [500/1428], Loss: 0.0602\n",
            "Epoch [1/5], Batch [510/1428], Loss: 0.0398\n",
            "Epoch [1/5], Batch [520/1428], Loss: 0.0715\n",
            "Epoch [1/5], Batch [530/1428], Loss: 0.0412\n",
            "Epoch [1/5], Batch [540/1428], Loss: 0.0331\n",
            "Epoch [1/5], Batch [550/1428], Loss: 0.0473\n",
            "Epoch [1/5], Batch [560/1428], Loss: 0.0292\n",
            "Epoch [1/5], Batch [570/1428], Loss: 0.0202\n",
            "Epoch [1/5], Batch [580/1428], Loss: 0.0246\n",
            "Epoch [1/5], Batch [590/1428], Loss: 0.0470\n",
            "Epoch [1/5], Batch [600/1428], Loss: 0.0546\n",
            "Epoch [1/5], Batch [610/1428], Loss: 0.0491\n",
            "Epoch [1/5], Batch [620/1428], Loss: 0.0455\n",
            "Epoch [1/5], Batch [630/1428], Loss: 0.0391\n",
            "Epoch [1/5], Batch [640/1428], Loss: 0.0336\n",
            "Epoch [1/5], Batch [650/1428], Loss: 0.0444\n",
            "Epoch [1/5], Batch [660/1428], Loss: 0.0407\n",
            "Epoch [1/5], Batch [670/1428], Loss: 0.0410\n",
            "Epoch [1/5], Batch [680/1428], Loss: 0.0732\n",
            "Epoch [1/5], Batch [690/1428], Loss: 0.0458\n",
            "Epoch [1/5], Batch [700/1428], Loss: 0.0227\n",
            "Epoch [1/5], Batch [710/1428], Loss: 0.0206\n",
            "Epoch [1/5], Batch [720/1428], Loss: 0.0382\n",
            "Epoch [1/5], Batch [730/1428], Loss: 0.0215\n",
            "Epoch [1/5], Batch [740/1428], Loss: 0.0279\n",
            "Epoch [1/5], Batch [750/1428], Loss: 0.0194\n",
            "Epoch [1/5], Batch [760/1428], Loss: 0.0254\n",
            "Epoch [1/5], Batch [770/1428], Loss: 0.0370\n",
            "Epoch [1/5], Batch [780/1428], Loss: 0.0405\n",
            "Epoch [1/5], Batch [790/1428], Loss: 0.0285\n",
            "Epoch [1/5], Batch [800/1428], Loss: 0.0486\n",
            "Epoch [1/5], Batch [810/1428], Loss: 0.0306\n",
            "Epoch [1/5], Batch [820/1428], Loss: 0.0461\n",
            "Epoch [1/5], Batch [830/1428], Loss: 0.0632\n",
            "Epoch [1/5], Batch [840/1428], Loss: 0.0386\n",
            "Epoch [1/5], Batch [850/1428], Loss: 0.0269\n",
            "Epoch [1/5], Batch [860/1428], Loss: 0.0267\n",
            "Epoch [1/5], Batch [870/1428], Loss: 0.0405\n",
            "Epoch [1/5], Batch [880/1428], Loss: 0.0419\n",
            "Epoch [1/5], Batch [890/1428], Loss: 0.0359\n",
            "Epoch [1/5], Batch [900/1428], Loss: 0.0454\n",
            "Epoch [1/5], Batch [910/1428], Loss: 0.0258\n",
            "Epoch [1/5], Batch [920/1428], Loss: 0.0473\n",
            "Epoch [1/5], Batch [930/1428], Loss: 0.0127\n",
            "Epoch [1/5], Batch [940/1428], Loss: 0.0120\n",
            "Epoch [1/5], Batch [950/1428], Loss: 0.0543\n",
            "Epoch [1/5], Batch [960/1428], Loss: 0.0264\n",
            "Epoch [1/5], Batch [970/1428], Loss: 0.0183\n",
            "Epoch [1/5], Batch [980/1428], Loss: 0.0414\n",
            "Epoch [1/5], Batch [990/1428], Loss: 0.0133\n",
            "Epoch [1/5], Batch [1000/1428], Loss: 0.0153\n",
            "Epoch [1/5], Batch [1010/1428], Loss: 0.0415\n",
            "Epoch [1/5], Batch [1020/1428], Loss: 0.0243\n",
            "Epoch [1/5], Batch [1030/1428], Loss: 0.0560\n",
            "Epoch [1/5], Batch [1040/1428], Loss: 0.0146\n",
            "Epoch [1/5], Batch [1050/1428], Loss: 0.0158\n",
            "Epoch [1/5], Batch [1060/1428], Loss: 0.0162\n",
            "Epoch [1/5], Batch [1070/1428], Loss: 0.0088\n",
            "Epoch [1/5], Batch [1080/1428], Loss: 0.0558\n",
            "Epoch [1/5], Batch [1090/1428], Loss: 0.0207\n",
            "Epoch [1/5], Batch [1100/1428], Loss: 0.0666\n",
            "Epoch [1/5], Batch [1110/1428], Loss: 0.0196\n",
            "Epoch [1/5], Batch [1120/1428], Loss: 0.0195\n",
            "Epoch [1/5], Batch [1130/1428], Loss: 0.0121\n",
            "Epoch [1/5], Batch [1140/1428], Loss: 0.0369\n",
            "Epoch [1/5], Batch [1150/1428], Loss: 0.0264\n",
            "Epoch [1/5], Batch [1160/1428], Loss: 0.0240\n",
            "Epoch [1/5], Batch [1170/1428], Loss: 0.0815\n",
            "Epoch [1/5], Batch [1180/1428], Loss: 0.0287\n",
            "Epoch [1/5], Batch [1190/1428], Loss: 0.0263\n",
            "Epoch [1/5], Batch [1200/1428], Loss: 0.0375\n",
            "Epoch [1/5], Batch [1210/1428], Loss: 0.0080\n",
            "Epoch [1/5], Batch [1220/1428], Loss: 0.0292\n",
            "Epoch [1/5], Batch [1230/1428], Loss: 0.0185\n",
            "Epoch [1/5], Batch [1240/1428], Loss: 0.0314\n",
            "Epoch [1/5], Batch [1250/1428], Loss: 0.0085\n",
            "Epoch [1/5], Batch [1260/1428], Loss: 0.0589\n",
            "Epoch [1/5], Batch [1270/1428], Loss: 0.0319\n",
            "Epoch [1/5], Batch [1280/1428], Loss: 0.0228\n",
            "Epoch [1/5], Batch [1290/1428], Loss: 0.0172\n",
            "Epoch [1/5], Batch [1300/1428], Loss: 0.0102\n",
            "Epoch [1/5], Batch [1310/1428], Loss: 0.0138\n",
            "Epoch [1/5], Batch [1320/1428], Loss: 0.0117\n",
            "Epoch [1/5], Batch [1330/1428], Loss: 0.0072\n",
            "Epoch [1/5], Batch [1340/1428], Loss: 0.0297\n",
            "Epoch [1/5], Batch [1350/1428], Loss: 0.0218\n",
            "Epoch [1/5], Batch [1360/1428], Loss: 0.0286\n",
            "Epoch [1/5], Batch [1370/1428], Loss: 0.0107\n",
            "Epoch [1/5], Batch [1380/1428], Loss: 0.0139\n",
            "Epoch [1/5], Batch [1390/1428], Loss: 0.0311\n",
            "Epoch [1/5], Batch [1400/1428], Loss: 0.0062\n",
            "Epoch [1/5], Batch [1410/1428], Loss: 0.0140\n",
            "Epoch [1/5], Batch [1420/1428], Loss: 0.0235\n",
            "Epoch 1/5, Loss: 0.05669796209702795\n",
            "Updated Learning Rate: [3.9800000000000005e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.90      0.94      0.92       753\n",
            "   overdrive       1.00      0.98      0.99       753\n",
            "  distortion       1.00      1.00      1.00       753\n",
            "        fuzz       1.00      1.00      1.00       753\n",
            "     tremolo       1.00      0.99      0.99       753\n",
            "      phaser       1.00      0.98      0.99       753\n",
            "     flanger       0.98      0.95      0.96       753\n",
            "      chorus       0.98      1.00      0.99       753\n",
            "       delay       0.98      0.88      0.93       753\n",
            " hall_reverb       0.97      0.16      0.27       753\n",
            "plate_reverb       0.50      1.00      0.66       753\n",
            "     octaver       1.00      0.98      0.99       753\n",
            " auto_filter       1.00      1.00      1.00       753\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      9789\n",
            "   macro avg       0.95      0.91      0.90      9789\n",
            "weighted avg       0.95      0.91      0.90      9789\n",
            " samples avg       0.90      0.91      0.90      9789\n",
            "\n",
            "\n",
            "Validation Loss: 0.0381, Accuracy: 0.8925, Precision: 0.9462, Recall: 0.9110, F1-score: 0.8996\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/last_model.mod\n",
            "Epoch [2/5], Batch [10/1428], Loss: 0.0117\n",
            "Epoch [2/5], Batch [20/1428], Loss: 0.0173\n",
            "Epoch [2/5], Batch [30/1428], Loss: 0.0107\n",
            "Epoch [2/5], Batch [40/1428], Loss: 0.0082\n",
            "Epoch [2/5], Batch [50/1428], Loss: 0.0070\n",
            "Epoch [2/5], Batch [60/1428], Loss: 0.0050\n",
            "Epoch [2/5], Batch [70/1428], Loss: 0.0053\n",
            "Epoch [2/5], Batch [80/1428], Loss: 0.0106\n",
            "Epoch [2/5], Batch [90/1428], Loss: 0.0143\n",
            "Epoch [2/5], Batch [100/1428], Loss: 0.0039\n",
            "Epoch [2/5], Batch [110/1428], Loss: 0.0067\n",
            "Epoch [2/5], Batch [120/1428], Loss: 0.0208\n",
            "Epoch [2/5], Batch [130/1428], Loss: 0.0216\n",
            "Epoch [2/5], Batch [140/1428], Loss: 0.0079\n",
            "Epoch [2/5], Batch [150/1428], Loss: 0.0096\n",
            "Epoch [2/5], Batch [160/1428], Loss: 0.0048\n",
            "Epoch [2/5], Batch [170/1428], Loss: 0.0121\n",
            "Epoch [2/5], Batch [180/1428], Loss: 0.0032\n",
            "Epoch [2/5], Batch [190/1428], Loss: 0.0145\n",
            "Epoch [2/5], Batch [200/1428], Loss: 0.0159\n",
            "Epoch [2/5], Batch [210/1428], Loss: 0.0032\n",
            "Epoch [2/5], Batch [220/1428], Loss: 0.0074\n",
            "Epoch [2/5], Batch [230/1428], Loss: 0.0055\n",
            "Epoch [2/5], Batch [240/1428], Loss: 0.0080\n",
            "Epoch [2/5], Batch [250/1428], Loss: 0.0095\n",
            "Epoch [2/5], Batch [260/1428], Loss: 0.0086\n",
            "Epoch [2/5], Batch [270/1428], Loss: 0.0354\n",
            "Epoch [2/5], Batch [280/1428], Loss: 0.0126\n",
            "Epoch [2/5], Batch [290/1428], Loss: 0.0338\n",
            "Epoch [2/5], Batch [300/1428], Loss: 0.0230\n",
            "Epoch [2/5], Batch [310/1428], Loss: 0.0135\n",
            "Epoch [2/5], Batch [320/1428], Loss: 0.0114\n",
            "Epoch [2/5], Batch [330/1428], Loss: 0.0078\n",
            "Epoch [2/5], Batch [340/1428], Loss: 0.0164\n",
            "Epoch [2/5], Batch [350/1428], Loss: 0.0114\n",
            "Epoch [2/5], Batch [360/1428], Loss: 0.0055\n",
            "Epoch [2/5], Batch [370/1428], Loss: 0.0072\n",
            "Epoch [2/5], Batch [380/1428], Loss: 0.0303\n",
            "Epoch [2/5], Batch [390/1428], Loss: 0.0096\n",
            "Epoch [2/5], Batch [400/1428], Loss: 0.0147\n",
            "Epoch [2/5], Batch [410/1428], Loss: 0.0070\n",
            "Epoch [2/5], Batch [420/1428], Loss: 0.0117\n",
            "Epoch [2/5], Batch [430/1428], Loss: 0.0054\n",
            "Epoch [2/5], Batch [440/1428], Loss: 0.0144\n",
            "Epoch [2/5], Batch [450/1428], Loss: 0.0067\n",
            "Epoch [2/5], Batch [460/1428], Loss: 0.0018\n",
            "Epoch [2/5], Batch [470/1428], Loss: 0.0163\n",
            "Epoch [2/5], Batch [480/1428], Loss: 0.0171\n",
            "Epoch [2/5], Batch [490/1428], Loss: 0.0029\n",
            "Epoch [2/5], Batch [500/1428], Loss: 0.0078\n",
            "Epoch [2/5], Batch [510/1428], Loss: 0.0154\n",
            "Epoch [2/5], Batch [520/1428], Loss: 0.0120\n",
            "Epoch [2/5], Batch [530/1428], Loss: 0.0139\n",
            "Epoch [2/5], Batch [540/1428], Loss: 0.0028\n",
            "Epoch [2/5], Batch [550/1428], Loss: 0.0034\n",
            "Epoch [2/5], Batch [560/1428], Loss: 0.0032\n",
            "Epoch [2/5], Batch [570/1428], Loss: 0.0082\n",
            "Epoch [2/5], Batch [580/1428], Loss: 0.0060\n",
            "Epoch [2/5], Batch [590/1428], Loss: 0.0039\n",
            "Epoch [2/5], Batch [600/1428], Loss: 0.0076\n",
            "Epoch [2/5], Batch [610/1428], Loss: 0.0109\n",
            "Epoch [2/5], Batch [620/1428], Loss: 0.0021\n",
            "Epoch [2/5], Batch [630/1428], Loss: 0.0061\n",
            "Epoch [2/5], Batch [640/1428], Loss: 0.0022\n",
            "Epoch [2/5], Batch [650/1428], Loss: 0.0126\n",
            "Epoch [2/5], Batch [660/1428], Loss: 0.0033\n",
            "Epoch [2/5], Batch [670/1428], Loss: 0.0025\n",
            "Epoch [2/5], Batch [680/1428], Loss: 0.0113\n",
            "Epoch [2/5], Batch [690/1428], Loss: 0.0031\n",
            "Epoch [2/5], Batch [700/1428], Loss: 0.0047\n",
            "Epoch [2/5], Batch [710/1428], Loss: 0.0027\n",
            "Epoch [2/5], Batch [720/1428], Loss: 0.0156\n",
            "Epoch [2/5], Batch [730/1428], Loss: 0.0039\n",
            "Epoch [2/5], Batch [740/1428], Loss: 0.0177\n",
            "Epoch [2/5], Batch [750/1428], Loss: 0.0047\n",
            "Epoch [2/5], Batch [760/1428], Loss: 0.0134\n",
            "Epoch [2/5], Batch [770/1428], Loss: 0.0189\n",
            "Epoch [2/5], Batch [780/1428], Loss: 0.0125\n",
            "Epoch [2/5], Batch [790/1428], Loss: 0.0145\n",
            "Epoch [2/5], Batch [800/1428], Loss: 0.0159\n",
            "Epoch [2/5], Batch [810/1428], Loss: 0.0153\n",
            "Epoch [2/5], Batch [820/1428], Loss: 0.0041\n",
            "Epoch [2/5], Batch [830/1428], Loss: 0.0092\n",
            "Epoch [2/5], Batch [840/1428], Loss: 0.0131\n",
            "Epoch [2/5], Batch [850/1428], Loss: 0.0037\n",
            "Epoch [2/5], Batch [860/1428], Loss: 0.0246\n",
            "Epoch [2/5], Batch [870/1428], Loss: 0.0041\n",
            "Epoch [2/5], Batch [880/1428], Loss: 0.0094\n",
            "Epoch [2/5], Batch [890/1428], Loss: 0.0118\n",
            "Epoch [2/5], Batch [900/1428], Loss: 0.0177\n",
            "Epoch [2/5], Batch [910/1428], Loss: 0.0055\n",
            "Epoch [2/5], Batch [920/1428], Loss: 0.0046\n",
            "Epoch [2/5], Batch [930/1428], Loss: 0.0056\n",
            "Epoch [2/5], Batch [940/1428], Loss: 0.0168\n",
            "Epoch [2/5], Batch [950/1428], Loss: 0.0096\n",
            "Epoch [2/5], Batch [960/1428], Loss: 0.0249\n",
            "Epoch [2/5], Batch [970/1428], Loss: 0.0197\n",
            "Epoch [2/5], Batch [980/1428], Loss: 0.0384\n",
            "Epoch [2/5], Batch [990/1428], Loss: 0.0030\n",
            "Epoch [2/5], Batch [1000/1428], Loss: 0.0048\n",
            "Epoch [2/5], Batch [1010/1428], Loss: 0.0076\n",
            "Epoch [2/5], Batch [1020/1428], Loss: 0.0015\n",
            "Epoch [2/5], Batch [1030/1428], Loss: 0.0040\n",
            "Epoch [2/5], Batch [1040/1428], Loss: 0.0070\n",
            "Epoch [2/5], Batch [1050/1428], Loss: 0.0127\n",
            "Epoch [2/5], Batch [1060/1428], Loss: 0.0032\n",
            "Epoch [2/5], Batch [1070/1428], Loss: 0.0069\n",
            "Epoch [2/5], Batch [1080/1428], Loss: 0.0046\n",
            "Epoch [2/5], Batch [1090/1428], Loss: 0.0084\n",
            "Epoch [2/5], Batch [1100/1428], Loss: 0.0015\n",
            "Epoch [2/5], Batch [1110/1428], Loss: 0.0008\n",
            "Epoch [2/5], Batch [1120/1428], Loss: 0.0019\n",
            "Epoch [2/5], Batch [1130/1428], Loss: 0.0189\n",
            "Epoch [2/5], Batch [1140/1428], Loss: 0.0037\n",
            "Epoch [2/5], Batch [1150/1428], Loss: 0.0014\n",
            "Epoch [2/5], Batch [1160/1428], Loss: 0.0033\n",
            "Epoch [2/5], Batch [1170/1428], Loss: 0.0098\n",
            "Epoch [2/5], Batch [1180/1428], Loss: 0.0125\n",
            "Epoch [2/5], Batch [1190/1428], Loss: 0.0123\n",
            "Epoch [2/5], Batch [1200/1428], Loss: 0.0060\n",
            "Epoch [2/5], Batch [1210/1428], Loss: 0.0112\n",
            "Epoch [2/5], Batch [1220/1428], Loss: 0.0057\n",
            "Epoch [2/5], Batch [1230/1428], Loss: 0.0018\n",
            "Epoch [2/5], Batch [1240/1428], Loss: 0.0064\n",
            "Epoch [2/5], Batch [1250/1428], Loss: 0.0053\n",
            "Epoch [2/5], Batch [1260/1428], Loss: 0.0017\n",
            "Epoch [2/5], Batch [1270/1428], Loss: 0.0108\n",
            "Epoch [2/5], Batch [1280/1428], Loss: 0.0041\n",
            "Epoch [2/5], Batch [1290/1428], Loss: 0.0106\n",
            "Epoch [2/5], Batch [1300/1428], Loss: 0.0078\n",
            "Epoch [2/5], Batch [1310/1428], Loss: 0.0094\n",
            "Epoch [2/5], Batch [1320/1428], Loss: 0.0157\n",
            "Epoch [2/5], Batch [1330/1428], Loss: 0.0074\n",
            "Epoch [2/5], Batch [1340/1428], Loss: 0.0017\n",
            "Epoch [2/5], Batch [1350/1428], Loss: 0.0043\n",
            "Epoch [2/5], Batch [1360/1428], Loss: 0.0061\n",
            "Epoch [2/5], Batch [1370/1428], Loss: 0.0015\n",
            "Epoch [2/5], Batch [1380/1428], Loss: 0.0007\n",
            "Epoch [2/5], Batch [1390/1428], Loss: 0.0045\n",
            "Epoch [2/5], Batch [1400/1428], Loss: 0.0106\n",
            "Epoch [2/5], Batch [1410/1428], Loss: 0.0029\n",
            "Epoch [2/5], Batch [1420/1428], Loss: 0.0042\n",
            "Epoch 2/5, Loss: 0.009382478749587074\n",
            "Updated Learning Rate: [1.58404e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.99      0.85      0.91       753\n",
            "   overdrive       1.00      1.00      1.00       753\n",
            "  distortion       1.00      1.00      1.00       753\n",
            "        fuzz       1.00      1.00      1.00       753\n",
            "     tremolo       1.00      0.99      1.00       753\n",
            "      phaser       1.00      0.99      1.00       753\n",
            "     flanger       0.91      0.99      0.95       753\n",
            "      chorus       1.00      0.99      0.99       753\n",
            "       delay       0.94      0.98      0.96       753\n",
            " hall_reverb       0.97      0.92      0.94       753\n",
            "plate_reverb       0.95      0.94      0.94       753\n",
            "     octaver       1.00      0.99      0.99       753\n",
            " auto_filter       1.00      1.00      1.00       753\n",
            "\n",
            "   micro avg       0.98      0.97      0.98      9789\n",
            "   macro avg       0.98      0.97      0.98      9789\n",
            "weighted avg       0.98      0.97      0.98      9789\n",
            " samples avg       0.97      0.97      0.97      9789\n",
            "\n",
            "\n",
            "Validation Loss: 0.0113, Accuracy: 0.9640, Precision: 0.9803, Recall: 0.9715, F1-score: 0.9752\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/last_model.mod\n",
            "Epoch [3/5], Batch [10/1428], Loss: 0.0049\n",
            "Epoch [3/5], Batch [20/1428], Loss: 0.0112\n",
            "Epoch [3/5], Batch [30/1428], Loss: 0.0014\n",
            "Epoch [3/5], Batch [40/1428], Loss: 0.0157\n",
            "Epoch [3/5], Batch [50/1428], Loss: 0.0109\n",
            "Epoch [3/5], Batch [60/1428], Loss: 0.0024\n",
            "Epoch [3/5], Batch [70/1428], Loss: 0.0045\n",
            "Epoch [3/5], Batch [80/1428], Loss: 0.0020\n",
            "Epoch [3/5], Batch [90/1428], Loss: 0.0008\n",
            "Epoch [3/5], Batch [100/1428], Loss: 0.0049\n",
            "Epoch [3/5], Batch [110/1428], Loss: 0.0027\n",
            "Epoch [3/5], Batch [120/1428], Loss: 0.0070\n",
            "Epoch [3/5], Batch [130/1428], Loss: 0.0017\n",
            "Epoch [3/5], Batch [140/1428], Loss: 0.0011\n",
            "Epoch [3/5], Batch [150/1428], Loss: 0.0008\n",
            "Epoch [3/5], Batch [160/1428], Loss: 0.0070\n",
            "Epoch [3/5], Batch [170/1428], Loss: 0.0009\n",
            "Epoch [3/5], Batch [180/1428], Loss: 0.0046\n",
            "Epoch [3/5], Batch [190/1428], Loss: 0.0181\n",
            "Epoch [3/5], Batch [200/1428], Loss: 0.0008\n",
            "Epoch [3/5], Batch [210/1428], Loss: 0.0055\n",
            "Epoch [3/5], Batch [220/1428], Loss: 0.0106\n",
            "Epoch [3/5], Batch [230/1428], Loss: 0.0110\n",
            "Epoch [3/5], Batch [240/1428], Loss: 0.0030\n",
            "Epoch [3/5], Batch [250/1428], Loss: 0.0095\n",
            "Epoch [3/5], Batch [260/1428], Loss: 0.0037\n",
            "Epoch [3/5], Batch [270/1428], Loss: 0.0008\n",
            "Epoch [3/5], Batch [280/1428], Loss: 0.0009\n",
            "Epoch [3/5], Batch [290/1428], Loss: 0.0306\n",
            "Epoch [3/5], Batch [300/1428], Loss: 0.0037\n",
            "Epoch [3/5], Batch [310/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [320/1428], Loss: 0.0040\n",
            "Epoch [3/5], Batch [330/1428], Loss: 0.0014\n",
            "Epoch [3/5], Batch [340/1428], Loss: 0.0122\n",
            "Epoch [3/5], Batch [350/1428], Loss: 0.0030\n",
            "Epoch [3/5], Batch [360/1428], Loss: 0.0020\n",
            "Epoch [3/5], Batch [370/1428], Loss: 0.0013\n",
            "Epoch [3/5], Batch [380/1428], Loss: 0.0021\n",
            "Epoch [3/5], Batch [390/1428], Loss: 0.0005\n",
            "Epoch [3/5], Batch [400/1428], Loss: 0.0014\n",
            "Epoch [3/5], Batch [410/1428], Loss: 0.0010\n",
            "Epoch [3/5], Batch [420/1428], Loss: 0.0092\n",
            "Epoch [3/5], Batch [430/1428], Loss: 0.0021\n",
            "Epoch [3/5], Batch [440/1428], Loss: 0.0019\n",
            "Epoch [3/5], Batch [450/1428], Loss: 0.0154\n",
            "Epoch [3/5], Batch [460/1428], Loss: 0.0033\n",
            "Epoch [3/5], Batch [470/1428], Loss: 0.0035\n",
            "Epoch [3/5], Batch [480/1428], Loss: 0.0015\n",
            "Epoch [3/5], Batch [490/1428], Loss: 0.0068\n",
            "Epoch [3/5], Batch [500/1428], Loss: 0.0088\n",
            "Epoch [3/5], Batch [510/1428], Loss: 0.0069\n",
            "Epoch [3/5], Batch [520/1428], Loss: 0.0010\n",
            "Epoch [3/5], Batch [530/1428], Loss: 0.0009\n",
            "Epoch [3/5], Batch [540/1428], Loss: 0.0015\n",
            "Epoch [3/5], Batch [550/1428], Loss: 0.0011\n",
            "Epoch [3/5], Batch [560/1428], Loss: 0.0106\n",
            "Epoch [3/5], Batch [570/1428], Loss: 0.0066\n",
            "Epoch [3/5], Batch [580/1428], Loss: 0.0043\n",
            "Epoch [3/5], Batch [590/1428], Loss: 0.0004\n",
            "Epoch [3/5], Batch [600/1428], Loss: 0.0042\n",
            "Epoch [3/5], Batch [610/1428], Loss: 0.0049\n",
            "Epoch [3/5], Batch [620/1428], Loss: 0.0016\n",
            "Epoch [3/5], Batch [630/1428], Loss: 0.0009\n",
            "Epoch [3/5], Batch [640/1428], Loss: 0.0020\n",
            "Epoch [3/5], Batch [650/1428], Loss: 0.0096\n",
            "Epoch [3/5], Batch [660/1428], Loss: 0.0069\n",
            "Epoch [3/5], Batch [670/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [680/1428], Loss: 0.0005\n",
            "Epoch [3/5], Batch [690/1428], Loss: 0.0007\n",
            "Epoch [3/5], Batch [700/1428], Loss: 0.0033\n",
            "Epoch [3/5], Batch [710/1428], Loss: 0.0060\n",
            "Epoch [3/5], Batch [720/1428], Loss: 0.0042\n",
            "Epoch [3/5], Batch [730/1428], Loss: 0.0048\n",
            "Epoch [3/5], Batch [740/1428], Loss: 0.0030\n",
            "Epoch [3/5], Batch [750/1428], Loss: 0.0014\n",
            "Epoch [3/5], Batch [760/1428], Loss: 0.0029\n",
            "Epoch [3/5], Batch [770/1428], Loss: 0.0004\n",
            "Epoch [3/5], Batch [780/1428], Loss: 0.0006\n",
            "Epoch [3/5], Batch [790/1428], Loss: 0.0045\n",
            "Epoch [3/5], Batch [800/1428], Loss: 0.0160\n",
            "Epoch [3/5], Batch [810/1428], Loss: 0.0038\n",
            "Epoch [3/5], Batch [820/1428], Loss: 0.0007\n",
            "Epoch [3/5], Batch [830/1428], Loss: 0.0056\n",
            "Epoch [3/5], Batch [840/1428], Loss: 0.0015\n",
            "Epoch [3/5], Batch [850/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [860/1428], Loss: 0.0013\n",
            "Epoch [3/5], Batch [870/1428], Loss: 0.0008\n",
            "Epoch [3/5], Batch [880/1428], Loss: 0.0097\n",
            "Epoch [3/5], Batch [890/1428], Loss: 0.0045\n",
            "Epoch [3/5], Batch [900/1428], Loss: 0.0014\n",
            "Epoch [3/5], Batch [910/1428], Loss: 0.0066\n",
            "Epoch [3/5], Batch [920/1428], Loss: 0.0121\n",
            "Epoch [3/5], Batch [930/1428], Loss: 0.0030\n",
            "Epoch [3/5], Batch [940/1428], Loss: 0.0013\n",
            "Epoch [3/5], Batch [950/1428], Loss: 0.0006\n",
            "Epoch [3/5], Batch [960/1428], Loss: 0.0003\n",
            "Epoch [3/5], Batch [970/1428], Loss: 0.0020\n",
            "Epoch [3/5], Batch [980/1428], Loss: 0.0016\n",
            "Epoch [3/5], Batch [990/1428], Loss: 0.0072\n",
            "Epoch [3/5], Batch [1000/1428], Loss: 0.0045\n",
            "Epoch [3/5], Batch [1010/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [1020/1428], Loss: 0.0081\n",
            "Epoch [3/5], Batch [1030/1428], Loss: 0.0016\n",
            "Epoch [3/5], Batch [1040/1428], Loss: 0.0007\n",
            "Epoch [3/5], Batch [1050/1428], Loss: 0.0029\n",
            "Epoch [3/5], Batch [1060/1428], Loss: 0.0011\n",
            "Epoch [3/5], Batch [1070/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [1080/1428], Loss: 0.0206\n",
            "Epoch [3/5], Batch [1090/1428], Loss: 0.0039\n",
            "Epoch [3/5], Batch [1100/1428], Loss: 0.0102\n",
            "Epoch [3/5], Batch [1110/1428], Loss: 0.0013\n",
            "Epoch [3/5], Batch [1120/1428], Loss: 0.0056\n",
            "Epoch [3/5], Batch [1130/1428], Loss: 0.0030\n",
            "Epoch [3/5], Batch [1140/1428], Loss: 0.0082\n",
            "Epoch [3/5], Batch [1150/1428], Loss: 0.0008\n",
            "Epoch [3/5], Batch [1160/1428], Loss: 0.0107\n",
            "Epoch [3/5], Batch [1170/1428], Loss: 0.0127\n",
            "Epoch [3/5], Batch [1180/1428], Loss: 0.0046\n",
            "Epoch [3/5], Batch [1190/1428], Loss: 0.0035\n",
            "Epoch [3/5], Batch [1200/1428], Loss: 0.0006\n",
            "Epoch [3/5], Batch [1210/1428], Loss: 0.0151\n",
            "Epoch [3/5], Batch [1220/1428], Loss: 0.0132\n",
            "Epoch [3/5], Batch [1230/1428], Loss: 0.0063\n",
            "Epoch [3/5], Batch [1240/1428], Loss: 0.0009\n",
            "Epoch [3/5], Batch [1250/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [1260/1428], Loss: 0.0027\n",
            "Epoch [3/5], Batch [1270/1428], Loss: 0.0012\n",
            "Epoch [3/5], Batch [1280/1428], Loss: 0.0009\n",
            "Epoch [3/5], Batch [1290/1428], Loss: 0.0013\n",
            "Epoch [3/5], Batch [1300/1428], Loss: 0.0032\n",
            "Epoch [3/5], Batch [1310/1428], Loss: 0.0029\n",
            "Epoch [3/5], Batch [1320/1428], Loss: 0.0023\n",
            "Epoch [3/5], Batch [1330/1428], Loss: 0.0023\n",
            "Epoch [3/5], Batch [1340/1428], Loss: 0.0043\n",
            "Epoch [3/5], Batch [1350/1428], Loss: 0.0027\n",
            "Epoch [3/5], Batch [1360/1428], Loss: 0.0003\n",
            "Epoch [3/5], Batch [1370/1428], Loss: 0.0003\n",
            "Epoch [3/5], Batch [1380/1428], Loss: 0.0147\n",
            "Epoch [3/5], Batch [1390/1428], Loss: 0.0123\n",
            "Epoch [3/5], Batch [1400/1428], Loss: 0.0046\n",
            "Epoch [3/5], Batch [1410/1428], Loss: 0.0054\n",
            "Epoch [3/5], Batch [1420/1428], Loss: 0.0003\n",
            "Epoch 3/5, Loss: 0.004406008495635262\n",
            "Updated Learning Rate: [6.304479200000001e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.94      0.95      0.94       753\n",
            "   overdrive       1.00      1.00      1.00       753\n",
            "  distortion       1.00      1.00      1.00       753\n",
            "        fuzz       1.00      1.00      1.00       753\n",
            "     tremolo       1.00      0.99      1.00       753\n",
            "      phaser       1.00      1.00      1.00       753\n",
            "     flanger       0.99      0.98      0.99       753\n",
            "      chorus       1.00      1.00      1.00       753\n",
            "       delay       0.98      0.95      0.96       753\n",
            " hall_reverb       0.92      0.94      0.93       753\n",
            "plate_reverb       0.94      0.95      0.95       753\n",
            "     octaver       1.00      1.00      1.00       753\n",
            " auto_filter       1.00      0.99      1.00       753\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      9789\n",
            "   macro avg       0.98      0.98      0.98      9789\n",
            "weighted avg       0.98      0.98      0.98      9789\n",
            " samples avg       0.98      0.98      0.98      9789\n",
            "\n",
            "\n",
            "Validation Loss: 0.0080, Accuracy: 0.9718, Precision: 0.9815, Recall: 0.9802, F1-score: 0.9808\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/last_model.mod\n",
            "Epoch [4/5], Batch [10/1428], Loss: 0.0071\n",
            "Epoch [4/5], Batch [20/1428], Loss: 0.0119\n",
            "Epoch [4/5], Batch [30/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [40/1428], Loss: 0.0038\n",
            "Epoch [4/5], Batch [50/1428], Loss: 0.0002\n",
            "Epoch [4/5], Batch [60/1428], Loss: 0.0078\n",
            "Epoch [4/5], Batch [70/1428], Loss: 0.0130\n",
            "Epoch [4/5], Batch [80/1428], Loss: 0.0123\n",
            "Epoch [4/5], Batch [90/1428], Loss: 0.0017\n",
            "Epoch [4/5], Batch [100/1428], Loss: 0.0047\n",
            "Epoch [4/5], Batch [110/1428], Loss: 0.0005\n",
            "Epoch [4/5], Batch [120/1428], Loss: 0.0007\n",
            "Epoch [4/5], Batch [130/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [140/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [150/1428], Loss: 0.0114\n",
            "Epoch [4/5], Batch [160/1428], Loss: 0.0037\n",
            "Epoch [4/5], Batch [170/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [180/1428], Loss: 0.0011\n",
            "Epoch [4/5], Batch [190/1428], Loss: 0.0038\n",
            "Epoch [4/5], Batch [200/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [210/1428], Loss: 0.0018\n",
            "Epoch [4/5], Batch [220/1428], Loss: 0.0024\n",
            "Epoch [4/5], Batch [230/1428], Loss: 0.0008\n",
            "Epoch [4/5], Batch [240/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [250/1428], Loss: 0.0016\n",
            "Epoch [4/5], Batch [260/1428], Loss: 0.0027\n",
            "Epoch [4/5], Batch [270/1428], Loss: 0.0011\n",
            "Epoch [4/5], Batch [280/1428], Loss: 0.0102\n",
            "Epoch [4/5], Batch [290/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [300/1428], Loss: 0.0010\n",
            "Epoch [4/5], Batch [310/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [320/1428], Loss: 0.0197\n",
            "Epoch [4/5], Batch [330/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [340/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [350/1428], Loss: 0.0085\n",
            "Epoch [4/5], Batch [360/1428], Loss: 0.0106\n",
            "Epoch [4/5], Batch [370/1428], Loss: 0.0034\n",
            "Epoch [4/5], Batch [380/1428], Loss: 0.0089\n",
            "Epoch [4/5], Batch [390/1428], Loss: 0.0094\n",
            "Epoch [4/5], Batch [400/1428], Loss: 0.0030\n",
            "Epoch [4/5], Batch [410/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [420/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [430/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [440/1428], Loss: 0.0027\n",
            "Epoch [4/5], Batch [450/1428], Loss: 0.0130\n",
            "Epoch [4/5], Batch [460/1428], Loss: 0.0012\n",
            "Epoch [4/5], Batch [470/1428], Loss: 0.0039\n",
            "Epoch [4/5], Batch [480/1428], Loss: 0.0008\n",
            "Epoch [4/5], Batch [490/1428], Loss: 0.0013\n",
            "Epoch [4/5], Batch [500/1428], Loss: 0.0095\n",
            "Epoch [4/5], Batch [510/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [520/1428], Loss: 0.0022\n",
            "Epoch [4/5], Batch [530/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [540/1428], Loss: 0.0027\n",
            "Epoch [4/5], Batch [550/1428], Loss: 0.0001\n",
            "Epoch [4/5], Batch [560/1428], Loss: 0.0130\n",
            "Epoch [4/5], Batch [570/1428], Loss: 0.0035\n",
            "Epoch [4/5], Batch [580/1428], Loss: 0.0012\n",
            "Epoch [4/5], Batch [590/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [600/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [610/1428], Loss: 0.0010\n",
            "Epoch [4/5], Batch [620/1428], Loss: 0.0097\n",
            "Epoch [4/5], Batch [630/1428], Loss: 0.0098\n",
            "Epoch [4/5], Batch [640/1428], Loss: 0.0010\n",
            "Epoch [4/5], Batch [650/1428], Loss: 0.0017\n",
            "Epoch [4/5], Batch [660/1428], Loss: 0.0019\n",
            "Epoch [4/5], Batch [670/1428], Loss: 0.0016\n",
            "Epoch [4/5], Batch [680/1428], Loss: 0.0024\n",
            "Epoch [4/5], Batch [690/1428], Loss: 0.0116\n",
            "Epoch [4/5], Batch [700/1428], Loss: 0.0008\n",
            "Epoch [4/5], Batch [710/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [720/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [730/1428], Loss: 0.0005\n",
            "Epoch [4/5], Batch [740/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [750/1428], Loss: 0.0007\n",
            "Epoch [4/5], Batch [760/1428], Loss: 0.0096\n",
            "Epoch [4/5], Batch [770/1428], Loss: 0.0002\n",
            "Epoch [4/5], Batch [780/1428], Loss: 0.0007\n",
            "Epoch [4/5], Batch [790/1428], Loss: 0.0011\n",
            "Epoch [4/5], Batch [800/1428], Loss: 0.0049\n",
            "Epoch [4/5], Batch [810/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [820/1428], Loss: 0.0024\n",
            "Epoch [4/5], Batch [830/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [840/1428], Loss: 0.0017\n",
            "Epoch [4/5], Batch [850/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [860/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [870/1428], Loss: 0.0046\n",
            "Epoch [4/5], Batch [880/1428], Loss: 0.0029\n",
            "Epoch [4/5], Batch [890/1428], Loss: 0.0021\n",
            "Epoch [4/5], Batch [900/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [910/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [920/1428], Loss: 0.0001\n",
            "Epoch [4/5], Batch [930/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [940/1428], Loss: 0.0139\n",
            "Epoch [4/5], Batch [950/1428], Loss: 0.0002\n",
            "Epoch [4/5], Batch [960/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [970/1428], Loss: 0.0001\n",
            "Epoch [4/5], Batch [980/1428], Loss: 0.0001\n",
            "Epoch [4/5], Batch [990/1428], Loss: 0.0027\n",
            "Epoch [4/5], Batch [1000/1428], Loss: 0.0071\n",
            "Epoch [4/5], Batch [1010/1428], Loss: 0.0016\n",
            "Epoch [4/5], Batch [1020/1428], Loss: 0.0078\n",
            "Epoch [4/5], Batch [1030/1428], Loss: 0.0126\n",
            "Epoch [4/5], Batch [1040/1428], Loss: 0.0010\n",
            "Epoch [4/5], Batch [1050/1428], Loss: 0.0005\n",
            "Epoch [4/5], Batch [1060/1428], Loss: 0.0012\n",
            "Epoch [4/5], Batch [1070/1428], Loss: 0.0089\n",
            "Epoch [4/5], Batch [1080/1428], Loss: 0.0081\n",
            "Epoch [4/5], Batch [1090/1428], Loss: 0.0034\n",
            "Epoch [4/5], Batch [1100/1428], Loss: 0.0023\n",
            "Epoch [4/5], Batch [1110/1428], Loss: 0.0081\n",
            "Epoch [4/5], Batch [1120/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [1130/1428], Loss: 0.0009\n",
            "Epoch [4/5], Batch [1140/1428], Loss: 0.0045\n",
            "Epoch [4/5], Batch [1150/1428], Loss: 0.0025\n",
            "Epoch [4/5], Batch [1160/1428], Loss: 0.0002\n",
            "Epoch [4/5], Batch [1170/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [1180/1428], Loss: 0.0032\n",
            "Epoch [4/5], Batch [1190/1428], Loss: 0.0061\n",
            "Epoch [4/5], Batch [1200/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [1210/1428], Loss: 0.0014\n",
            "Epoch [4/5], Batch [1220/1428], Loss: 0.0028\n",
            "Epoch [4/5], Batch [1230/1428], Loss: 0.0006\n",
            "Epoch [4/5], Batch [1240/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [1250/1428], Loss: 0.0023\n",
            "Epoch [4/5], Batch [1260/1428], Loss: 0.0013\n",
            "Epoch [4/5], Batch [1270/1428], Loss: 0.0036\n",
            "Epoch [4/5], Batch [1280/1428], Loss: 0.0158\n",
            "Epoch [4/5], Batch [1290/1428], Loss: 0.0007\n",
            "Epoch [4/5], Batch [1300/1428], Loss: 0.0095\n",
            "Epoch [4/5], Batch [1310/1428], Loss: 0.0052\n",
            "Epoch [4/5], Batch [1320/1428], Loss: 0.0004\n",
            "Epoch [4/5], Batch [1330/1428], Loss: 0.0045\n",
            "Epoch [4/5], Batch [1340/1428], Loss: 0.0019\n",
            "Epoch [4/5], Batch [1350/1428], Loss: 0.0005\n",
            "Epoch [4/5], Batch [1360/1428], Loss: 0.0049\n",
            "Epoch [4/5], Batch [1370/1428], Loss: 0.0003\n",
            "Epoch [4/5], Batch [1380/1428], Loss: 0.0036\n",
            "Epoch [4/5], Batch [1390/1428], Loss: 0.0054\n",
            "Epoch [4/5], Batch [1400/1428], Loss: 0.0007\n",
            "Epoch [4/5], Batch [1410/1428], Loss: 0.0064\n",
            "Epoch [4/5], Batch [1420/1428], Loss: 0.0003\n",
            "Epoch 4/5, Loss: 0.0031343766805097587\n",
            "Updated Learning Rate: [2.5091827216000004e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.97      0.91      0.94       753\n",
            "   overdrive       1.00      0.99      1.00       753\n",
            "  distortion       1.00      1.00      1.00       753\n",
            "        fuzz       1.00      1.00      1.00       753\n",
            "     tremolo       0.99      0.99      0.99       753\n",
            "      phaser       1.00      0.99      1.00       753\n",
            "     flanger       0.99      0.99      0.99       753\n",
            "      chorus       1.00      1.00      1.00       753\n",
            "       delay       0.97      0.95      0.96       753\n",
            " hall_reverb       0.91      0.96      0.93       753\n",
            "plate_reverb       0.95      0.95      0.95       753\n",
            "     octaver       1.00      0.99      0.99       753\n",
            " auto_filter       1.00      1.00      1.00       753\n",
            "\n",
            "   micro avg       0.98      0.98      0.98      9789\n",
            "   macro avg       0.98      0.98      0.98      9789\n",
            "weighted avg       0.98      0.98      0.98      9789\n",
            " samples avg       0.98      0.98      0.98      9789\n",
            "\n",
            "\n",
            "Validation Loss: 0.0090, Accuracy: 0.9727, Precision: 0.9821, Recall: 0.9781, F1-score: 0.9800\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/last_model.mod\n",
            "Epoch [5/5], Batch [10/1428], Loss: 0.0017\n",
            "Epoch [5/5], Batch [20/1428], Loss: 0.0103\n",
            "Epoch [5/5], Batch [30/1428], Loss: 0.0045\n",
            "Epoch [5/5], Batch [40/1428], Loss: 0.0032\n",
            "Epoch [5/5], Batch [50/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [60/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [70/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [80/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [90/1428], Loss: 0.0040\n",
            "Epoch [5/5], Batch [100/1428], Loss: 0.0045\n",
            "Epoch [5/5], Batch [110/1428], Loss: 0.0010\n",
            "Epoch [5/5], Batch [120/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [130/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [140/1428], Loss: 0.0026\n",
            "Epoch [5/5], Batch [150/1428], Loss: 0.0010\n",
            "Epoch [5/5], Batch [160/1428], Loss: 0.0009\n",
            "Epoch [5/5], Batch [170/1428], Loss: 0.0009\n",
            "Epoch [5/5], Batch [180/1428], Loss: 0.0091\n",
            "Epoch [5/5], Batch [190/1428], Loss: 0.0012\n",
            "Epoch [5/5], Batch [200/1428], Loss: 0.0012\n",
            "Epoch [5/5], Batch [210/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [220/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [230/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [240/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [250/1428], Loss: 0.0046\n",
            "Epoch [5/5], Batch [260/1428], Loss: 0.0010\n",
            "Epoch [5/5], Batch [270/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [280/1428], Loss: 0.0105\n",
            "Epoch [5/5], Batch [290/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [300/1428], Loss: 0.0012\n",
            "Epoch [5/5], Batch [310/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [320/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [330/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [340/1428], Loss: 0.0060\n",
            "Epoch [5/5], Batch [350/1428], Loss: 0.0001\n",
            "Epoch [5/5], Batch [360/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [370/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [380/1428], Loss: 0.0008\n",
            "Epoch [5/5], Batch [390/1428], Loss: 0.0023\n",
            "Epoch [5/5], Batch [400/1428], Loss: 0.0009\n",
            "Epoch [5/5], Batch [410/1428], Loss: 0.0001\n",
            "Epoch [5/5], Batch [420/1428], Loss: 0.0072\n",
            "Epoch [5/5], Batch [430/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [440/1428], Loss: 0.0001\n",
            "Epoch [5/5], Batch [450/1428], Loss: 0.0019\n",
            "Epoch [5/5], Batch [460/1428], Loss: 0.0062\n",
            "Epoch [5/5], Batch [470/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [480/1428], Loss: 0.0013\n",
            "Epoch [5/5], Batch [490/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [500/1428], Loss: 0.0025\n",
            "Epoch [5/5], Batch [510/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [520/1428], Loss: 0.0024\n",
            "Epoch [5/5], Batch [530/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [540/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [550/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [560/1428], Loss: 0.0039\n",
            "Epoch [5/5], Batch [570/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [580/1428], Loss: 0.0107\n",
            "Epoch [5/5], Batch [590/1428], Loss: 0.0015\n",
            "Epoch [5/5], Batch [600/1428], Loss: 0.0085\n",
            "Epoch [5/5], Batch [610/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [620/1428], Loss: 0.0015\n",
            "Epoch [5/5], Batch [630/1428], Loss: 0.0021\n",
            "Epoch [5/5], Batch [640/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [650/1428], Loss: 0.0014\n",
            "Epoch [5/5], Batch [660/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [670/1428], Loss: 0.0011\n",
            "Epoch [5/5], Batch [680/1428], Loss: 0.0017\n",
            "Epoch [5/5], Batch [690/1428], Loss: 0.0196\n",
            "Epoch [5/5], Batch [700/1428], Loss: 0.0011\n",
            "Epoch [5/5], Batch [710/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [720/1428], Loss: 0.0022\n",
            "Epoch [5/5], Batch [730/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [740/1428], Loss: 0.0037\n",
            "Epoch [5/5], Batch [750/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [760/1428], Loss: 0.0018\n",
            "Epoch [5/5], Batch [770/1428], Loss: 0.0025\n",
            "Epoch [5/5], Batch [780/1428], Loss: 0.0017\n",
            "Epoch [5/5], Batch [790/1428], Loss: 0.0020\n",
            "Epoch [5/5], Batch [800/1428], Loss: 0.0015\n",
            "Epoch [5/5], Batch [810/1428], Loss: 0.0010\n",
            "Epoch [5/5], Batch [820/1428], Loss: 0.0038\n",
            "Epoch [5/5], Batch [830/1428], Loss: 0.0025\n",
            "Epoch [5/5], Batch [840/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [850/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [860/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [870/1428], Loss: 0.0110\n",
            "Epoch [5/5], Batch [880/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [890/1428], Loss: 0.0001\n",
            "Epoch [5/5], Batch [900/1428], Loss: 0.0008\n",
            "Epoch [5/5], Batch [910/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [920/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [930/1428], Loss: 0.0039\n",
            "Epoch [5/5], Batch [940/1428], Loss: 0.0051\n",
            "Epoch [5/5], Batch [950/1428], Loss: 0.0027\n",
            "Epoch [5/5], Batch [960/1428], Loss: 0.0015\n",
            "Epoch [5/5], Batch [970/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [980/1428], Loss: 0.0024\n",
            "Epoch [5/5], Batch [990/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [1000/1428], Loss: 0.0025\n",
            "Epoch [5/5], Batch [1010/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [1020/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [1030/1428], Loss: 0.0008\n",
            "Epoch [5/5], Batch [1040/1428], Loss: 0.0014\n",
            "Epoch [5/5], Batch [1050/1428], Loss: 0.0001\n",
            "Epoch [5/5], Batch [1060/1428], Loss: 0.0025\n",
            "Epoch [5/5], Batch [1070/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [1080/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [1090/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [1100/1428], Loss: 0.0016\n",
            "Epoch [5/5], Batch [1110/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [1120/1428], Loss: 0.0021\n",
            "Epoch [5/5], Batch [1130/1428], Loss: 0.0035\n",
            "Epoch [5/5], Batch [1140/1428], Loss: 0.0010\n",
            "Epoch [5/5], Batch [1150/1428], Loss: 0.0027\n",
            "Epoch [5/5], Batch [1160/1428], Loss: 0.0001\n",
            "Epoch [5/5], Batch [1170/1428], Loss: 0.0010\n",
            "Epoch [5/5], Batch [1180/1428], Loss: 0.0012\n",
            "Epoch [5/5], Batch [1190/1428], Loss: 0.0067\n",
            "Epoch [5/5], Batch [1200/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [1210/1428], Loss: 0.0018\n",
            "Epoch [5/5], Batch [1220/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [1230/1428], Loss: 0.0086\n",
            "Epoch [5/5], Batch [1240/1428], Loss: 0.0022\n",
            "Epoch [5/5], Batch [1250/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [1260/1428], Loss: 0.0044\n",
            "Epoch [5/5], Batch [1270/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [1280/1428], Loss: 0.0028\n",
            "Epoch [5/5], Batch [1290/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [1300/1428], Loss: 0.0014\n",
            "Epoch [5/5], Batch [1310/1428], Loss: 0.0006\n",
            "Epoch [5/5], Batch [1320/1428], Loss: 0.0004\n",
            "Epoch [5/5], Batch [1330/1428], Loss: 0.0036\n",
            "Epoch [5/5], Batch [1340/1428], Loss: 0.0015\n",
            "Epoch [5/5], Batch [1350/1428], Loss: 0.0005\n",
            "Epoch [5/5], Batch [1360/1428], Loss: 0.0007\n",
            "Epoch [5/5], Batch [1370/1428], Loss: 0.0011\n",
            "Epoch [5/5], Batch [1380/1428], Loss: 0.0003\n",
            "Epoch [5/5], Batch [1390/1428], Loss: 0.0014\n",
            "Epoch [5/5], Batch [1400/1428], Loss: 0.0002\n",
            "Epoch [5/5], Batch [1410/1428], Loss: 0.0008\n",
            "Epoch [5/5], Batch [1420/1428], Loss: 0.0005\n",
            "Epoch 5/5, Loss: 0.002588372624454716\n",
            "Updated Learning Rate: [9.986547231968e-07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.98      0.92      0.95       753\n",
            "   overdrive       1.00      0.99      1.00       753\n",
            "  distortion       1.00      1.00      1.00       753\n",
            "        fuzz       1.00      1.00      1.00       753\n",
            "     tremolo       0.99      1.00      0.99       753\n",
            "      phaser       1.00      0.99      1.00       753\n",
            "     flanger       0.99      0.99      0.99       753\n",
            "      chorus       1.00      1.00      1.00       753\n",
            "       delay       0.98      0.96      0.97       753\n",
            " hall_reverb       0.93      0.96      0.95       753\n",
            "plate_reverb       0.96      0.95      0.95       753\n",
            "     octaver       1.00      0.99      1.00       753\n",
            " auto_filter       1.00      1.00      1.00       753\n",
            "\n",
            "   micro avg       0.99      0.98      0.98      9789\n",
            "   macro avg       0.99      0.98      0.98      9789\n",
            "weighted avg       0.99      0.98      0.98      9789\n",
            " samples avg       0.98      0.98      0.98      9789\n",
            "\n",
            "\n",
            "Validation Loss: 0.0071, Accuracy: 0.9766, Precision: 0.9855, Recall: 0.9813, F1-score: 0.9833\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Capstone 210/Models/last_model.mod\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=6, pin_memory=True)\n",
        "\n",
        "num_classes = len(train_dataset.label_map)\n",
        "\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.398)  # 0.0001 → 0.00001 over 5 epochs\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "print_freq = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    print(f\"Updated Learning Rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrograms, labels in val_loader:\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            outputs = model(spectrograms)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert logits to binary predictions\n",
        "\n",
        "            # Store for metric computation\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Convert lists to numpy arrays for metric calculations\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # Print classification report\n",
        "    class_names = train_dataset.label_map\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(f\"\\nValidation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\\n\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load test dataset\n",
        "h5_test_path = \"/content/drive/MyDrive/Capstone 210/Final Dataset/Test/final_test_single_effects.h5\"\n",
        "csv_test_path = \"/content/drive/MyDrive/Capstone 210/Final Dataset/Test/final_test_single_effects.csv\"\n",
        "\n",
        "test_dataset = SpectrogramDataset(h5_test_path, csv_test_path)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=12, pin_memory=True)\n",
        "\n",
        "num_classes = len(test_dataset.label_map)\n",
        "\n",
        "# Load a saved model for test dataset metrics\n",
        "model = spectrogramCNN(num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_load_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "print(\"\\nEvaluating with external test dataset...\")\n",
        "\n",
        "model.eval()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loss = 0.0\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectrograms, labels in test_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Convert logits to binary predictions\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "# Compute test metrics\n",
        "test_preds = np.array(test_preds)\n",
        "test_labels = np.array(test_labels)\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "test_precision = precision_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-score: {test_f1:.4f}\\n\")\n",
        "\n",
        "# Print classification report\n",
        "class_names = test_dataset.label_map\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGiHpYYikrqv",
        "outputId": "bb18fb85-705c-47aa-95b6-cbab2f854284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "\n",
            "Evaluating with external test dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-9afac7085df7>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.0063, Accuracy: 0.9774, Precision: 0.9894, Recall: 0.9818, F1-score: 0.9855\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       clean       0.96      0.95      0.95       757\n",
            "   overdrive       1.00      0.99      1.00       757\n",
            "  distortion       1.00      0.99      1.00       757\n",
            "        fuzz       1.00      1.00      1.00       757\n",
            "     tremolo       1.00      0.99      1.00       757\n",
            "      phaser       0.98      0.99      0.99       757\n",
            "     flanger       1.00      0.97      0.98       757\n",
            "      chorus       1.00      1.00      1.00       757\n",
            "       delay       0.98      0.96      0.97       757\n",
            " hall_reverb       0.99      0.95      0.97       757\n",
            "plate_reverb       0.96      0.98      0.97       757\n",
            "     octaver       0.99      0.99      0.99       757\n",
            " auto_filter       1.00      0.99      0.99       757\n",
            "\n",
            "   micro avg       0.99      0.98      0.99      9841\n",
            "   macro avg       0.99      0.98      0.99      9841\n",
            "weighted avg       0.99      0.98      0.99      9841\n",
            " samples avg       0.98      0.98      0.98      9841\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4M37nJkp9Vq1"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}